{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeb4b011-d2aa-443a-a94a-0bb182363c9b",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters cell (must be tagged as \"parameters\" in the notebook)\n",
    "execution_date = None\n",
    "dag_run_id = None\n",
    "ds = None\n",
    "ds_nodash = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22ffd22c-78ab-42db-bbd1-cee4f9b7483f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: kagglehub in c:\\users\\megin\\appdata\\roaming\\python\\python312\\site-packages (0.3.11)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: psycopg2-binary in c:\\users\\megin\\appdata\\roaming\\python\\python312\\site-packages (2.9.10)\n",
      "Requirement already satisfied: sqlalchemy in c:\\users\\megin\\appdata\\roaming\\python\\python312\\site-packages (1.4.54)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from kagglehub) (24.1)\n",
      "Requirement already satisfied: pyyaml in c:\\programdata\\anaconda3\\lib\\site-packages (from kagglehub) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from kagglehub) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from sqlalchemy) (3.0.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->kagglehub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->kagglehub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->kagglehub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->kagglehub) (2024.12.14)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->kagglehub) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install kagglehub pandas psycopg2-binary sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aebe01f6-0b27-4a7c-8339-4c84690278a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['KaggleDatasetAdapter', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', 'auth', 'cache', 'clients', 'colab_cache_resolver', 'competition', 'competition_download', 'config', 'dataset_download', 'dataset_load', 'dataset_upload', 'datasets', 'datasets_enums', 'datasets_helpers', 'env', 'exceptions', 'gcs_upload', 'get_package_asset_path', 'handle', 'http_resolver', 'integrity', 'kaggle_cache_resolver', 'kagglehub', 'load_dataset', 'logger', 'login', 'model_download', 'model_upload', 'models', 'models_helpers', 'notebook_output_download', 'notebooks', 'package_import', 'packages', 'registry', 'resolver', 'signing', 'tracker', 'utility_script_install', 'utility_scripts', 'whoami']\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "print(dir(kagglehub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dc15916-c74b-4cd6-9e1f-79a82761becb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: kaggle in c:\\users\\megin\\appdata\\roaming\\python\\python312\\site-packages (1.7.4.2)\n",
      "Requirement already satisfied: bleach in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle) (4.1.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle) (2024.12.14)\n",
      "Requirement already satisfied: charset-normalizer in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle) (3.3.2)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle) (3.7)\n",
      "Requirement already satisfied: protobuf in c:\\users\\megin\\appdata\\roaming\\python\\python312\\site-packages (from kaggle) (5.29.4)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: python-slugify in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle) (5.0.2)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle) (2.32.3)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in c:\\users\\megin\\appdata\\roaming\\python\\python312\\site-packages (from kaggle) (78.1.1)\n",
      "Requirement already satisfied: six>=1.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: text-unidecode in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle) (1.3)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle) (4.66.5)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle) (2.2.3)\n",
      "Requirement already satisfied: webencodings in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle) (0.5.1)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from bleach->kaggle) (24.1)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->kaggle) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaa93ad1-1263-4f1f-bdee-aa3b4116c65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle module is successfully installed!\n"
     ]
    }
   ],
   "source": [
    "import kaggle\n",
    "print(\"Kaggle module is successfully installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdd709ed-dd9d-4679-a689-5b4c22affbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle API authentication successful!\n"
     ]
    }
   ],
   "source": [
    "kaggle.api.authenticate()\n",
    "print(\"Kaggle API authentication successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bead18a8",
   "metadata": {},
   "source": [
    "Pull Data from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db10617-1625-4d69-af65-18e7676ad3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: transactions.csv → Stored in: C:/Users/megin/fashion_dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\megin\\AppData\\Roaming\\Python\\Python312\\site-packages\\kagglehub\\pandas_datasets.py:91: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  result = read_function(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: customers.csv → Stored in: C:/Users/megin/fashion_dataset\n",
      "Downloaded: discounts.csv → Stored in: C:/Users/megin/fashion_dataset\n",
      "Downloaded: employees.csv → Stored in: C:/Users/megin/fashion_dataset\n",
      "Downloaded: products.csv → Stored in: C:/Users/megin/fashion_dataset\n",
      "Downloaded: stores.csv → Stored in: C:/Users/megin/fashion_dataset\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Define dataset folder (custom cache location)\n",
    "dataset_path = \"/home/megin_mathew/fashion_dataset\"\n",
    "\n",
    "# Set KaggleHub cache override\n",
    "os.environ[\"KAGGLEHUB_CACHE\"] = dataset_path\n",
    "\n",
    "# File names in the dataset\n",
    "file_names = [\n",
    "    \"transactions.csv\", \"customers.csv\", \"discounts.csv\",\n",
    "    \"employees.csv\", \"products.csv\", \"stores.csv\"\n",
    "]\n",
    "\n",
    "# Ensure cache directory exists\n",
    "os.makedirs(dataset_path, exist_ok=True)\n",
    "\n",
    "# Download each file individually\n",
    "for file_name in file_names:\n",
    "    df = kagglehub.dataset_load(\n",
    "        KaggleDatasetAdapter.PANDAS,\n",
    "        \"ricgomes/global-fashion-retail-stores-dataset\",\n",
    "        file_name\n",
    "    )\n",
    "    \n",
    "    # Save to custom cache location\n",
    "    df.to_csv(os.path.join(dataset_path, file_name), index=False)\n",
    "    print(f\"Downloaded: {file_name} → Stored in: {dataset_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be147cd-fa04-4969-9abf-c7d5147a5996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set custom cache location\n",
    "os.environ[\"KAGGLEHUB_CACHE\"] = \"/home/megin_mathew/fashion_dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2d6b94-15e3-4213-a744-4ad1eb5a9de1",
   "metadata": {},
   "source": [
    "Optimized And Cleansed and uses Hadoop along with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100e364d",
   "metadata": {},
   "source": [
    "Cleansing Preprocessing Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d3ff1f-8a99-47c8-b7a2-ae4eac67cc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 18:07:08,075 - INFO - Starting data load process...\n",
      "2025-04-21 18:07:08,471 - INFO - Processing stores.csv...\n",
      "2025-04-21 18:07:16,037 - INFO - Processed DataFrame schema: root\n",
      " |-- store_id: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- store_name: string (nullable = true)\n",
      " |-- number_of_employees: integer (nullable = true)\n",
      " |-- zip_code: string (nullable = true)\n",
      " |-- latitude: float (nullable = true)\n",
      " |-- longitude: float (nullable = true)\n",
      "\n",
      "2025-04-21 18:07:16,951 - INFO - Processing 35 records for table store\n",
      "2025-04-21 18:07:16,953 - INFO - Processing small store table with direct conversion\n",
      "2025-04-21 18:07:17,638 - INFO - Starting to upsert 35 records in batches of 1000\n",
      "2025-04-21 18:07:17,667 - INFO - Insert failed for batch 1, switching to upsert mode\n",
      "2025-04-21 18:07:17,720 - INFO - Successfully upserted batch 1/1\n",
      "2025-04-21 18:07:17,722 - INFO - Successfully processed 35 records to store\n",
      "2025-04-21 18:07:17,723 - INFO - Completed stores.csv in 0:00:09.251108\n",
      "2025-04-21 18:07:17,724 - INFO - Processing employees.csv...\n",
      "2025-04-21 18:07:18,117 - INFO - Processed DataFrame schema: root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- store_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- position: string (nullable = true)\n",
      "\n",
      "2025-04-21 18:07:18,315 - INFO - Processing 404 records for table employee\n",
      "2025-04-21 18:07:18,317 - INFO - Processing small employee table with direct conversion\n",
      "2025-04-21 18:07:18,520 - INFO - Starting to upsert 404 records in batches of 1000\n",
      "2025-04-21 18:07:18,538 - INFO - Insert failed for batch 1, switching to upsert mode\n",
      "2025-04-21 18:07:18,912 - INFO - Successfully upserted batch 1/1\n",
      "2025-04-21 18:07:18,913 - INFO - Successfully processed 404 records to employee\n",
      "2025-04-21 18:07:18,914 - INFO - Completed employees.csv in 0:00:01.187267\n",
      "2025-04-21 18:07:18,916 - INFO - Processing products.csv...\n",
      "2025-04-21 18:07:19,583 - INFO - Processed DataFrame schema: root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- sub_category: string (nullable = true)\n",
      " |-- description_pt: string (nullable = true)\n",
      " |-- description_de: string (nullable = true)\n",
      " |-- description_fr: string (nullable = true)\n",
      " |-- description_es: string (nullable = true)\n",
      " |-- description_en: string (nullable = true)\n",
      " |-- description_zh: string (nullable = true)\n",
      " |-- color: string (nullable = true)\n",
      " |-- sizes: string (nullable = true)\n",
      " |-- production_cost: double (nullable = true)\n",
      "\n",
      "2025-04-21 18:07:19,801 - INFO - Processing 17940 records for table product\n",
      "2025-04-21 18:07:20,836 - INFO - Starting to upsert 10000 records in batches of 1000\n",
      "2025-04-21 18:07:20,910 - INFO - Insert failed for batch 1, switching to upsert mode\n",
      "2025-04-21 18:07:21,827 - INFO - Successfully upserted batch 1/10\n",
      "2025-04-21 18:07:21,888 - INFO - Insert failed for batch 2, switching to upsert mode\n",
      "2025-04-21 18:07:22,799 - INFO - Successfully upserted batch 2/10\n",
      "2025-04-21 18:07:22,868 - INFO - Insert failed for batch 3, switching to upsert mode\n",
      "2025-04-21 18:07:23,854 - INFO - Successfully upserted batch 3/10\n",
      "2025-04-21 18:07:23,894 - INFO - Insert failed for batch 4, switching to upsert mode\n",
      "2025-04-21 18:07:24,827 - INFO - Successfully upserted batch 4/10\n",
      "2025-04-21 18:07:24,888 - INFO - Insert failed for batch 5, switching to upsert mode\n",
      "2025-04-21 18:07:25,910 - INFO - Successfully upserted batch 5/10\n",
      "2025-04-21 18:07:25,968 - INFO - Insert failed for batch 6, switching to upsert mode\n",
      "2025-04-21 18:07:26,911 - INFO - Successfully upserted batch 6/10\n",
      "2025-04-21 18:07:26,971 - INFO - Insert failed for batch 7, switching to upsert mode\n",
      "2025-04-21 18:07:27,963 - INFO - Successfully upserted batch 7/10\n",
      "2025-04-21 18:07:28,021 - INFO - Insert failed for batch 8, switching to upsert mode\n",
      "2025-04-21 18:07:29,220 - INFO - Successfully upserted batch 8/10\n",
      "2025-04-21 18:07:29,278 - INFO - Insert failed for batch 9, switching to upsert mode\n",
      "2025-04-21 18:07:30,404 - INFO - Successfully upserted batch 9/10\n",
      "2025-04-21 18:07:30,465 - INFO - Insert failed for batch 10, switching to upsert mode\n",
      "2025-04-21 18:07:31,463 - INFO - Successfully upserted batch 10/10\n",
      "2025-04-21 18:07:31,465 - INFO - Processed 10000/17940 records\n",
      "2025-04-21 18:07:31,563 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:31,564 - INFO - Processed 17940/17940 records\n",
      "2025-04-21 18:07:31,566 - INFO - Completed products.csv in 0:00:12.646754\n",
      "2025-04-21 18:07:31,569 - INFO - Processing discounts.csv...\n",
      "2025-04-21 18:07:32,052 - INFO - Processed DataFrame schema: root\n",
      " |-- start_date: date (nullable = true)\n",
      " |-- end_date: date (nullable = true)\n",
      " |-- discount_rate: double (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- sub_category: string (nullable = true)\n",
      "\n",
      "2025-04-21 18:07:32,224 - INFO - Processing 181 records for table discount\n",
      "2025-04-21 18:07:32,226 - INFO - Processing small discount table with direct conversion\n",
      "2025-04-21 18:07:32,649 - INFO - Starting to upsert 181 records in batches of 1000\n",
      "2025-04-21 18:07:32,679 - INFO - Successfully inserted batch 1/1\n",
      "2025-04-21 18:07:32,680 - INFO - Successfully processed 181 records to discount\n",
      "2025-04-21 18:07:32,681 - INFO - Completed discounts.csv in 0:00:01.110676\n",
      "2025-04-21 18:07:32,684 - INFO - Processing customers.csv...\n",
      "2025-04-21 18:07:33,706 - INFO - Processed DataFrame schema: root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- join_date: date (nullable = true)\n",
      " |-- telephone: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- date_of_birth: date (nullable = true)\n",
      " |-- job_title: string (nullable = true)\n",
      "\n",
      "2025-04-21 18:07:34,429 - INFO - Processing 1643306 records for table customer\n",
      "2025-04-21 18:07:35,068 - INFO - Starting to upsert 10000 records in batches of 1000\n",
      "2025-04-21 18:07:35,077 - INFO - Insert failed for batch 1, switching to upsert mode\n",
      "2025-04-21 18:07:36,176 - INFO - Successfully upserted batch 1/10\n",
      "2025-04-21 18:07:36,177 - INFO - Insert failed for batch 2, switching to upsert mode\n",
      "2025-04-21 18:07:36,958 - INFO - Successfully upserted batch 2/10\n",
      "2025-04-21 18:07:36,961 - INFO - Insert failed for batch 3, switching to upsert mode\n",
      "2025-04-21 18:07:37,842 - INFO - Successfully upserted batch 3/10\n",
      "2025-04-21 18:07:37,847 - INFO - Insert failed for batch 4, switching to upsert mode\n",
      "2025-04-21 18:07:38,645 - INFO - Successfully upserted batch 4/10\n",
      "2025-04-21 18:07:38,646 - INFO - Insert failed for batch 5, switching to upsert mode\n",
      "2025-04-21 18:07:39,433 - INFO - Successfully upserted batch 5/10\n",
      "2025-04-21 18:07:39,435 - INFO - Insert failed for batch 6, switching to upsert mode\n",
      "2025-04-21 18:07:40,235 - INFO - Successfully upserted batch 6/10\n",
      "2025-04-21 18:07:40,237 - INFO - Insert failed for batch 7, switching to upsert mode\n",
      "2025-04-21 18:07:41,083 - INFO - Successfully upserted batch 7/10\n",
      "2025-04-21 18:07:41,086 - INFO - Insert failed for batch 8, switching to upsert mode\n",
      "2025-04-21 18:07:42,159 - INFO - Successfully upserted batch 8/10\n",
      "2025-04-21 18:07:42,165 - INFO - Insert failed for batch 9, switching to upsert mode\n",
      "2025-04-21 18:07:43,103 - INFO - Successfully upserted batch 9/10\n",
      "2025-04-21 18:07:43,106 - INFO - Insert failed for batch 10, switching to upsert mode\n",
      "2025-04-21 18:07:44,146 - INFO - Successfully upserted batch 10/10\n",
      "2025-04-21 18:07:44,147 - INFO - Processed 10000/1643306 records\n",
      "2025-04-21 18:07:44,199 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:44,201 - INFO - Processed 20000/1643306 records\n",
      "2025-04-21 18:07:44,248 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:44,252 - INFO - Processed 30000/1643306 records\n",
      "2025-04-21 18:07:44,299 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:44,301 - INFO - Processed 40000/1643306 records\n",
      "2025-04-21 18:07:44,346 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:44,348 - INFO - Processed 50000/1643306 records\n",
      "2025-04-21 18:07:44,401 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:44,402 - INFO - Processed 60000/1643306 records\n",
      "2025-04-21 18:07:44,447 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:44,448 - INFO - Processed 70000/1643306 records\n",
      "2025-04-21 18:07:44,477 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:44,478 - INFO - Processed 80000/1643306 records\n",
      "2025-04-21 18:07:44,526 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:44,527 - INFO - Processed 90000/1643306 records\n",
      "2025-04-21 18:07:44,574 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:44,575 - INFO - Processed 100000/1643306 records\n",
      "2025-04-21 18:07:44,638 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:44,640 - INFO - Processed 110000/1643306 records\n",
      "2025-04-21 18:07:44,699 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:44,700 - INFO - Processed 120000/1643306 records\n",
      "2025-04-21 18:07:44,729 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:44,730 - INFO - Processed 130000/1643306 records\n",
      "2025-04-21 18:07:44,778 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:44,780 - INFO - Processed 140000/1643306 records\n",
      "2025-04-21 18:07:44,842 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:44,843 - INFO - Processed 150000/1643306 records\n",
      "2025-04-21 18:07:44,875 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:44,876 - INFO - Processed 160000/1643306 records\n",
      "2025-04-21 18:07:44,904 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:44,906 - INFO - Processed 170000/1643306 records\n",
      "2025-04-21 18:07:44,949 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:44,950 - INFO - Processed 180000/1643306 records\n",
      "2025-04-21 18:07:44,985 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:44,986 - INFO - Processed 190000/1643306 records\n",
      "2025-04-21 18:07:45,017 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:45,018 - INFO - Processed 200000/1643306 records\n",
      "2025-04-21 18:07:45,050 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:45,052 - INFO - Processed 210000/1643306 records\n",
      "2025-04-21 18:07:45,083 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:45,084 - INFO - Processed 220000/1643306 records\n",
      "2025-04-21 18:07:45,138 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:45,140 - INFO - Processed 230000/1643306 records\n",
      "2025-04-21 18:07:45,175 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:45,177 - INFO - Processed 240000/1643306 records\n",
      "2025-04-21 18:07:45,205 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:45,207 - INFO - Processed 250000/1643306 records\n",
      "2025-04-21 18:07:45,265 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:45,267 - INFO - Processed 260000/1643306 records\n",
      "2025-04-21 18:07:45,305 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:45,307 - INFO - Processed 270000/1643306 records\n",
      "2025-04-21 18:07:45,338 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:45,340 - INFO - Processed 280000/1643306 records\n",
      "2025-04-21 18:07:45,378 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:45,379 - INFO - Processed 290000/1643306 records\n",
      "2025-04-21 18:07:45,420 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:45,421 - INFO - Processed 300000/1643306 records\n",
      "2025-04-21 18:07:45,468 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:45,470 - INFO - Processed 310000/1643306 records\n",
      "2025-04-21 18:07:45,497 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:45,498 - INFO - Processed 320000/1643306 records\n",
      "2025-04-21 18:07:45,522 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:45,524 - INFO - Processed 330000/1643306 records\n",
      "2025-04-21 18:07:45,564 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:45,566 - INFO - Processed 340000/1643306 records\n",
      "2025-04-21 18:07:45,610 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:45,613 - INFO - Processed 350000/1643306 records\n",
      "2025-04-21 18:07:45,641 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:45,643 - INFO - Processed 360000/1643306 records\n",
      "2025-04-21 18:07:45,689 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:45,690 - INFO - Processed 370000/1643306 records\n",
      "2025-04-21 18:07:45,750 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:45,751 - INFO - Processed 380000/1643306 records\n",
      "2025-04-21 18:07:45,779 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:45,780 - INFO - Processed 390000/1643306 records\n",
      "2025-04-21 18:07:45,829 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:45,830 - INFO - Processed 400000/1643306 records\n",
      "2025-04-21 18:07:45,873 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:45,874 - INFO - Processed 410000/1643306 records\n",
      "2025-04-21 18:07:45,923 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:45,924 - INFO - Processed 420000/1643306 records\n",
      "2025-04-21 18:07:45,970 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:45,972 - INFO - Processed 430000/1643306 records\n",
      "2025-04-21 18:07:46,032 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:46,034 - INFO - Processed 440000/1643306 records\n",
      "2025-04-21 18:07:46,077 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:46,079 - INFO - Processed 450000/1643306 records\n",
      "2025-04-21 18:07:46,105 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:46,107 - INFO - Processed 460000/1643306 records\n",
      "2025-04-21 18:07:46,142 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:46,144 - INFO - Processed 470000/1643306 records\n",
      "2025-04-21 18:07:46,189 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:46,190 - INFO - Processed 480000/1643306 records\n",
      "2025-04-21 18:07:46,236 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:46,237 - INFO - Processed 490000/1643306 records\n",
      "2025-04-21 18:07:46,282 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:46,283 - INFO - Processed 500000/1643306 records\n",
      "2025-04-21 18:07:46,345 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:46,346 - INFO - Processed 510000/1643306 records\n",
      "2025-04-21 18:07:46,382 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:46,385 - INFO - Processed 520000/1643306 records\n",
      "2025-04-21 18:07:46,439 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:46,440 - INFO - Processed 530000/1643306 records\n",
      "2025-04-21 18:07:46,466 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:46,467 - INFO - Processed 540000/1643306 records\n",
      "2025-04-21 18:07:46,495 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:46,496 - INFO - Processed 550000/1643306 records\n",
      "2025-04-21 18:07:46,521 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:46,522 - INFO - Processed 560000/1643306 records\n",
      "2025-04-21 18:07:46,567 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:46,569 - INFO - Processed 570000/1643306 records\n",
      "2025-04-21 18:07:46,616 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:46,618 - INFO - Processed 580000/1643306 records\n",
      "2025-04-21 18:07:46,679 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:46,681 - INFO - Processed 590000/1643306 records\n",
      "2025-04-21 18:07:46,714 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:46,716 - INFO - Processed 600000/1643306 records\n",
      "2025-04-21 18:07:46,754 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:46,756 - INFO - Processed 610000/1643306 records\n",
      "2025-04-21 18:07:46,790 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:46,793 - INFO - Processed 620000/1643306 records\n",
      "2025-04-21 18:07:46,847 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:46,848 - INFO - Processed 630000/1643306 records\n",
      "2025-04-21 18:07:46,875 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:46,876 - INFO - Processed 640000/1643306 records\n",
      "2025-04-21 18:07:46,903 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:46,904 - INFO - Processed 650000/1643306 records\n",
      "2025-04-21 18:07:46,929 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:46,931 - INFO - Processed 660000/1643306 records\n",
      "2025-04-21 18:07:46,957 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:46,959 - INFO - Processed 670000/1643306 records\n",
      "2025-04-21 18:07:47,005 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:47,007 - INFO - Processed 680000/1643306 records\n",
      "2025-04-21 18:07:47,053 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:47,054 - INFO - Processed 690000/1643306 records\n",
      "2025-04-21 18:07:47,080 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:47,082 - INFO - Processed 700000/1643306 records\n",
      "2025-04-21 18:07:47,108 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:47,109 - INFO - Processed 710000/1643306 records\n",
      "2025-04-21 18:07:47,163 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:47,164 - INFO - Processed 720000/1643306 records\n",
      "2025-04-21 18:07:47,190 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:47,192 - INFO - Processed 730000/1643306 records\n",
      "2025-04-21 18:07:47,220 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:47,223 - INFO - Processed 740000/1643306 records\n",
      "2025-04-21 18:07:47,273 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:47,274 - INFO - Processed 750000/1643306 records\n",
      "2025-04-21 18:07:47,302 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:47,304 - INFO - Processed 760000/1643306 records\n",
      "2025-04-21 18:07:47,351 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:47,353 - INFO - Processed 770000/1643306 records\n",
      "2025-04-21 18:07:47,379 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:47,380 - INFO - Processed 780000/1643306 records\n",
      "2025-04-21 18:07:47,407 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:47,409 - INFO - Processed 790000/1643306 records\n",
      "2025-04-21 18:07:47,436 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:47,438 - INFO - Processed 800000/1643306 records\n",
      "2025-04-21 18:07:47,466 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:47,467 - INFO - Processed 810000/1643306 records\n",
      "2025-04-21 18:07:47,493 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:47,495 - INFO - Processed 820000/1643306 records\n",
      "2025-04-21 18:07:47,538 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:47,540 - INFO - Processed 830000/1643306 records\n",
      "2025-04-21 18:07:47,587 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:47,588 - INFO - Processed 840000/1643306 records\n",
      "2025-04-21 18:07:47,613 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:47,614 - INFO - Processed 850000/1643306 records\n",
      "2025-04-21 18:07:47,648 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:47,651 - INFO - Processed 860000/1643306 records\n",
      "2025-04-21 18:07:47,676 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:47,678 - INFO - Processed 870000/1643306 records\n",
      "2025-04-21 18:07:47,706 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:47,707 - INFO - Processed 880000/1643306 records\n",
      "2025-04-21 18:07:47,760 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:47,761 - INFO - Processed 890000/1643306 records\n",
      "2025-04-21 18:07:47,790 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:47,792 - INFO - Processed 900000/1643306 records\n",
      "2025-04-21 18:07:47,824 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:47,825 - INFO - Processed 910000/1643306 records\n",
      "2025-04-21 18:07:47,854 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:47,856 - INFO - Processed 920000/1643306 records\n",
      "2025-04-21 18:07:47,882 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:47,883 - INFO - Processed 930000/1643306 records\n",
      "2025-04-21 18:07:47,931 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:47,932 - INFO - Processed 940000/1643306 records\n",
      "2025-04-21 18:07:47,955 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:47,957 - INFO - Processed 950000/1643306 records\n",
      "2025-04-21 18:07:47,991 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:47,993 - INFO - Processed 960000/1643306 records\n",
      "2025-04-21 18:07:48,039 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:48,041 - INFO - Processed 970000/1643306 records\n",
      "2025-04-21 18:07:48,070 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:48,071 - INFO - Processed 980000/1643306 records\n",
      "2025-04-21 18:07:48,096 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:48,097 - INFO - Processed 990000/1643306 records\n",
      "2025-04-21 18:07:48,124 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:48,125 - INFO - Processed 1000000/1643306 records\n",
      "2025-04-21 18:07:48,163 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:48,164 - INFO - Processed 1010000/1643306 records\n",
      "2025-04-21 18:07:48,191 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:48,193 - INFO - Processed 1020000/1643306 records\n",
      "2025-04-21 18:07:48,242 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:48,243 - INFO - Processed 1030000/1643306 records\n",
      "2025-04-21 18:07:48,288 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:48,289 - INFO - Processed 1040000/1643306 records\n",
      "2025-04-21 18:07:48,316 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:48,317 - INFO - Processed 1050000/1643306 records\n",
      "2025-04-21 18:07:48,366 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:48,368 - INFO - Processed 1060000/1643306 records\n",
      "2025-04-21 18:07:48,393 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:48,394 - INFO - Processed 1070000/1643306 records\n",
      "2025-04-21 18:07:48,422 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:48,424 - INFO - Processed 1080000/1643306 records\n",
      "2025-04-21 18:07:48,452 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:48,453 - INFO - Processed 1090000/1643306 records\n",
      "2025-04-21 18:07:48,493 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:48,495 - INFO - Processed 1100000/1643306 records\n",
      "2025-04-21 18:07:48,541 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:48,543 - INFO - Processed 1110000/1643306 records\n",
      "2025-04-21 18:07:48,566 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:48,568 - INFO - Processed 1120000/1643306 records\n",
      "2025-04-21 18:07:48,602 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:48,603 - INFO - Processed 1130000/1643306 records\n",
      "2025-04-21 18:07:48,650 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:48,652 - INFO - Processed 1140000/1643306 records\n",
      "2025-04-21 18:07:48,686 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:48,689 - INFO - Processed 1150000/1643306 records\n",
      "2025-04-21 18:07:48,718 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:48,719 - INFO - Processed 1160000/1643306 records\n",
      "2025-04-21 18:07:48,759 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:48,760 - INFO - Processed 1170000/1643306 records\n",
      "2025-04-21 18:07:48,785 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:48,786 - INFO - Processed 1180000/1643306 records\n",
      "2025-04-21 18:07:48,813 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:48,814 - INFO - Processed 1190000/1643306 records\n",
      "2025-04-21 18:07:48,853 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:48,854 - INFO - Processed 1200000/1643306 records\n",
      "2025-04-21 18:07:48,883 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:48,885 - INFO - Processed 1210000/1643306 records\n",
      "2025-04-21 18:07:48,908 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:48,909 - INFO - Processed 1220000/1643306 records\n",
      "2025-04-21 18:07:48,949 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:48,951 - INFO - Processed 1230000/1643306 records\n",
      "2025-04-21 18:07:49,001 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:49,003 - INFO - Processed 1240000/1643306 records\n",
      "2025-04-21 18:07:49,043 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:49,045 - INFO - Processed 1250000/1643306 records\n",
      "2025-04-21 18:07:49,091 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:49,093 - INFO - Processed 1260000/1643306 records\n",
      "2025-04-21 18:07:49,137 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:49,138 - INFO - Processed 1270000/1643306 records\n",
      "2025-04-21 18:07:49,159 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:49,160 - INFO - Processed 1280000/1643306 records\n",
      "2025-04-21 18:07:49,201 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:49,210 - INFO - Processed 1290000/1643306 records\n",
      "2025-04-21 18:07:49,234 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:49,236 - INFO - Processed 1300000/1643306 records\n",
      "2025-04-21 18:07:49,260 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:49,261 - INFO - Processed 1310000/1643306 records\n",
      "2025-04-21 18:07:49,310 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:49,311 - INFO - Processed 1320000/1643306 records\n",
      "2025-04-21 18:07:49,346 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:49,347 - INFO - Processed 1330000/1643306 records\n",
      "2025-04-21 18:07:49,389 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:49,390 - INFO - Processed 1340000/1643306 records\n",
      "2025-04-21 18:07:49,417 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:49,418 - INFO - Processed 1350000/1643306 records\n",
      "2025-04-21 18:07:49,465 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:49,466 - INFO - Processed 1360000/1643306 records\n",
      "2025-04-21 18:07:49,513 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:49,514 - INFO - Processed 1370000/1643306 records\n",
      "2025-04-21 18:07:49,544 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:49,546 - INFO - Processed 1380000/1643306 records\n",
      "2025-04-21 18:07:49,572 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:49,574 - INFO - Processed 1390000/1643306 records\n",
      "2025-04-21 18:07:49,597 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:49,598 - INFO - Processed 1400000/1643306 records\n",
      "2025-04-21 18:07:49,623 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:49,624 - INFO - Processed 1410000/1643306 records\n",
      "2025-04-21 18:07:49,667 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:49,668 - INFO - Processed 1420000/1643306 records\n",
      "2025-04-21 18:07:49,690 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:49,691 - INFO - Processed 1430000/1643306 records\n",
      "2025-04-21 18:07:49,731 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:49,733 - INFO - Processed 1440000/1643306 records\n",
      "2025-04-21 18:07:49,757 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:49,758 - INFO - Processed 1450000/1643306 records\n",
      "2025-04-21 18:07:49,781 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:49,783 - INFO - Processed 1460000/1643306 records\n",
      "2025-04-21 18:07:49,824 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:49,826 - INFO - Processed 1470000/1643306 records\n",
      "2025-04-21 18:07:49,870 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:49,872 - INFO - Processed 1480000/1643306 records\n",
      "2025-04-21 18:07:49,918 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:49,920 - INFO - Processed 1490000/1643306 records\n",
      "2025-04-21 18:07:49,942 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:49,944 - INFO - Processed 1500000/1643306 records\n",
      "2025-04-21 18:07:49,996 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:49,998 - INFO - Processed 1510000/1643306 records\n",
      "2025-04-21 18:07:50,042 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:50,043 - INFO - Processed 1520000/1643306 records\n",
      "2025-04-21 18:07:50,066 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:50,067 - INFO - Processed 1530000/1643306 records\n",
      "2025-04-21 18:07:50,105 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:50,107 - INFO - Processed 1540000/1643306 records\n",
      "2025-04-21 18:07:50,137 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:50,139 - INFO - Processed 1550000/1643306 records\n",
      "2025-04-21 18:07:50,181 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:50,182 - INFO - Processed 1560000/1643306 records\n",
      "2025-04-21 18:07:50,231 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:50,233 - INFO - Processed 1570000/1643306 records\n",
      "2025-04-21 18:07:50,292 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:50,293 - INFO - Processed 1580000/1643306 records\n",
      "2025-04-21 18:07:50,341 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:50,342 - INFO - Processed 1590000/1643306 records\n",
      "2025-04-21 18:07:50,388 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:50,389 - INFO - Processed 1600000/1643306 records\n",
      "2025-04-21 18:07:50,414 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:50,416 - INFO - Processed 1610000/1643306 records\n",
      "2025-04-21 18:07:50,449 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:50,450 - INFO - Processed 1620000/1643306 records\n",
      "2025-04-21 18:07:50,497 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:50,499 - INFO - Processed 1630000/1643306 records\n",
      "2025-04-21 18:07:50,544 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:50,546 - INFO - Processed 1640000/1643306 records\n",
      "2025-04-21 18:07:50,572 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:50,573 - INFO - Processed 1643306/1643306 records\n",
      "2025-04-21 18:07:50,575 - INFO - Completed customers.csv in 0:00:17.889747\n",
      "2025-04-21 18:07:50,586 - INFO - Processing transactions.csv...\n",
      "2025-04-21 18:07:52,703 - INFO - Processed DataFrame schema: root\n",
      " |-- invoice_id: string (nullable = true)\n",
      " |-- line_number: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- size: string (nullable = true)\n",
      " |-- color: string (nullable = true)\n",
      " |-- unit_price: double (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- transaction_date: string (nullable = true)\n",
      " |-- discount: double (nullable = true)\n",
      " |-- line_total: double (nullable = true)\n",
      " |-- store_id: integer (nullable = true)\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- currency_symbol: string (nullable = true)\n",
      " |-- sku: string (nullable = true)\n",
      " |-- transaction_type: string (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- invoice_total: double (nullable = true)\n",
      " |-- is_return: boolean (nullable = true)\n",
      "\n",
      "2025-04-21 18:07:54,746 - INFO - Processing 6416827 records for table transaction\n",
      "2025-04-21 18:07:55,368 - INFO - Starting to upsert 10000 records in batches of 1000\n",
      "2025-04-21 18:07:55,656 - INFO - Successfully inserted batch 1/10\n",
      "2025-04-21 18:07:55,824 - INFO - Successfully inserted batch 2/10\n",
      "2025-04-21 18:07:56,012 - INFO - Successfully inserted batch 3/10\n",
      "2025-04-21 18:07:56,153 - INFO - Successfully inserted batch 4/10\n",
      "2025-04-21 18:07:56,288 - INFO - Successfully inserted batch 5/10\n",
      "2025-04-21 18:07:56,466 - INFO - Successfully inserted batch 6/10\n",
      "2025-04-21 18:07:56,647 - INFO - Successfully inserted batch 7/10\n",
      "2025-04-21 18:07:56,836 - INFO - Successfully inserted batch 8/10\n",
      "2025-04-21 18:07:57,029 - INFO - Successfully inserted batch 9/10\n",
      "2025-04-21 18:07:57,246 - INFO - Successfully inserted batch 10/10\n",
      "2025-04-21 18:07:57,246 - INFO - Processed 10000/6416827 records\n",
      "2025-04-21 18:07:57,284 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:57,286 - INFO - Processed 20000/6416827 records\n",
      "2025-04-21 18:07:57,318 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:57,320 - INFO - Processed 30000/6416827 records\n",
      "2025-04-21 18:07:57,380 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:57,384 - INFO - Processed 40000/6416827 records\n",
      "2025-04-21 18:07:57,446 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:57,448 - INFO - Processed 50000/6416827 records\n",
      "2025-04-21 18:07:57,503 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:57,504 - INFO - Processed 60000/6416827 records\n",
      "2025-04-21 18:07:57,549 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:57,551 - INFO - Processed 70000/6416827 records\n",
      "2025-04-21 18:07:57,596 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:57,597 - INFO - Processed 80000/6416827 records\n",
      "2025-04-21 18:07:57,648 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:57,649 - INFO - Processed 90000/6416827 records\n",
      "2025-04-21 18:07:57,681 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:57,683 - INFO - Processed 100000/6416827 records\n",
      "2025-04-21 18:07:57,736 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:57,738 - INFO - Processed 110000/6416827 records\n",
      "2025-04-21 18:07:57,771 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:57,773 - INFO - Processed 120000/6416827 records\n",
      "2025-04-21 18:07:57,814 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:57,815 - INFO - Processed 130000/6416827 records\n",
      "2025-04-21 18:07:57,878 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:57,880 - INFO - Processed 140000/6416827 records\n",
      "2025-04-21 18:07:57,917 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:57,918 - INFO - Processed 150000/6416827 records\n",
      "2025-04-21 18:07:57,971 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:57,973 - INFO - Processed 160000/6416827 records\n",
      "2025-04-21 18:07:58,019 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:58,021 - INFO - Processed 170000/6416827 records\n",
      "2025-04-21 18:07:58,058 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:58,059 - INFO - Processed 180000/6416827 records\n",
      "2025-04-21 18:07:58,110 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:58,112 - INFO - Processed 190000/6416827 records\n",
      "2025-04-21 18:07:58,143 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:58,145 - INFO - Processed 200000/6416827 records\n",
      "2025-04-21 18:07:58,179 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:58,180 - INFO - Processed 210000/6416827 records\n",
      "2025-04-21 18:07:58,210 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:58,211 - INFO - Processed 220000/6416827 records\n",
      "2025-04-21 18:07:58,265 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:58,267 - INFO - Processed 230000/6416827 records\n",
      "2025-04-21 18:07:58,312 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:58,313 - INFO - Processed 240000/6416827 records\n",
      "2025-04-21 18:07:58,345 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:58,346 - INFO - Processed 250000/6416827 records\n",
      "2025-04-21 18:07:58,392 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:58,394 - INFO - Processed 260000/6416827 records\n",
      "2025-04-21 18:07:58,440 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:58,442 - INFO - Processed 270000/6416827 records\n",
      "2025-04-21 18:07:58,469 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:58,471 - INFO - Processed 280000/6416827 records\n",
      "2025-04-21 18:07:58,500 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:58,502 - INFO - Processed 290000/6416827 records\n",
      "2025-04-21 18:07:58,546 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:58,547 - INFO - Processed 300000/6416827 records\n",
      "2025-04-21 18:07:58,575 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:58,577 - INFO - Processed 310000/6416827 records\n",
      "2025-04-21 18:07:58,611 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:58,613 - INFO - Processed 320000/6416827 records\n",
      "2025-04-21 18:07:58,657 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:58,658 - INFO - Processed 330000/6416827 records\n",
      "2025-04-21 18:07:58,690 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:58,691 - INFO - Processed 340000/6416827 records\n",
      "2025-04-21 18:07:58,749 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:58,750 - INFO - Processed 350000/6416827 records\n",
      "2025-04-21 18:07:58,779 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:58,781 - INFO - Processed 360000/6416827 records\n",
      "2025-04-21 18:07:58,828 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:58,829 - INFO - Processed 370000/6416827 records\n",
      "2025-04-21 18:07:58,862 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:58,864 - INFO - Processed 380000/6416827 records\n",
      "2025-04-21 18:07:58,893 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:58,894 - INFO - Processed 390000/6416827 records\n",
      "2025-04-21 18:07:58,939 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:58,940 - INFO - Processed 400000/6416827 records\n",
      "2025-04-21 18:07:58,969 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:58,971 - INFO - Processed 410000/6416827 records\n",
      "2025-04-21 18:07:59,018 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:59,019 - INFO - Processed 420000/6416827 records\n",
      "2025-04-21 18:07:59,049 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:59,051 - INFO - Processed 430000/6416827 records\n",
      "2025-04-21 18:07:59,079 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:59,082 - INFO - Processed 440000/6416827 records\n",
      "2025-04-21 18:07:59,112 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:59,113 - INFO - Processed 450000/6416827 records\n",
      "2025-04-21 18:07:59,146 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:59,148 - INFO - Processed 460000/6416827 records\n",
      "2025-04-21 18:07:59,194 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:59,195 - INFO - Processed 470000/6416827 records\n",
      "2025-04-21 18:07:59,225 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:59,226 - INFO - Processed 480000/6416827 records\n",
      "2025-04-21 18:07:59,253 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:59,255 - INFO - Processed 490000/6416827 records\n",
      "2025-04-21 18:07:59,304 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:59,305 - INFO - Processed 500000/6416827 records\n",
      "2025-04-21 18:07:59,336 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:59,338 - INFO - Processed 510000/6416827 records\n",
      "2025-04-21 18:07:59,367 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:59,369 - INFO - Processed 520000/6416827 records\n",
      "2025-04-21 18:07:59,397 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:59,398 - INFO - Processed 530000/6416827 records\n",
      "2025-04-21 18:07:59,428 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:59,430 - INFO - Processed 540000/6416827 records\n",
      "2025-04-21 18:07:59,474 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:59,476 - INFO - Processed 550000/6416827 records\n",
      "2025-04-21 18:07:59,510 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:59,511 - INFO - Processed 560000/6416827 records\n",
      "2025-04-21 18:07:59,540 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:59,542 - INFO - Processed 570000/6416827 records\n",
      "2025-04-21 18:07:59,574 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:59,576 - INFO - Processed 580000/6416827 records\n",
      "2025-04-21 18:07:59,630 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:59,631 - INFO - Processed 590000/6416827 records\n",
      "2025-04-21 18:07:59,662 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:59,663 - INFO - Processed 600000/6416827 records\n",
      "2025-04-21 18:07:59,694 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:59,696 - INFO - Processed 610000/6416827 records\n",
      "2025-04-21 18:07:59,743 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:59,745 - INFO - Processed 620000/6416827 records\n",
      "2025-04-21 18:07:59,802 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:59,804 - INFO - Processed 630000/6416827 records\n",
      "2025-04-21 18:07:59,850 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:59,851 - INFO - Processed 640000/6416827 records\n",
      "2025-04-21 18:07:59,898 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:59,899 - INFO - Processed 650000/6416827 records\n",
      "2025-04-21 18:07:59,943 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:59,944 - INFO - Processed 660000/6416827 records\n",
      "2025-04-21 18:07:59,974 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:07:59,976 - INFO - Processed 670000/6416827 records\n",
      "2025-04-21 18:08:00,008 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:00,010 - INFO - Processed 680000/6416827 records\n",
      "2025-04-21 18:08:00,067 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:00,068 - INFO - Processed 690000/6416827 records\n",
      "2025-04-21 18:08:00,097 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:00,098 - INFO - Processed 700000/6416827 records\n",
      "2025-04-21 18:08:00,125 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:00,127 - INFO - Processed 710000/6416827 records\n",
      "2025-04-21 18:08:00,192 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:00,194 - INFO - Processed 720000/6416827 records\n",
      "2025-04-21 18:08:00,237 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:00,238 - INFO - Processed 730000/6416827 records\n",
      "2025-04-21 18:08:00,286 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:00,288 - INFO - Processed 740000/6416827 records\n",
      "2025-04-21 18:08:00,334 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:00,336 - INFO - Processed 750000/6416827 records\n",
      "2025-04-21 18:08:00,386 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:00,388 - INFO - Processed 760000/6416827 records\n",
      "2025-04-21 18:08:00,444 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:00,445 - INFO - Processed 770000/6416827 records\n",
      "2025-04-21 18:08:00,490 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:00,492 - INFO - Processed 780000/6416827 records\n",
      "2025-04-21 18:08:00,530 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:00,531 - INFO - Processed 790000/6416827 records\n",
      "2025-04-21 18:08:00,561 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:00,563 - INFO - Processed 800000/6416827 records\n",
      "2025-04-21 18:08:00,588 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:00,589 - INFO - Processed 810000/6416827 records\n",
      "2025-04-21 18:08:00,618 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:00,620 - INFO - Processed 820000/6416827 records\n",
      "2025-04-21 18:08:00,673 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:00,675 - INFO - Processed 830000/6416827 records\n",
      "2025-04-21 18:08:00,707 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:00,709 - INFO - Processed 840000/6416827 records\n",
      "2025-04-21 18:08:00,782 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:00,784 - INFO - Processed 850000/6416827 records\n",
      "2025-04-21 18:08:00,819 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:00,821 - INFO - Processed 860000/6416827 records\n",
      "2025-04-21 18:08:00,850 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:00,851 - INFO - Processed 870000/6416827 records\n",
      "2025-04-21 18:08:00,891 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:00,893 - INFO - Processed 880000/6416827 records\n",
      "2025-04-21 18:08:00,928 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:00,930 - INFO - Processed 890000/6416827 records\n",
      "2025-04-21 18:08:00,969 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:00,973 - INFO - Processed 900000/6416827 records\n",
      "2025-04-21 18:08:01,015 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:01,017 - INFO - Processed 910000/6416827 records\n",
      "2025-04-21 18:08:01,065 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:01,067 - INFO - Processed 920000/6416827 records\n",
      "2025-04-21 18:08:01,099 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:01,100 - INFO - Processed 930000/6416827 records\n",
      "2025-04-21 18:08:01,159 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:01,160 - INFO - Processed 940000/6416827 records\n",
      "2025-04-21 18:08:01,205 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:01,206 - INFO - Processed 950000/6416827 records\n",
      "2025-04-21 18:08:01,254 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:01,256 - INFO - Processed 960000/6416827 records\n",
      "2025-04-21 18:08:01,282 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:01,283 - INFO - Processed 970000/6416827 records\n",
      "2025-04-21 18:08:01,311 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:01,312 - INFO - Processed 980000/6416827 records\n",
      "2025-04-21 18:08:01,361 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:01,362 - INFO - Processed 990000/6416827 records\n",
      "2025-04-21 18:08:01,390 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:01,391 - INFO - Processed 1000000/6416827 records\n",
      "2025-04-21 18:08:01,419 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:01,420 - INFO - Processed 1010000/6416827 records\n",
      "2025-04-21 18:08:01,450 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:01,451 - INFO - Processed 1020000/6416827 records\n",
      "2025-04-21 18:08:01,481 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:01,483 - INFO - Processed 1030000/6416827 records\n",
      "2025-04-21 18:08:01,533 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:01,535 - INFO - Processed 1040000/6416827 records\n",
      "2025-04-21 18:08:01,581 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:01,584 - INFO - Processed 1050000/6416827 records\n",
      "2025-04-21 18:08:01,641 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:01,643 - INFO - Processed 1060000/6416827 records\n",
      "2025-04-21 18:08:01,688 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:01,690 - INFO - Processed 1070000/6416827 records\n",
      "2025-04-21 18:08:01,717 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:01,718 - INFO - Processed 1080000/6416827 records\n",
      "2025-04-21 18:08:01,768 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:01,769 - INFO - Processed 1090000/6416827 records\n",
      "2025-04-21 18:08:01,814 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:01,815 - INFO - Processed 1100000/6416827 records\n",
      "2025-04-21 18:08:01,851 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:01,852 - INFO - Processed 1110000/6416827 records\n",
      "2025-04-21 18:08:01,906 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:01,907 - INFO - Processed 1120000/6416827 records\n",
      "2025-04-21 18:08:01,954 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:01,956 - INFO - Processed 1130000/6416827 records\n",
      "2025-04-21 18:08:02,000 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:02,003 - INFO - Processed 1140000/6416827 records\n",
      "2025-04-21 18:08:02,031 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:02,033 - INFO - Processed 1150000/6416827 records\n",
      "2025-04-21 18:08:02,060 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:02,062 - INFO - Processed 1160000/6416827 records\n",
      "2025-04-21 18:08:02,087 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:02,089 - INFO - Processed 1170000/6416827 records\n",
      "2025-04-21 18:08:02,115 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:02,117 - INFO - Processed 1180000/6416827 records\n",
      "2025-04-21 18:08:02,145 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:02,147 - INFO - Processed 1190000/6416827 records\n",
      "2025-04-21 18:08:02,182 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:02,183 - INFO - Processed 1200000/6416827 records\n",
      "2025-04-21 18:08:02,243 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:02,244 - INFO - Processed 1210000/6416827 records\n",
      "2025-04-21 18:08:02,286 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:02,287 - INFO - Processed 1220000/6416827 records\n",
      "2025-04-21 18:08:02,333 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:02,336 - INFO - Processed 1230000/6416827 records\n",
      "2025-04-21 18:08:02,362 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:02,364 - INFO - Processed 1240000/6416827 records\n",
      "2025-04-21 18:08:02,394 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:02,395 - INFO - Processed 1250000/6416827 records\n",
      "2025-04-21 18:08:02,424 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:02,425 - INFO - Processed 1260000/6416827 records\n",
      "2025-04-21 18:08:02,475 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:02,476 - INFO - Processed 1270000/6416827 records\n",
      "2025-04-21 18:08:02,505 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:02,506 - INFO - Processed 1280000/6416827 records\n",
      "2025-04-21 18:08:02,535 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:02,536 - INFO - Processed 1290000/6416827 records\n",
      "2025-04-21 18:08:02,584 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:02,585 - INFO - Processed 1300000/6416827 records\n",
      "2025-04-21 18:08:02,614 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:02,615 - INFO - Processed 1310000/6416827 records\n",
      "2025-04-21 18:08:02,659 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:02,660 - INFO - Processed 1320000/6416827 records\n",
      "2025-04-21 18:08:02,705 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:02,706 - INFO - Processed 1330000/6416827 records\n",
      "2025-04-21 18:08:02,733 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:02,735 - INFO - Processed 1340000/6416827 records\n",
      "2025-04-21 18:08:02,785 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:02,787 - INFO - Processed 1350000/6416827 records\n",
      "2025-04-21 18:08:02,835 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:02,836 - INFO - Processed 1360000/6416827 records\n",
      "2025-04-21 18:08:02,879 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:02,881 - INFO - Processed 1370000/6416827 records\n",
      "2025-04-21 18:08:02,920 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:02,922 - INFO - Processed 1380000/6416827 records\n",
      "2025-04-21 18:08:02,973 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:02,974 - INFO - Processed 1390000/6416827 records\n",
      "2025-04-21 18:08:03,020 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:03,022 - INFO - Processed 1400000/6416827 records\n",
      "2025-04-21 18:08:03,065 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:03,069 - INFO - Processed 1410000/6416827 records\n",
      "2025-04-21 18:08:03,097 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:03,099 - INFO - Processed 1420000/6416827 records\n",
      "2025-04-21 18:08:03,125 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:03,127 - INFO - Processed 1430000/6416827 records\n",
      "2025-04-21 18:08:03,156 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:03,158 - INFO - Processed 1440000/6416827 records\n",
      "2025-04-21 18:08:03,186 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:03,188 - INFO - Processed 1450000/6416827 records\n",
      "2025-04-21 18:08:03,240 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:03,241 - INFO - Processed 1460000/6416827 records\n",
      "2025-04-21 18:08:03,287 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:03,289 - INFO - Processed 1470000/6416827 records\n",
      "2025-04-21 18:08:03,318 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:03,319 - INFO - Processed 1480000/6416827 records\n",
      "2025-04-21 18:08:03,347 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:03,348 - INFO - Processed 1490000/6416827 records\n",
      "2025-04-21 18:08:03,376 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:03,377 - INFO - Processed 1500000/6416827 records\n",
      "2025-04-21 18:08:03,407 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:03,409 - INFO - Processed 1510000/6416827 records\n",
      "2025-04-21 18:08:03,458 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:03,459 - INFO - Processed 1520000/6416827 records\n",
      "2025-04-21 18:08:03,488 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:03,490 - INFO - Processed 1530000/6416827 records\n",
      "2025-04-21 18:08:03,536 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:03,538 - INFO - Processed 1540000/6416827 records\n",
      "2025-04-21 18:08:03,583 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:03,585 - INFO - Processed 1550000/6416827 records\n",
      "2025-04-21 18:08:03,614 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:03,617 - INFO - Processed 1560000/6416827 records\n",
      "2025-04-21 18:08:03,661 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:03,664 - INFO - Processed 1570000/6416827 records\n",
      "2025-04-21 18:08:03,707 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:03,708 - INFO - Processed 1580000/6416827 records\n",
      "2025-04-21 18:08:03,754 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:03,755 - INFO - Processed 1590000/6416827 records\n",
      "2025-04-21 18:08:03,801 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:03,802 - INFO - Processed 1600000/6416827 records\n",
      "2025-04-21 18:08:03,832 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:03,833 - INFO - Processed 1610000/6416827 records\n",
      "2025-04-21 18:08:03,861 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:03,862 - INFO - Processed 1620000/6416827 records\n",
      "2025-04-21 18:08:03,895 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:03,898 - INFO - Processed 1630000/6416827 records\n",
      "2025-04-21 18:08:03,941 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:03,943 - INFO - Processed 1640000/6416827 records\n",
      "2025-04-21 18:08:03,971 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:03,972 - INFO - Processed 1650000/6416827 records\n",
      "2025-04-21 18:08:04,022 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:04,024 - INFO - Processed 1660000/6416827 records\n",
      "2025-04-21 18:08:04,070 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:04,072 - INFO - Processed 1670000/6416827 records\n",
      "2025-04-21 18:08:04,130 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:04,132 - INFO - Processed 1680000/6416827 records\n",
      "2025-04-21 18:08:04,175 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:04,177 - INFO - Processed 1690000/6416827 records\n",
      "2025-04-21 18:08:04,223 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:04,225 - INFO - Processed 1700000/6416827 records\n",
      "2025-04-21 18:08:04,269 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:04,271 - INFO - Processed 1710000/6416827 records\n",
      "2025-04-21 18:08:04,299 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:04,300 - INFO - Processed 1720000/6416827 records\n",
      "2025-04-21 18:08:04,332 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:04,333 - INFO - Processed 1730000/6416827 records\n",
      "2025-04-21 18:08:04,360 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:04,362 - INFO - Processed 1740000/6416827 records\n",
      "2025-04-21 18:08:04,458 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:04,460 - INFO - Processed 1750000/6416827 records\n",
      "2025-04-21 18:08:04,494 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:04,495 - INFO - Processed 1760000/6416827 records\n",
      "2025-04-21 18:08:04,535 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:04,536 - INFO - Processed 1770000/6416827 records\n",
      "2025-04-21 18:08:04,580 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:04,581 - INFO - Processed 1780000/6416827 records\n",
      "2025-04-21 18:08:04,629 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:04,630 - INFO - Processed 1790000/6416827 records\n",
      "2025-04-21 18:08:04,657 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:04,658 - INFO - Processed 1800000/6416827 records\n",
      "2025-04-21 18:08:04,688 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:04,689 - INFO - Processed 1810000/6416827 records\n",
      "2025-04-21 18:08:04,718 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:04,720 - INFO - Processed 1820000/6416827 records\n",
      "2025-04-21 18:08:04,744 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:04,746 - INFO - Processed 1830000/6416827 records\n",
      "2025-04-21 18:08:04,803 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:04,804 - INFO - Processed 1840000/6416827 records\n",
      "2025-04-21 18:08:04,851 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:04,852 - INFO - Processed 1850000/6416827 records\n",
      "2025-04-21 18:08:04,882 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:04,885 - INFO - Processed 1860000/6416827 records\n",
      "2025-04-21 18:08:04,924 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:04,926 - INFO - Processed 1870000/6416827 records\n",
      "2025-04-21 18:08:04,971 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:04,974 - INFO - Processed 1880000/6416827 records\n",
      "2025-04-21 18:08:05,002 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:05,003 - INFO - Processed 1890000/6416827 records\n",
      "2025-04-21 18:08:05,069 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:05,070 - INFO - Processed 1900000/6416827 records\n",
      "2025-04-21 18:08:05,102 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:05,104 - INFO - Processed 1910000/6416827 records\n",
      "2025-04-21 18:08:05,158 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:05,159 - INFO - Processed 1920000/6416827 records\n",
      "2025-04-21 18:08:05,205 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:05,207 - INFO - Processed 1930000/6416827 records\n",
      "2025-04-21 18:08:05,234 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:05,235 - INFO - Processed 1940000/6416827 records\n",
      "2025-04-21 18:08:05,264 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:05,266 - INFO - Processed 1950000/6416827 records\n",
      "2025-04-21 18:08:05,318 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:05,319 - INFO - Processed 1960000/6416827 records\n",
      "2025-04-21 18:08:05,379 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:05,381 - INFO - Processed 1970000/6416827 records\n",
      "2025-04-21 18:08:05,409 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:05,411 - INFO - Processed 1980000/6416827 records\n",
      "2025-04-21 18:08:05,442 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:05,444 - INFO - Processed 1990000/6416827 records\n",
      "2025-04-21 18:08:05,488 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:05,490 - INFO - Processed 2000000/6416827 records\n",
      "2025-04-21 18:08:05,537 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:05,539 - INFO - Processed 2010000/6416827 records\n",
      "2025-04-21 18:08:05,583 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:05,585 - INFO - Processed 2020000/6416827 records\n",
      "2025-04-21 18:08:05,630 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:05,632 - INFO - Processed 2030000/6416827 records\n",
      "2025-04-21 18:08:05,679 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:05,681 - INFO - Processed 2040000/6416827 records\n",
      "2025-04-21 18:08:05,709 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:05,710 - INFO - Processed 2050000/6416827 records\n",
      "2025-04-21 18:08:05,739 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:05,741 - INFO - Processed 2060000/6416827 records\n",
      "2025-04-21 18:08:05,767 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:05,770 - INFO - Processed 2070000/6416827 records\n",
      "2025-04-21 18:08:05,797 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:05,798 - INFO - Processed 2080000/6416827 records\n",
      "2025-04-21 18:08:05,827 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:05,828 - INFO - Processed 2090000/6416827 records\n",
      "2025-04-21 18:08:05,854 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:05,856 - INFO - Processed 2100000/6416827 records\n",
      "2025-04-21 18:08:05,895 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:05,896 - INFO - Processed 2110000/6416827 records\n",
      "2025-04-21 18:08:05,926 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:05,927 - INFO - Processed 2120000/6416827 records\n",
      "2025-04-21 18:08:05,974 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:05,977 - INFO - Processed 2130000/6416827 records\n",
      "2025-04-21 18:08:06,022 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:06,023 - INFO - Processed 2140000/6416827 records\n",
      "2025-04-21 18:08:06,066 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:06,068 - INFO - Processed 2150000/6416827 records\n",
      "2025-04-21 18:08:06,113 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:06,114 - INFO - Processed 2160000/6416827 records\n",
      "2025-04-21 18:08:06,145 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:06,146 - INFO - Processed 2170000/6416827 records\n",
      "2025-04-21 18:08:06,198 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:06,201 - INFO - Processed 2180000/6416827 records\n",
      "2025-04-21 18:08:06,256 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:06,257 - INFO - Processed 2190000/6416827 records\n",
      "2025-04-21 18:08:06,304 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:06,306 - INFO - Processed 2200000/6416827 records\n",
      "2025-04-21 18:08:06,353 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:06,354 - INFO - Processed 2210000/6416827 records\n",
      "2025-04-21 18:08:06,399 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:06,401 - INFO - Processed 2220000/6416827 records\n",
      "2025-04-21 18:08:06,435 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:06,437 - INFO - Processed 2230000/6416827 records\n",
      "2025-04-21 18:08:06,467 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:06,468 - INFO - Processed 2240000/6416827 records\n",
      "2025-04-21 18:08:06,524 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:06,525 - INFO - Processed 2250000/6416827 records\n",
      "2025-04-21 18:08:06,558 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:06,559 - INFO - Processed 2260000/6416827 records\n",
      "2025-04-21 18:08:06,600 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:06,601 - INFO - Processed 2270000/6416827 records\n",
      "2025-04-21 18:08:06,646 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:06,648 - INFO - Processed 2280000/6416827 records\n",
      "2025-04-21 18:08:06,676 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:06,678 - INFO - Processed 2290000/6416827 records\n",
      "2025-04-21 18:08:06,704 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:06,706 - INFO - Processed 2300000/6416827 records\n",
      "2025-04-21 18:08:06,733 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:06,735 - INFO - Processed 2310000/6416827 records\n",
      "2025-04-21 18:08:06,765 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:06,766 - INFO - Processed 2320000/6416827 records\n",
      "2025-04-21 18:08:06,796 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:06,798 - INFO - Processed 2330000/6416827 records\n",
      "2025-04-21 18:08:06,825 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:06,826 - INFO - Processed 2340000/6416827 records\n",
      "2025-04-21 18:08:06,863 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:06,865 - INFO - Processed 2350000/6416827 records\n",
      "2025-04-21 18:08:06,912 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:06,914 - INFO - Processed 2360000/6416827 records\n",
      "2025-04-21 18:08:06,943 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:06,944 - INFO - Processed 2370000/6416827 records\n",
      "2025-04-21 18:08:06,973 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:06,976 - INFO - Processed 2380000/6416827 records\n",
      "2025-04-21 18:08:07,020 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:07,022 - INFO - Processed 2390000/6416827 records\n",
      "2025-04-21 18:08:07,052 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:07,053 - INFO - Processed 2400000/6416827 records\n",
      "2025-04-21 18:08:07,081 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:07,082 - INFO - Processed 2410000/6416827 records\n",
      "2025-04-21 18:08:07,110 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:07,111 - INFO - Processed 2420000/6416827 records\n",
      "2025-04-21 18:08:07,140 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:07,141 - INFO - Processed 2430000/6416827 records\n",
      "2025-04-21 18:08:07,191 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:07,192 - INFO - Processed 2440000/6416827 records\n",
      "2025-04-21 18:08:07,238 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:07,240 - INFO - Processed 2450000/6416827 records\n",
      "2025-04-21 18:08:07,285 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:07,286 - INFO - Processed 2460000/6416827 records\n",
      "2025-04-21 18:08:07,313 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:07,315 - INFO - Processed 2470000/6416827 records\n",
      "2025-04-21 18:08:07,348 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:07,350 - INFO - Processed 2480000/6416827 records\n",
      "2025-04-21 18:08:07,393 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:07,395 - INFO - Processed 2490000/6416827 records\n",
      "2025-04-21 18:08:07,423 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:07,425 - INFO - Processed 2500000/6416827 records\n",
      "2025-04-21 18:08:07,454 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:07,456 - INFO - Processed 2510000/6416827 records\n",
      "2025-04-21 18:08:07,491 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:07,493 - INFO - Processed 2520000/6416827 records\n",
      "2025-04-21 18:08:07,535 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:07,537 - INFO - Processed 2530000/6416827 records\n",
      "2025-04-21 18:08:07,583 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:07,584 - INFO - Processed 2540000/6416827 records\n",
      "2025-04-21 18:08:07,612 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:07,613 - INFO - Processed 2550000/6416827 records\n",
      "2025-04-21 18:08:07,663 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:07,665 - INFO - Processed 2560000/6416827 records\n",
      "2025-04-21 18:08:07,708 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:07,710 - INFO - Processed 2570000/6416827 records\n",
      "2025-04-21 18:08:07,737 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:07,738 - INFO - Processed 2580000/6416827 records\n",
      "2025-04-21 18:08:07,765 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:07,766 - INFO - Processed 2590000/6416827 records\n",
      "2025-04-21 18:08:07,793 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:07,794 - INFO - Processed 2600000/6416827 records\n",
      "2025-04-21 18:08:07,823 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:07,824 - INFO - Processed 2610000/6416827 records\n",
      "2025-04-21 18:08:07,848 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:07,850 - INFO - Processed 2620000/6416827 records\n",
      "2025-04-21 18:08:07,899 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:07,901 - INFO - Processed 2630000/6416827 records\n",
      "2025-04-21 18:08:07,948 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:07,949 - INFO - Processed 2640000/6416827 records\n",
      "2025-04-21 18:08:07,993 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:07,996 - INFO - Processed 2650000/6416827 records\n",
      "2025-04-21 18:08:08,030 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:08,032 - INFO - Processed 2660000/6416827 records\n",
      "2025-04-21 18:08:08,062 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:08,064 - INFO - Processed 2670000/6416827 records\n",
      "2025-04-21 18:08:08,102 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:08,105 - INFO - Processed 2680000/6416827 records\n",
      "2025-04-21 18:08:08,136 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:08,137 - INFO - Processed 2690000/6416827 records\n",
      "2025-04-21 18:08:08,167 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:08,168 - INFO - Processed 2700000/6416827 records\n",
      "2025-04-21 18:08:08,196 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:08,197 - INFO - Processed 2710000/6416827 records\n",
      "2025-04-21 18:08:08,225 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:08,227 - INFO - Processed 2720000/6416827 records\n",
      "2025-04-21 18:08:08,273 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:08,276 - INFO - Processed 2730000/6416827 records\n",
      "2025-04-21 18:08:08,324 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:08,326 - INFO - Processed 2740000/6416827 records\n",
      "2025-04-21 18:08:08,356 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:08,357 - INFO - Processed 2750000/6416827 records\n",
      "2025-04-21 18:08:08,403 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:08,405 - INFO - Processed 2760000/6416827 records\n",
      "2025-04-21 18:08:08,433 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:08,434 - INFO - Processed 2770000/6416827 records\n",
      "2025-04-21 18:08:08,462 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:08,463 - INFO - Processed 2780000/6416827 records\n",
      "2025-04-21 18:08:08,493 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:08,495 - INFO - Processed 2790000/6416827 records\n",
      "2025-04-21 18:08:08,541 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:08,544 - INFO - Processed 2800000/6416827 records\n",
      "2025-04-21 18:08:08,576 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:08,578 - INFO - Processed 2810000/6416827 records\n",
      "2025-04-21 18:08:08,606 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:08,608 - INFO - Processed 2820000/6416827 records\n",
      "2025-04-21 18:08:08,651 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:08,653 - INFO - Processed 2830000/6416827 records\n",
      "2025-04-21 18:08:08,698 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:08,699 - INFO - Processed 2840000/6416827 records\n",
      "2025-04-21 18:08:08,746 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:08,748 - INFO - Processed 2850000/6416827 records\n",
      "2025-04-21 18:08:08,781 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:08,783 - INFO - Processed 2860000/6416827 records\n",
      "2025-04-21 18:08:08,824 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:08,825 - INFO - Processed 2870000/6416827 records\n",
      "2025-04-21 18:08:08,874 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:08,877 - INFO - Processed 2880000/6416827 records\n",
      "2025-04-21 18:08:08,906 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:08,908 - INFO - Processed 2890000/6416827 records\n",
      "2025-04-21 18:08:08,936 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:08,938 - INFO - Processed 2900000/6416827 records\n",
      "2025-04-21 18:08:08,964 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:08,965 - INFO - Processed 2910000/6416827 records\n",
      "2025-04-21 18:08:09,012 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:09,013 - INFO - Processed 2920000/6416827 records\n",
      "2025-04-21 18:08:09,062 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:09,064 - INFO - Processed 2930000/6416827 records\n",
      "2025-04-21 18:08:09,101 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:09,103 - INFO - Processed 2940000/6416827 records\n",
      "2025-04-21 18:08:09,156 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:09,158 - INFO - Processed 2950000/6416827 records\n",
      "2025-04-21 18:08:09,206 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:09,208 - INFO - Processed 2960000/6416827 records\n",
      "2025-04-21 18:08:09,255 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:09,256 - INFO - Processed 2970000/6416827 records\n",
      "2025-04-21 18:08:09,296 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:09,297 - INFO - Processed 2980000/6416827 records\n",
      "2025-04-21 18:08:09,326 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:09,327 - INFO - Processed 2990000/6416827 records\n",
      "2025-04-21 18:08:09,371 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:09,373 - INFO - Processed 3000000/6416827 records\n",
      "2025-04-21 18:08:09,419 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:09,420 - INFO - Processed 3010000/6416827 records\n",
      "2025-04-21 18:08:09,448 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:09,450 - INFO - Processed 3020000/6416827 records\n",
      "2025-04-21 18:08:09,478 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:09,479 - INFO - Processed 3030000/6416827 records\n",
      "2025-04-21 18:08:09,506 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:09,507 - INFO - Processed 3040000/6416827 records\n",
      "2025-04-21 18:08:09,536 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:09,537 - INFO - Processed 3050000/6416827 records\n",
      "2025-04-21 18:08:09,566 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:09,568 - INFO - Processed 3060000/6416827 records\n",
      "2025-04-21 18:08:09,597 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:09,600 - INFO - Processed 3070000/6416827 records\n",
      "2025-04-21 18:08:09,626 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:09,627 - INFO - Processed 3080000/6416827 records\n",
      "2025-04-21 18:08:09,653 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:09,660 - INFO - Processed 3090000/6416827 records\n",
      "2025-04-21 18:08:09,688 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:09,690 - INFO - Processed 3100000/6416827 records\n",
      "2025-04-21 18:08:09,738 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:09,740 - INFO - Processed 3110000/6416827 records\n",
      "2025-04-21 18:08:09,774 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:09,776 - INFO - Processed 3120000/6416827 records\n",
      "2025-04-21 18:08:09,827 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:09,829 - INFO - Processed 3130000/6416827 records\n",
      "2025-04-21 18:08:09,876 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:09,878 - INFO - Processed 3140000/6416827 records\n",
      "2025-04-21 18:08:09,920 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:09,922 - INFO - Processed 3150000/6416827 records\n",
      "2025-04-21 18:08:09,950 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:09,951 - INFO - Processed 3160000/6416827 records\n",
      "2025-04-21 18:08:09,978 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:09,980 - INFO - Processed 3170000/6416827 records\n",
      "2025-04-21 18:08:10,007 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:10,009 - INFO - Processed 3180000/6416827 records\n",
      "2025-04-21 18:08:10,035 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:10,037 - INFO - Processed 3190000/6416827 records\n",
      "2025-04-21 18:08:10,063 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:10,064 - INFO - Processed 3200000/6416827 records\n",
      "2025-04-21 18:08:10,092 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:10,094 - INFO - Processed 3210000/6416827 records\n",
      "2025-04-21 18:08:10,121 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:10,122 - INFO - Processed 3220000/6416827 records\n",
      "2025-04-21 18:08:10,151 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:10,153 - INFO - Processed 3230000/6416827 records\n",
      "2025-04-21 18:08:10,203 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:10,206 - INFO - Processed 3240000/6416827 records\n",
      "2025-04-21 18:08:10,234 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:10,236 - INFO - Processed 3250000/6416827 records\n",
      "2025-04-21 18:08:10,265 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:10,266 - INFO - Processed 3260000/6416827 records\n",
      "2025-04-21 18:08:10,296 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:10,298 - INFO - Processed 3270000/6416827 records\n",
      "2025-04-21 18:08:10,328 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:10,329 - INFO - Processed 3280000/6416827 records\n",
      "2025-04-21 18:08:10,375 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:10,376 - INFO - Processed 3290000/6416827 records\n",
      "2025-04-21 18:08:10,404 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:10,405 - INFO - Processed 3300000/6416827 records\n",
      "2025-04-21 18:08:10,454 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:10,455 - INFO - Processed 3310000/6416827 records\n",
      "2025-04-21 18:08:10,507 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:10,508 - INFO - Processed 3320000/6416827 records\n",
      "2025-04-21 18:08:10,564 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:10,565 - INFO - Processed 3330000/6416827 records\n",
      "2025-04-21 18:08:10,593 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:10,597 - INFO - Processed 3340000/6416827 records\n",
      "2025-04-21 18:08:10,624 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:10,626 - INFO - Processed 3350000/6416827 records\n",
      "2025-04-21 18:08:10,652 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:10,653 - INFO - Processed 3360000/6416827 records\n",
      "2025-04-21 18:08:10,691 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:10,692 - INFO - Processed 3370000/6416827 records\n",
      "2025-04-21 18:08:10,726 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:10,728 - INFO - Processed 3380000/6416827 records\n",
      "2025-04-21 18:08:10,756 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:10,758 - INFO - Processed 3390000/6416827 records\n",
      "2025-04-21 18:08:10,801 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:10,802 - INFO - Processed 3400000/6416827 records\n",
      "2025-04-21 18:08:10,847 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:10,849 - INFO - Processed 3410000/6416827 records\n",
      "2025-04-21 18:08:10,875 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:10,877 - INFO - Processed 3420000/6416827 records\n",
      "2025-04-21 18:08:10,906 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:10,907 - INFO - Processed 3430000/6416827 records\n",
      "2025-04-21 18:08:10,936 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:10,938 - INFO - Processed 3440000/6416827 records\n",
      "2025-04-21 18:08:10,966 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:10,968 - INFO - Processed 3450000/6416827 records\n",
      "2025-04-21 18:08:11,022 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:11,023 - INFO - Processed 3460000/6416827 records\n",
      "2025-04-21 18:08:11,056 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:11,058 - INFO - Processed 3470000/6416827 records\n",
      "2025-04-21 18:08:11,101 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:11,103 - INFO - Processed 3480000/6416827 records\n",
      "2025-04-21 18:08:11,142 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:11,144 - INFO - Processed 3490000/6416827 records\n",
      "2025-04-21 18:08:11,177 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:11,178 - INFO - Processed 3500000/6416827 records\n",
      "2025-04-21 18:08:11,205 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:11,206 - INFO - Processed 3510000/6416827 records\n",
      "2025-04-21 18:08:11,234 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:11,235 - INFO - Processed 3520000/6416827 records\n",
      "2025-04-21 18:08:11,263 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:11,264 - INFO - Processed 3530000/6416827 records\n",
      "2025-04-21 18:08:11,304 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:11,305 - INFO - Processed 3540000/6416827 records\n",
      "2025-04-21 18:08:11,331 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:11,334 - INFO - Processed 3550000/6416827 records\n",
      "2025-04-21 18:08:11,382 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:11,384 - INFO - Processed 3560000/6416827 records\n",
      "2025-04-21 18:08:11,434 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:11,436 - INFO - Processed 3570000/6416827 records\n",
      "2025-04-21 18:08:11,481 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:11,483 - INFO - Processed 3580000/6416827 records\n",
      "2025-04-21 18:08:11,526 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:11,528 - INFO - Processed 3590000/6416827 records\n",
      "2025-04-21 18:08:11,569 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:11,571 - INFO - Processed 3600000/6416827 records\n",
      "2025-04-21 18:08:11,599 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:11,602 - INFO - Processed 3610000/6416827 records\n",
      "2025-04-21 18:08:11,655 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:11,656 - INFO - Processed 3620000/6416827 records\n",
      "2025-04-21 18:08:11,702 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:11,705 - INFO - Processed 3630000/6416827 records\n",
      "2025-04-21 18:08:11,760 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:11,761 - INFO - Processed 3640000/6416827 records\n",
      "2025-04-21 18:08:11,806 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:11,808 - INFO - Processed 3650000/6416827 records\n",
      "2025-04-21 18:08:11,854 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:11,856 - INFO - Processed 3660000/6416827 records\n",
      "2025-04-21 18:08:11,882 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:11,883 - INFO - Processed 3670000/6416827 records\n",
      "2025-04-21 18:08:11,908 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:11,909 - INFO - Processed 3680000/6416827 records\n",
      "2025-04-21 18:08:11,935 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:11,938 - INFO - Processed 3690000/6416827 records\n",
      "2025-04-21 18:08:11,967 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:11,969 - INFO - Processed 3700000/6416827 records\n",
      "2025-04-21 18:08:11,997 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:11,998 - INFO - Processed 3710000/6416827 records\n",
      "2025-04-21 18:08:12,041 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:12,043 - INFO - Processed 3720000/6416827 records\n",
      "2025-04-21 18:08:12,069 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:12,072 - INFO - Processed 3730000/6416827 records\n",
      "2025-04-21 18:08:12,101 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:12,104 - INFO - Processed 3740000/6416827 records\n",
      "2025-04-21 18:08:12,133 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:12,134 - INFO - Processed 3750000/6416827 records\n",
      "2025-04-21 18:08:12,163 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:12,164 - INFO - Processed 3760000/6416827 records\n",
      "2025-04-21 18:08:12,193 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:12,194 - INFO - Processed 3770000/6416827 records\n",
      "2025-04-21 18:08:12,247 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:12,249 - INFO - Processed 3780000/6416827 records\n",
      "2025-04-21 18:08:12,275 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:12,277 - INFO - Processed 3790000/6416827 records\n",
      "2025-04-21 18:08:12,303 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:12,304 - INFO - Processed 3800000/6416827 records\n",
      "2025-04-21 18:08:12,331 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:12,332 - INFO - Processed 3810000/6416827 records\n",
      "2025-04-21 18:08:12,362 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:12,365 - INFO - Processed 3820000/6416827 records\n",
      "2025-04-21 18:08:12,424 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:12,427 - INFO - Processed 3830000/6416827 records\n",
      "2025-04-21 18:08:12,487 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:12,488 - INFO - Processed 3840000/6416827 records\n",
      "2025-04-21 18:08:12,522 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:12,524 - INFO - Processed 3850000/6416827 records\n",
      "2025-04-21 18:08:12,551 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:12,554 - INFO - Processed 3860000/6416827 records\n",
      "2025-04-21 18:08:12,583 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:12,584 - INFO - Processed 3870000/6416827 records\n",
      "2025-04-21 18:08:12,622 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:12,625 - INFO - Processed 3880000/6416827 records\n",
      "2025-04-21 18:08:12,654 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:12,656 - INFO - Processed 3890000/6416827 records\n",
      "2025-04-21 18:08:12,682 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:12,685 - INFO - Processed 3900000/6416827 records\n",
      "2025-04-21 18:08:12,711 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:12,713 - INFO - Processed 3910000/6416827 records\n",
      "2025-04-21 18:08:12,742 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:12,744 - INFO - Processed 3920000/6416827 records\n",
      "2025-04-21 18:08:12,773 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:12,774 - INFO - Processed 3930000/6416827 records\n",
      "2025-04-21 18:08:12,802 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:12,804 - INFO - Processed 3940000/6416827 records\n",
      "2025-04-21 18:08:12,833 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:12,835 - INFO - Processed 3950000/6416827 records\n",
      "2025-04-21 18:08:12,862 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:12,864 - INFO - Processed 3960000/6416827 records\n",
      "2025-04-21 18:08:12,891 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:12,893 - INFO - Processed 3970000/6416827 records\n",
      "2025-04-21 18:08:12,935 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:12,937 - INFO - Processed 3980000/6416827 records\n",
      "2025-04-21 18:08:12,967 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:12,971 - INFO - Processed 3990000/6416827 records\n",
      "2025-04-21 18:08:13,004 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:13,005 - INFO - Processed 4000000/6416827 records\n",
      "2025-04-21 18:08:13,045 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:13,046 - INFO - Processed 4010000/6416827 records\n",
      "2025-04-21 18:08:13,073 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:13,074 - INFO - Processed 4020000/6416827 records\n",
      "2025-04-21 18:08:13,102 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:13,103 - INFO - Processed 4030000/6416827 records\n",
      "2025-04-21 18:08:13,131 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:13,132 - INFO - Processed 4040000/6416827 records\n",
      "2025-04-21 18:08:13,163 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:13,164 - INFO - Processed 4050000/6416827 records\n",
      "2025-04-21 18:08:13,215 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:13,217 - INFO - Processed 4060000/6416827 records\n",
      "2025-04-21 18:08:13,247 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:13,248 - INFO - Processed 4070000/6416827 records\n",
      "2025-04-21 18:08:13,296 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:13,297 - INFO - Processed 4080000/6416827 records\n",
      "2025-04-21 18:08:13,343 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:13,344 - INFO - Processed 4090000/6416827 records\n",
      "2025-04-21 18:08:13,374 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:13,376 - INFO - Processed 4100000/6416827 records\n",
      "2025-04-21 18:08:13,405 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:13,406 - INFO - Processed 4110000/6416827 records\n",
      "2025-04-21 18:08:13,452 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:13,454 - INFO - Processed 4120000/6416827 records\n",
      "2025-04-21 18:08:13,496 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:13,498 - INFO - Processed 4130000/6416827 records\n",
      "2025-04-21 18:08:13,544 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:13,546 - INFO - Processed 4140000/6416827 records\n",
      "2025-04-21 18:08:13,581 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:13,582 - INFO - Processed 4150000/6416827 records\n",
      "2025-04-21 18:08:13,621 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:13,623 - INFO - Processed 4160000/6416827 records\n",
      "2025-04-21 18:08:13,671 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:13,674 - INFO - Processed 4170000/6416827 records\n",
      "2025-04-21 18:08:13,708 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:13,709 - INFO - Processed 4180000/6416827 records\n",
      "2025-04-21 18:08:13,760 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:13,762 - INFO - Processed 4190000/6416827 records\n",
      "2025-04-21 18:08:13,789 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:13,791 - INFO - Processed 4200000/6416827 records\n",
      "2025-04-21 18:08:13,820 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:13,821 - INFO - Processed 4210000/6416827 records\n",
      "2025-04-21 18:08:13,847 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:13,848 - INFO - Processed 4220000/6416827 records\n",
      "2025-04-21 18:08:13,885 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:13,886 - INFO - Processed 4230000/6416827 records\n",
      "2025-04-21 18:08:13,914 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:13,915 - INFO - Processed 4240000/6416827 records\n",
      "2025-04-21 18:08:13,944 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:13,947 - INFO - Processed 4250000/6416827 records\n",
      "2025-04-21 18:08:13,977 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:13,978 - INFO - Processed 4260000/6416827 records\n",
      "2025-04-21 18:08:14,029 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:14,031 - INFO - Processed 4270000/6416827 records\n",
      "2025-04-21 18:08:14,058 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:14,061 - INFO - Processed 4280000/6416827 records\n",
      "2025-04-21 18:08:14,087 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:14,090 - INFO - Processed 4290000/6416827 records\n",
      "2025-04-21 18:08:14,140 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:14,142 - INFO - Processed 4300000/6416827 records\n",
      "2025-04-21 18:08:14,171 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:14,173 - INFO - Processed 4310000/6416827 records\n",
      "2025-04-21 18:08:14,202 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:14,204 - INFO - Processed 4320000/6416827 records\n",
      "2025-04-21 18:08:14,248 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:14,249 - INFO - Processed 4330000/6416827 records\n",
      "2025-04-21 18:08:14,276 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:14,277 - INFO - Processed 4340000/6416827 records\n",
      "2025-04-21 18:08:14,327 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:14,329 - INFO - Processed 4350000/6416827 records\n",
      "2025-04-21 18:08:14,357 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:14,358 - INFO - Processed 4360000/6416827 records\n",
      "2025-04-21 18:08:14,387 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:14,388 - INFO - Processed 4370000/6416827 records\n",
      "2025-04-21 18:08:14,416 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:14,418 - INFO - Processed 4380000/6416827 records\n",
      "2025-04-21 18:08:14,470 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:14,471 - INFO - Processed 4390000/6416827 records\n",
      "2025-04-21 18:08:14,498 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:14,499 - INFO - Processed 4400000/6416827 records\n",
      "2025-04-21 18:08:14,528 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:14,531 - INFO - Processed 4410000/6416827 records\n",
      "2025-04-21 18:08:14,558 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:14,560 - INFO - Processed 4420000/6416827 records\n",
      "2025-04-21 18:08:14,587 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:14,588 - INFO - Processed 4430000/6416827 records\n",
      "2025-04-21 18:08:14,618 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:14,620 - INFO - Processed 4440000/6416827 records\n",
      "2025-04-21 18:08:14,673 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:14,675 - INFO - Processed 4450000/6416827 records\n",
      "2025-04-21 18:08:14,719 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:14,720 - INFO - Processed 4460000/6416827 records\n",
      "2025-04-21 18:08:14,750 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:14,752 - INFO - Processed 4470000/6416827 records\n",
      "2025-04-21 18:08:14,781 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:14,784 - INFO - Processed 4480000/6416827 records\n",
      "2025-04-21 18:08:14,809 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:14,811 - INFO - Processed 4490000/6416827 records\n",
      "2025-04-21 18:08:14,844 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:14,847 - INFO - Processed 4500000/6416827 records\n",
      "2025-04-21 18:08:14,884 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:14,886 - INFO - Processed 4510000/6416827 records\n",
      "2025-04-21 18:08:14,939 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:14,940 - INFO - Processed 4520000/6416827 records\n",
      "2025-04-21 18:08:14,968 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:14,970 - INFO - Processed 4530000/6416827 records\n",
      "2025-04-21 18:08:15,017 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:15,018 - INFO - Processed 4540000/6416827 records\n",
      "2025-04-21 18:08:15,049 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:15,051 - INFO - Processed 4550000/6416827 records\n",
      "2025-04-21 18:08:15,084 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:15,085 - INFO - Processed 4560000/6416827 records\n",
      "2025-04-21 18:08:15,109 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:15,116 - INFO - Processed 4570000/6416827 records\n",
      "2025-04-21 18:08:15,159 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:15,161 - INFO - Processed 4580000/6416827 records\n",
      "2025-04-21 18:08:15,187 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:15,189 - INFO - Processed 4590000/6416827 records\n",
      "2025-04-21 18:08:15,215 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:15,216 - INFO - Processed 4600000/6416827 records\n",
      "2025-04-21 18:08:15,241 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:15,242 - INFO - Processed 4610000/6416827 records\n",
      "2025-04-21 18:08:15,284 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:15,287 - INFO - Processed 4620000/6416827 records\n",
      "2025-04-21 18:08:15,316 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:15,317 - INFO - Processed 4630000/6416827 records\n",
      "2025-04-21 18:08:15,345 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:15,347 - INFO - Processed 4640000/6416827 records\n",
      "2025-04-21 18:08:15,375 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:15,377 - INFO - Processed 4650000/6416827 records\n",
      "2025-04-21 18:08:15,405 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:15,407 - INFO - Processed 4660000/6416827 records\n",
      "2025-04-21 18:08:15,434 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:15,436 - INFO - Processed 4670000/6416827 records\n",
      "2025-04-21 18:08:15,467 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:15,469 - INFO - Processed 4680000/6416827 records\n",
      "2025-04-21 18:08:15,496 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:15,497 - INFO - Processed 4690000/6416827 records\n",
      "2025-04-21 18:08:15,528 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:15,530 - INFO - Processed 4700000/6416827 records\n",
      "2025-04-21 18:08:15,581 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:15,582 - INFO - Processed 4710000/6416827 records\n",
      "2025-04-21 18:08:15,627 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:15,628 - INFO - Processed 4720000/6416827 records\n",
      "2025-04-21 18:08:15,677 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:15,678 - INFO - Processed 4730000/6416827 records\n",
      "2025-04-21 18:08:15,710 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:15,712 - INFO - Processed 4740000/6416827 records\n",
      "2025-04-21 18:08:15,742 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:15,743 - INFO - Processed 4750000/6416827 records\n",
      "2025-04-21 18:08:15,771 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:15,773 - INFO - Processed 4760000/6416827 records\n",
      "2025-04-21 18:08:15,799 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:15,801 - INFO - Processed 4770000/6416827 records\n",
      "2025-04-21 18:08:15,829 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:15,831 - INFO - Processed 4780000/6416827 records\n",
      "2025-04-21 18:08:15,857 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:15,859 - INFO - Processed 4790000/6416827 records\n",
      "2025-04-21 18:08:15,886 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:15,887 - INFO - Processed 4800000/6416827 records\n",
      "2025-04-21 18:08:15,915 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:15,918 - INFO - Processed 4810000/6416827 records\n",
      "2025-04-21 18:08:15,946 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:15,949 - INFO - Processed 4820000/6416827 records\n",
      "2025-04-21 18:08:15,999 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:16,002 - INFO - Processed 4830000/6416827 records\n",
      "2025-04-21 18:08:16,030 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:16,032 - INFO - Processed 4840000/6416827 records\n",
      "2025-04-21 18:08:16,058 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:16,060 - INFO - Processed 4850000/6416827 records\n",
      "2025-04-21 18:08:16,093 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:16,096 - INFO - Processed 4860000/6416827 records\n",
      "2025-04-21 18:08:16,121 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:16,123 - INFO - Processed 4870000/6416827 records\n",
      "2025-04-21 18:08:16,179 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:16,181 - INFO - Processed 4880000/6416827 records\n",
      "2025-04-21 18:08:16,217 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:16,219 - INFO - Processed 4890000/6416827 records\n",
      "2025-04-21 18:08:16,256 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:16,258 - INFO - Processed 4900000/6416827 records\n",
      "2025-04-21 18:08:16,317 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:16,319 - INFO - Processed 4910000/6416827 records\n",
      "2025-04-21 18:08:16,377 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:16,378 - INFO - Processed 4920000/6416827 records\n",
      "2025-04-21 18:08:16,430 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:16,431 - INFO - Processed 4930000/6416827 records\n",
      "2025-04-21 18:08:16,459 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:16,461 - INFO - Processed 4940000/6416827 records\n",
      "2025-04-21 18:08:16,487 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:16,488 - INFO - Processed 4950000/6416827 records\n",
      "2025-04-21 18:08:16,517 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:16,519 - INFO - Processed 4960000/6416827 records\n",
      "2025-04-21 18:08:16,571 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:16,574 - INFO - Processed 4970000/6416827 records\n",
      "2025-04-21 18:08:16,606 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:16,607 - INFO - Processed 4980000/6416827 records\n",
      "2025-04-21 18:08:16,661 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:16,662 - INFO - Processed 4990000/6416827 records\n",
      "2025-04-21 18:08:16,714 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:16,718 - INFO - Processed 5000000/6416827 records\n",
      "2025-04-21 18:08:16,749 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:16,751 - INFO - Processed 5010000/6416827 records\n",
      "2025-04-21 18:08:16,801 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:16,802 - INFO - Processed 5020000/6416827 records\n",
      "2025-04-21 18:08:16,847 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:16,848 - INFO - Processed 5030000/6416827 records\n",
      "2025-04-21 18:08:16,875 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:16,877 - INFO - Processed 5040000/6416827 records\n",
      "2025-04-21 18:08:16,908 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:16,909 - INFO - Processed 5050000/6416827 records\n",
      "2025-04-21 18:08:16,956 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:16,958 - INFO - Processed 5060000/6416827 records\n",
      "2025-04-21 18:08:17,004 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:17,005 - INFO - Processed 5070000/6416827 records\n",
      "2025-04-21 18:08:17,033 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:17,035 - INFO - Processed 5080000/6416827 records\n",
      "2025-04-21 18:08:17,081 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:17,082 - INFO - Processed 5090000/6416827 records\n",
      "2025-04-21 18:08:17,127 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:17,128 - INFO - Processed 5100000/6416827 records\n",
      "2025-04-21 18:08:17,176 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:17,177 - INFO - Processed 5110000/6416827 records\n",
      "2025-04-21 18:08:17,208 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:17,209 - INFO - Processed 5120000/6416827 records\n",
      "2025-04-21 18:08:17,236 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:17,237 - INFO - Processed 5130000/6416827 records\n",
      "2025-04-21 18:08:17,260 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:17,261 - INFO - Processed 5140000/6416827 records\n",
      "2025-04-21 18:08:17,284 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:17,286 - INFO - Processed 5150000/6416827 records\n",
      "2025-04-21 18:08:17,334 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:17,336 - INFO - Processed 5160000/6416827 records\n",
      "2025-04-21 18:08:17,386 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:17,388 - INFO - Processed 5170000/6416827 records\n",
      "2025-04-21 18:08:17,441 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:17,443 - INFO - Processed 5180000/6416827 records\n",
      "2025-04-21 18:08:17,493 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:17,495 - INFO - Processed 5190000/6416827 records\n",
      "2025-04-21 18:08:17,536 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:17,537 - INFO - Processed 5200000/6416827 records\n",
      "2025-04-21 18:08:17,564 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:17,566 - INFO - Processed 5210000/6416827 records\n",
      "2025-04-21 18:08:17,596 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:17,597 - INFO - Processed 5220000/6416827 records\n",
      "2025-04-21 18:08:17,623 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:17,624 - INFO - Processed 5230000/6416827 records\n",
      "2025-04-21 18:08:17,652 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:17,654 - INFO - Processed 5240000/6416827 records\n",
      "2025-04-21 18:08:17,707 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:17,709 - INFO - Processed 5250000/6416827 records\n",
      "2025-04-21 18:08:17,737 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:17,739 - INFO - Processed 5260000/6416827 records\n",
      "2025-04-21 18:08:17,791 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:17,793 - INFO - Processed 5270000/6416827 records\n",
      "2025-04-21 18:08:17,821 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:17,823 - INFO - Processed 5280000/6416827 records\n",
      "2025-04-21 18:08:17,869 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:17,872 - INFO - Processed 5290000/6416827 records\n",
      "2025-04-21 18:08:17,930 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:17,931 - INFO - Processed 5300000/6416827 records\n",
      "2025-04-21 18:08:17,974 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:17,976 - INFO - Processed 5310000/6416827 records\n",
      "2025-04-21 18:08:18,023 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:18,024 - INFO - Processed 5320000/6416827 records\n",
      "2025-04-21 18:08:18,069 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:18,071 - INFO - Processed 5330000/6416827 records\n",
      "2025-04-21 18:08:18,098 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:18,100 - INFO - Processed 5340000/6416827 records\n",
      "2025-04-21 18:08:18,123 - INFO - Starting to upsert 0 records in batches of 1000\n",
      "2025-04-21 18:08:18,124 - INFO - Processed 5350000/6416827 records\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys \n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Float, Date, DateTime, Boolean, ForeignKey, Text, text\n",
    "from sqlalchemy.orm import declarative_base, sessionmaker, relationship\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, to_date, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType, BooleanType\n",
    "\n",
    "# Configure logging\n",
    "log_directory = '/home/megin_mathew/logs/' # Or another directory where the user has permissions\n",
    "log_file_path = os.path.join(log_directory, 'data_loader.log')\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file_path, encoding='utf-8'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Spark with Hadoop workaround for Windows\n",
    "def create_spark_session():\n",
    "    \"\"\"Create and configure Spark session\"\"\"\n",
    "    try:\n",
    "        # Set Hadoop home directory if on Windows\n",
    "        if os.name == 'nt':\n",
    "            os.environ['HADOOP_HOME'] = 'C:\\\\hadoop'\n",
    "            os.environ['PATH'] = os.environ['PATH'] + ';' + os.environ['HADOOP_HOME'] + '\\\\bin'\n",
    "            \n",
    "            # Set Python path explicitly\n",
    "            python_path = sys.executable\n",
    "            os.environ['PYSPARK_PYTHON'] = python_path\n",
    "            os.environ['PYSPARK_DRIVER_PYTHON'] = python_path\n",
    "        \n",
    "        # Create temp directory if it doesn't exist\n",
    "        temp_dir = \"/home/megin_mathew/spark_temp\"\n",
    "        if not os.path.exists(temp_dir):\n",
    "            os.makedirs(temp_dir)\n",
    "            \n",
    "        return SparkSession.builder \\\n",
    "            .appName(\"GlobalFashionRetailDataLoader\") \\\n",
    "            .config(\"spark.local.dir\", temp_dir) \\\n",
    "            .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "            .config(\"spark.executor.memory\", \"8g\") \\\n",
    "            .config(\"spark.driver.memory\", \"8g\") \\\n",
    "            .config(\"spark.memory.offHeap.enabled\", \"false\") \\\n",
    "            .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "            .config(\"spark.default.parallelism\", \"200\") \\\n",
    "            .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "            .config(\"spark.network.timeout\", \"600s\") \\\n",
    "            .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "            .config(\"spark.python.profile\", \"false\") \\\n",
    "            .config(\"spark.executor.instances\", \"4\") \\\n",
    "            .config(\"spark.executor.cores\", \"2\") \\\n",
    "            .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "            .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "            .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"true\") \\\n",
    "            .getOrCreate()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create Spark session: {str(e)}\")\n",
    "        raise\n",
    "# Initialize Spark\n",
    "spark = create_spark_session()\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(\"/home/megin_mathew/airflow/notebooks/.env\")\n",
    "\n",
    "# Database schema definition\n",
    "Base = declarative_base()\n",
    "SCHEMA_NAME = \"GFRetail\"\n",
    "\n",
    "class Stores(Base):\n",
    "    __tablename__ = 'stores'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    store_id = Column(Integer, primary_key=True)\n",
    "    country = Column(String(100), nullable=True)\n",
    "    city = Column(String(100), nullable=True)\n",
    "    store_name = Column(String(100))\n",
    "    number_of_employees = Column(Integer)\n",
    "    zip_code = Column(String(20))\n",
    "    latitude = Column(Float, nullable=True)\n",
    "    longitude = Column(Float, nullable=True)\n",
    "\n",
    "class Employees(Base):\n",
    "    __tablename__ = 'employees'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    employee_id = Column(Integer, primary_key=True)\n",
    "    store_id = Column(Integer, ForeignKey(f'{SCHEMA_NAME}.stores.store_id'))\n",
    "    name = Column(String(255), nullable=True)\n",
    "    position = Column(String(100), nullable=True)\n",
    "\n",
    "class Customers(Base):\n",
    "    __tablename__ = 'customers'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    customer_id = Column(Integer, primary_key=True)\n",
    "    name = Column(String(255), nullable=True)\n",
    "    email = Column(String(255), nullable=True)\n",
    "    join_date = Column(Date, nullable=True)\n",
    "    telephone = Column(String(50), nullable=True)\n",
    "    city = Column(String(100), nullable=True)\n",
    "    country = Column(String(100), nullable=True)\n",
    "    gender = Column(String(20), nullable=True)\n",
    "    date_of_birth = Column(Date, nullable=True)\n",
    "    job_title = Column(String(100), nullable=True)\n",
    "\n",
    "class Products(Base):\n",
    "    __tablename__ = 'products'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    product_id = Column(Integer, primary_key=True)\n",
    "    category = Column(String(100), nullable=True)\n",
    "    sub_category = Column(String(100))\n",
    "    description_pt = Column(Text)\n",
    "    description_de = Column(Text)\n",
    "    description_fr = Column(Text)\n",
    "    description_es = Column(Text)\n",
    "    description_en = Column(Text)\n",
    "    description_zh = Column(Text)\n",
    "    color = Column(String(50), nullable=True)\n",
    "    sizes = Column(String(100), nullable=True)\n",
    "    production_cost = Column(Float)\n",
    "\n",
    "class Discounts(Base):\n",
    "    __tablename__ = 'discounts'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    start_date = Column(Date)\n",
    "    end_date = Column(Date)\n",
    "    discount_rate = Column(Float)\n",
    "    description = Column(String(255), nullable=True)\n",
    "    category = Column(String(100), nullable=True)\n",
    "    sub_category = Column(String(100))\n",
    "\n",
    "class Transactions(Base):\n",
    "    __tablename__ = 'transactions'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    invoice_id = Column(String(50), nullable=True)\n",
    "    line_number = Column(Integer)\n",
    "    customer_id = Column(Integer, ForeignKey(f'{SCHEMA_NAME}.customers.customer_id'))\n",
    "    product_id = Column(Integer, ForeignKey(f'{SCHEMA_NAME}.products.product_id'))\n",
    "    size = Column(String(10))\n",
    "    color = Column(String(50), nullable=True)\n",
    "    unit_price = Column(Float)\n",
    "    quantity = Column(Integer, nullable=True)\n",
    "    transaction_date = Column(DateTime)\n",
    "    discount = Column(Float, nullable=True)\n",
    "    line_total = Column(Float)\n",
    "    store_id = Column(Integer, ForeignKey(f'{SCHEMA_NAME}.stores.store_id'))\n",
    "    employee_id = Column(Integer, ForeignKey(f'{SCHEMA_NAME}.employees.employee_id'))\n",
    "    currency = Column(String(10), nullable=True)\n",
    "    currency_symbol = Column(String(5))\n",
    "    sku = Column(String(100))\n",
    "    transaction_type = Column(String(20))\n",
    "    payment_method = Column(String(50))\n",
    "    invoice_total = Column(Float)\n",
    "    is_return = Column(Boolean)\n",
    "\n",
    "# Data processing functions\n",
    "def process_stores(spark_df):\n",
    "    spark_df = spark_df.withColumn(\"country\", when(col(\"Country\").isNull(), lit(\"Unknown\")).otherwise(col(\"Country\"))) \\\n",
    "                      .withColumn(\"city\", when(col(\"City\").isNull(), lit(\"Unknown\")).otherwise(col(\"City\"))) \\\n",
    "                      .withColumn(\"latitude\", col(\"Latitude\").cast(FloatType())) \\\n",
    "                      .withColumn(\"longitude\", col(\"Longitude\").cast(FloatType()))\n",
    "    \n",
    "    return spark_df.select(\n",
    "        col(\"Store ID\").alias(\"store_id\"),\n",
    "        col(\"country\"),\n",
    "        col(\"city\"),\n",
    "        col(\"Store Name\").alias(\"store_name\"),\n",
    "        col(\"Number of Employees\").alias(\"number_of_employees\"),\n",
    "        col(\"ZIP Code\").alias(\"zip_code\"),\n",
    "        col(\"latitude\"),\n",
    "        col(\"longitude\")\n",
    "    )\n",
    "\n",
    "def process_employees(spark_df):\n",
    "    spark_df = spark_df.withColumn(\"name\", when(col(\"Name\").isNull(), lit(\"Unknown\")).otherwise(col(\"Name\"))) \\\n",
    "                      .withColumn(\"position\", when(col(\"Position\").isNull(), lit(\"Unknown\")).otherwise(col(\"Position\")))\n",
    "    \n",
    "    return spark_df.select(\n",
    "        col(\"Employee ID\").alias(\"employee_id\"),\n",
    "        col(\"Store ID\").alias(\"store_id\"),\n",
    "        col(\"name\"),\n",
    "        col(\"position\")\n",
    "    )\n",
    "\n",
    "def process_customers(spark_df):\n",
    "    spark_df = spark_df.withColumn(\"join_date\", to_date(col(\"Date Of Birth\"), \"yyyy-MM-dd\")) \\\n",
    "                      .withColumn(\"name\", when(col(\"Name\").isNull(), lit(\"Unknown\")).otherwise(col(\"Name\"))) \\\n",
    "                      .withColumn(\"email\", when(col(\"Email\").isNull(), lit(\"unknown@example.com\")).otherwise(col(\"Email\")))\n",
    "    \n",
    "    return spark_df.select(\n",
    "        col(\"Customer ID\").alias(\"customer_id\"),\n",
    "        col(\"name\"),\n",
    "        col(\"email\"),\n",
    "        col(\"join_date\"),\n",
    "        col(\"Telephone\").alias(\"telephone\"),\n",
    "        col(\"City\").alias(\"city\"),\n",
    "        col(\"Country\").alias(\"country\"),\n",
    "        col(\"Gender\").alias(\"gender\"),\n",
    "        to_date(col(\"Date Of Birth\"), \"yyyy-MM-dd\").alias(\"date_of_birth\"),\n",
    "        col(\"Job Title\").alias(\"job_title\")\n",
    "    )\n",
    "\n",
    "def process_products(spark_df):\n",
    "    spark_df = spark_df.withColumn(\"category\", when(col(\"Category\").isNull(), lit(\"Unknown\")).otherwise(col(\"Category\"))) \\\n",
    "                      .withColumn(\"color\", when(col(\"Color\").isNull(), lit(\"Unknown\")).otherwise(col(\"Color\"))) \\\n",
    "                      .withColumn(\"sizes\", when(col(\"Sizes\").isNull(), lit(\"Unknown\")).otherwise(col(\"Sizes\")))\n",
    "    \n",
    "    return spark_df.select(\n",
    "        col(\"Product ID\").alias(\"product_id\"),\n",
    "        col(\"category\"),\n",
    "        col(\"Sub Category\").alias(\"sub_category\"),\n",
    "        col(\"Description PT\").alias(\"description_pt\"),\n",
    "        col(\"Description DE\").alias(\"description_de\"),\n",
    "        col(\"Description FR\").alias(\"description_fr\"),\n",
    "        col(\"Description ES\").alias(\"description_es\"),\n",
    "        col(\"Description EN\").alias(\"description_en\"),\n",
    "        col(\"Description ZH\").alias(\"description_zh\"),\n",
    "        col(\"color\"),\n",
    "        col(\"sizes\"),\n",
    "        col(\"Production Cost\").alias(\"production_cost\")\n",
    "    )\n",
    "\n",
    "def process_discounts(spark_df):\n",
    "    spark_df = spark_df.withColumn(\"description\", when(col(\"Description\").isNull(), lit(\"No description\")).otherwise(col(\"Description\"))) \\\n",
    "                      .withColumn(\"category\", when(col(\"Category\").isNull(), lit(\"General\")).otherwise(col(\"Category\")))\n",
    "    \n",
    "    return spark_df.select(\n",
    "        to_date(col(\"Start\"), \"yyyy-MM-dd\").alias(\"start_date\"),\n",
    "        to_date(col(\"End\"), \"yyyy-MM-dd\").alias(\"end_date\"),\n",
    "        col(\"Discont\").alias(\"discount_rate\"),\n",
    "        col(\"description\"),\n",
    "        col(\"category\"),\n",
    "        col(\"Sub Category\").alias(\"sub_category\")\n",
    "    )\n",
    "\n",
    "def process_transactions(spark_df):\n",
    "    spark_df = spark_df.withColumn(\"invoice_id\", when(col(\"Invoice ID\").isNull(), lit(\"UNKNOWN\")).otherwise(col(\"Invoice ID\"))) \\\n",
    "                      .withColumn(\"quantity\", when(col(\"Quantity\").isNull(), lit(1)).otherwise(col(\"Quantity\"))) \\\n",
    "                      .withColumn(\"color\", when(col(\"Color\").isNull(), lit(\"Unknown\")).otherwise(col(\"Color\"))) \\\n",
    "                      .withColumn(\"discount\", when(col(\"Discount\").isNull(), lit(0.0)).otherwise(col(\"Discount\"))) \\\n",
    "                      .withColumn(\"currency\", when(col(\"Currency\").isNull(), lit(\"USD\")).otherwise(col(\"Currency\"))) \\\n",
    "                      .withColumn(\"is_return\", col(\"Transaction Type\") == \"Return\")\n",
    "    \n",
    "    return spark_df.select(\n",
    "        col(\"invoice_id\"),\n",
    "        col(\"Line\").alias(\"line_number\"),\n",
    "        col(\"Customer ID\").alias(\"customer_id\"),\n",
    "        col(\"Product ID\").alias(\"product_id\"),\n",
    "        col(\"Size\").alias(\"size\"),\n",
    "        col(\"color\"),\n",
    "        col(\"Unit Price\").alias(\"unit_price\"),\n",
    "        col(\"quantity\"),\n",
    "        col(\"Date\").alias(\"transaction_date\"),\n",
    "        col(\"discount\"),\n",
    "        col(\"Line Total\").alias(\"line_total\"),\n",
    "        col(\"Store ID\").alias(\"store_id\"),\n",
    "        col(\"Employee ID\").alias(\"employee_id\"),\n",
    "        col(\"currency\"),\n",
    "        col(\"Currency Symbol\").alias(\"currency_symbol\"),\n",
    "        col(\"SKU\").alias(\"sku\"),\n",
    "        col(\"Transaction Type\").alias(\"transaction_type\"),\n",
    "        col(\"Payment Method\").alias(\"payment_method\"),\n",
    "        col(\"Invoice Total\").alias(\"invoice_total\"),\n",
    "        col(\"is_return\")\n",
    "    )\n",
    "\n",
    "# In setup_database function, add table creation after schema creation\n",
    "def setup_database(engine):\n",
    "    \"\"\"Create schema and set permissions if needed\"\"\"\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            # Check if schema exists\n",
    "            schema_exists = conn.execute(\n",
    "                text(\"SELECT 1 FROM information_schema.schemata WHERE schema_name = :schema\"),\n",
    "                {'schema': SCHEMA_NAME}\n",
    "            ).scalar()\n",
    "            \n",
    "            if not schema_exists:\n",
    "                logger.info(f\"Creating schema {SCHEMA_NAME}\")\n",
    "                conn.execute(text(f\"CREATE SCHEMA {SCHEMA_NAME}\"))\n",
    "                conn.execute(text(f\"\"\"\n",
    "                    GRANT ALL ON SCHEMA {SCHEMA_NAME} TO {os.getenv('DB_USER', 'postgres')};\n",
    "                    ALTER DEFAULT PRIVILEGES IN SCHEMA {SCHEMA_NAME} \n",
    "                    GRANT ALL ON TABLES TO {os.getenv('DB_USER', 'postgres')};\n",
    "                \"\"\"))\n",
    "            \n",
    "            # Create all tables if they don't exist\n",
    "                Base.metadata.create_all(engine)\n",
    "            \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Database setup failed: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def upsert_data(session, table_class, records, primary_key, batch_size=1000):\n",
    "    \"\"\"Upsert data into the database with batch processing\"\"\"\n",
    "    try:\n",
    "        total_records = len(records)\n",
    "        logger.info(f\"Starting to upsert {total_records} records in batches of {batch_size}\")\n",
    "        \n",
    "        for i in range(0, total_records, batch_size):\n",
    "            batch = records[i:i + batch_size]\n",
    "            batch_num = (i // batch_size) + 1\n",
    "            total_batches = (total_records - 1) // batch_size + 1\n",
    "            \n",
    "            try:\n",
    "                session.bulk_insert_mappings(table_class, batch)\n",
    "                session.commit()\n",
    "                logger.info(f\"Successfully inserted batch {batch_num}/{total_batches}\")\n",
    "            except Exception as e:\n",
    "                session.rollback()\n",
    "                logger.info(f\"Insert failed for batch {batch_num}, switching to upsert mode\")\n",
    "                \n",
    "                for record in batch:\n",
    "                    try:\n",
    "                        existing = session.query(table_class).filter(\n",
    "                            getattr(table_class, primary_key) == record[primary_key]\n",
    "                        ).first()\n",
    "                        \n",
    "                        if existing:\n",
    "                            for key, value in record.items():\n",
    "                                setattr(existing, key, value)\n",
    "                        else:\n",
    "                            new_record = table_class(**record)\n",
    "                            session.add(new_record)\n",
    "                    except Exception as rec_error:\n",
    "                        logger.error(f\"Error processing record {record[primary_key]}: {str(rec_error)}\")\n",
    "                        continue\n",
    "                \n",
    "                try:\n",
    "                    session.commit()\n",
    "                    logger.info(f\"Successfully upserted batch {batch_num}/{total_batches}\")\n",
    "                except Exception as commit_error:\n",
    "                    session.rollback()\n",
    "                    logger.error(f\"Failed to commit batch {batch_num}: {str(commit_error)}\")\n",
    "                    continue\n",
    "                \n",
    "    except Exception as e:\n",
    "        session.rollback()\n",
    "        logger.error(f\"Error in upsert_data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def write_spark_df_to_postgres(spark_df, table_name, engine):\n",
    "    \"\"\"Write Spark DataFrame to PostgreSQL with optimized methods\"\"\"\n",
    "    try:\n",
    "        table_class = globals()[table_name.capitalize()]\n",
    "        primary_key = {\n",
    "            'stores': 'store_id',\n",
    "            'employees': 'employee_id',\n",
    "            'customers': 'customer_id',\n",
    "            'products': 'product_id',\n",
    "            'discounts': 'id',\n",
    "            'transactions': 'id'\n",
    "        }.get(table_name)\n",
    "        \n",
    "        if not primary_key:\n",
    "            logger.error(f\"Primary key not defined for table {table_name}\")\n",
    "            return\n",
    "            \n",
    "        # Skip count verification for large tables to avoid Spark worker crashes\n",
    "        if table_name in ['transactions']:  # Add other large tables if needed\n",
    "            total_rows = \"unknown (skipped count for large table)\"\n",
    "        else:\n",
    "            total_rows = spark_df.count()\n",
    "            \n",
    "        logger.info(f\"Processing {total_rows} records for table {table_name}\")\n",
    "        \n",
    "        # For small tables, use direct conversion\n",
    "        if isinstance(total_rows, int) and total_rows <= 10000:\n",
    "            logger.info(f\"Processing small {table_name} table with direct conversion\")\n",
    "            pandas_df = spark_df.toPandas()\n",
    "            records = pandas_df.to_dict('records')\n",
    "            \n",
    "            Session = sessionmaker(bind=engine)\n",
    "            session = Session()\n",
    "            try:\n",
    "                upsert_data(session, table_class, records, primary_key)\n",
    "                \n",
    "                # Verify count using SQLAlchemy which handles schema names correctly\n",
    "                count = session.query(table_class).count()\n",
    "                logger.info(f\"Verified {count} records in {table_name}\")\n",
    "                \n",
    "            finally:\n",
    "                session.close()\n",
    "            return\n",
    "        \n",
    "        # For larger tables\n",
    "        logger.info(f\"Processing large {table_name} table with optimized chunks\")\n",
    "        pandas_iter = spark_df.toLocalIterator() \n",
    "        records_buffer = []\n",
    "        \n",
    "        for i, row in enumerate(pandas_iter, 1):\n",
    "            records_buffer.append(row.asDict())\n",
    "            \n",
    "            if i % 10000 == 0 or (isinstance(total_rows, int) and i == total_rows):\n",
    "                Session = sessionmaker(bind=engine)\n",
    "                session = Session()\n",
    "                try:\n",
    "                    upsert_data(session, table_class, records_buffer, primary_key)\n",
    "                    logger.info(f\"Processed {i} records\")\n",
    "                    records_buffer = []\n",
    "                finally:\n",
    "                    session.close()\n",
    "        \n",
    "        # Skip final verification for large tables\n",
    "        if table_name not in ['transactions']:  # Add other large tables if needed\n",
    "            with engine.connect() as conn:\n",
    "                # Use quoted identifiers for the schema and table name\n",
    "                count = conn.execute(text(f'SELECT COUNT(*) FROM \"{SCHEMA_NAME}\".\"{table_name}\"')).scalar()\n",
    "                logger.info(f\"Final count for {table_name}: {count} records\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error writing to {table_name}: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "def load_data_to_postgres():\n",
    "    \"\"\"Main function to load data to PostgreSQL\"\"\"\n",
    "    #DB_USER = os.getenv('DB_USER', 'postgres')\n",
    "    #DB_PASSWORD = os.getenv('DB_PASSWORD', 'Grandvalley_2026')\n",
    "    #DB_HOST = os.getenv('DB_HOST', 'localhost')\n",
    "    #DB_PORT = os.getenv('DB_PORT', '5432')\n",
    "    #DB_NAME = os.getenv('DB_NAME', 'Global_Fashion_Retail_Stores')\n",
    "    #DATASET_PATH = os.getenv('DATASET_PATH', 'C:/Users/megin/fashion_dataset')\n",
    "    DB_USER = os.getenv('DB_USER')\n",
    "    DB_PASSWORD = os.getenv('DB_PASSWORD')\n",
    "    DB_HOST = os.getenv('DB_HOST')\n",
    "    DB_PORT = os.getenv('DB_PORT')\n",
    "    DB_NAME = os.getenv('DB_NAME')\n",
    "    DATASET_PATH = os.getenv('DATASET_PATH')\n",
    "\n",
    "    required_vars = {\n",
    "    'DB_USER': DB_USER,\n",
    "    'DB_PASSWORD': DB_PASSWORD,\n",
    "    'DB_HOST': DB_HOST,\n",
    "    'DB_NAME': DB_NAME,\n",
    "    'DATASET_PATH': DATASET_PATH\n",
    "    }\n",
    "    \n",
    "    for var_name, var_value in required_vars.items():\n",
    "        if not var_value:\n",
    "            raise ValueError(f\"Missing required environment variable: {var_name}\")\n",
    "    \n",
    "    # Set default for PORT only (since it often doesn't change)\n",
    "    DB_PORT = DB_PORT if DB_PORT else '5432'\n",
    "    try:\n",
    "        engine = create_engine(\n",
    "            f'postgresql+psycopg2://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}',\n",
    "            pool_size=20,\n",
    "            max_overflow=30,\n",
    "            pool_pre_ping=True,\n",
    "            pool_recycle=3600,\n",
    "            connect_args={'connect_timeout': 10}\n",
    "        )\n",
    "        \n",
    "        if not setup_database(engine):\n",
    "            raise RuntimeError(\"Database setup failed\")\n",
    "        \n",
    "        Base.metadata.create_all(engine)\n",
    "        \n",
    "        file_processors = [\n",
    "            ('stores.csv', process_stores, 'stores'),\n",
    "            ('employees.csv', process_employees, 'employees'),\n",
    "            ('products.csv', process_products, 'products'),\n",
    "            ('discounts.csv', process_discounts, 'discounts'),\n",
    "            ('customers.csv', process_customers, 'customers'),\n",
    "            ('transactions.csv', process_transactions, 'transactions')\n",
    "        ]\n",
    "        \n",
    "        for file_name, processor, table_name in file_processors:\n",
    "            file_path = os.path.join(DATASET_PATH, file_name)\n",
    "            if os.path.exists(file_path):\n",
    "                logger.info(f\"Processing {file_name}...\")\n",
    "                start_time = datetime.now()\n",
    "                \n",
    "                try:\n",
    "                    if file_name in ['customers.csv', 'transactions.csv']:\n",
    "                        sample_df = spark.read.csv(file_path, header=True, inferSchema=True, samplingRatio=0.1)\n",
    "                        schema = sample_df.schema\n",
    "                        spark_df = spark.read.csv(file_path, header=True, schema=schema)\n",
    "                    else:\n",
    "                        spark_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "                    \n",
    "                    processed_df = processor(spark_df)\n",
    "                    logger.info(f\"Processed DataFrame schema: {processed_df._jdf.schema().treeString()}\")\n",
    "                    \n",
    "                    write_spark_df_to_postgres(processed_df, table_name, engine)\n",
    "                    \n",
    "                    logger.info(f\"Completed {file_name} in {datetime.now() - start_time}\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing {file_name}: {str(e)}\", exc_info=True)\n",
    "                    \n",
    "                    if file_name == 'products.csv':\n",
    "                        logger.info(\"Attempting alternative approach for products.csv\")\n",
    "                        try:\n",
    "                            products_pd = pd.read_csv(file_path)\n",
    "                            products_spark = spark.createDataFrame(products_pd)\n",
    "                            processed_df = processor(products_spark)\n",
    "                            write_spark_df_to_postgres(processed_df, table_name, engine)\n",
    "                            logger.info(f\"Successfully processed products.csv with alternative approach\")\n",
    "                        except Exception as alt_e:\n",
    "                            logger.error(f\"Alternative approach also failed: {str(alt_e)}\")\n",
    "                            raise\n",
    "                    else:\n",
    "                        raise\n",
    "            else:\n",
    "                logger.warning(f\"File not found: {file_path}\")\n",
    "        \n",
    "        logger.info(\"Data load completed successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Database error: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "    finally:\n",
    "        engine.dispose()\n",
    "        spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        logger.info(\"Starting data load process...\")\n",
    "        start_time = datetime.now()\n",
    "        load_data_to_postgres()\n",
    "        logger.info(f\"Process completed in {datetime.now() - start_time}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fatal error: {str(e)}\", exc_info=True)\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1df72ed-687e-495d-88a1-2458e5ba3892",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cleanse and Aggregate for Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf53cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Float, Date, DateTime, Boolean, ForeignKey, Text, text, PrimaryKeyConstraint\n",
    "from sqlalchemy.orm import declarative_base, sessionmaker, relationship\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, to_date, lit, datediff, current_date, dayofweek, sum as spark_sum, avg as spark_avg, count as spark_count, to_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType, BooleanType, TimestampType\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('data_processor.log', encoding='utf-8'), # Changed log file name\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Spark with Hadoop workaround for Windows and JDBC driver config\n",
    "def create_spark_session():\n",
    "    \"\"\"Create and configure Spark session, including JDBC driver via spark.driver.extraClassPath.\"\"\"\n",
    "    try:\n",
    "        # Set Hadoop home directory if on Windows\n",
    "        # This is often needed for Spark to function correctly on Windows, even without HDFS\n",
    "        if os.name == 'nt':\n",
    "            # Ensure these paths are correct for your Windows setup if needed\n",
    "            # Consider adding HADOOP_HOME_WIN to your .env or set it here if consistent\n",
    "            hadoop_home = os.getenv('HADOOP_HOME_WIN', 'C:\\\\hadoop')\n",
    "            os.environ['HADOOP_HOME'] = hadoop_home\n",
    "            # Add Hadoop bin to PATH if it exists\n",
    "            hadoop_bin = os.path.join(hadoop_home, 'bin')\n",
    "            if os.path.exists(hadoop_bin):\n",
    "                 os.environ['PATH'] = os.environ['PATH'] + ';' + hadoop_bin\n",
    "\n",
    "            # Set Python path explicitly for PySpark\n",
    "            python_path = sys.executable\n",
    "            os.environ['PYSPARK_PYTHON'] = python_path\n",
    "            os.environ['PYSPARK_DRIVER_PYTHON'] = python_path\n",
    "\n",
    "        # Create temp directory if it doesn't exist\n",
    "        # Use environment variable with default for flexibility\n",
    "        temp_dir = os.getenv('SPARK_TEMP_DIR', 'D:/spark_temp')\n",
    "        if not os.path.exists(temp_dir):\n",
    "            os.makedirs(temp_dir)\n",
    "            logger.info(f\"Created Spark temporary directory: {temp_dir}\")\n",
    "\n",
    "        spark_builder = SparkSession.builder \\\n",
    "            .appName(\"GlobalFashionRetailDataProcessor\") \\\n",
    "            .config(\"spark.local.dir\", temp_dir) \\\n",
    "            .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "            .config(\"spark.executor.memory\", \"8g\") \\\n",
    "            .config(\"spark.driver.memory\", \"8g\") \\\n",
    "            .config(\"spark.memory.offHeap.enabled\", \"false\") \\\n",
    "            .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "            .config(\"spark.default.parallelism\", \"200\") \\\n",
    "            .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "            .config(\"spark.network.timeout\", \"600s\") \\\n",
    "            .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "            .config(\"spark.python.profile\", \"false\") \\\n",
    "            .config(\"spark.executor.instances\", \"4\") \\\n",
    "            .config(\"spark.executor.cores\", \"2\") \\\n",
    "            .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "            .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "            .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"true\") \\\n",
    "            .config(\"spark.sql.debug.maxToStringFields\", 100) # Increase for wider dataframes in logging\n",
    "\n",
    "        # === Configure JDBC driver via spark.driver.extraClassPath ===\n",
    "        # Manually specify the path to the downloaded JAR file using the POSTGRES_JDBC_DRIVER_PATH env var\n",
    "        # This config is specifically for the driver program (where your Python script runs)\n",
    "        jdbc_jar_path = os.getenv('POSTGRES_JDBC_DRIVER_PATH')\n",
    "\n",
    "        if not jdbc_jar_path:\n",
    "            logger.error(\"POSTGRES_JDBC_DRIVER_PATH environment variable is not set.\")\n",
    "            raise ValueError(\"POSTGRES_JDBC_DRIVER_PATH environment variable must be set to the path of the PostgreSQL JDBC driver JAR.\")\n",
    "\n",
    "        if not os.path.exists(jdbc_jar_path):\n",
    "             logger.error(f\"PostgreSQL JDBC driver not found at specified path: {jdbc_jar_path}\")\n",
    "             raise FileNotFoundError(f\"PostgreSQL JDBC driver not found at {jdbc_jar_path}. Please download the JAR and set POSTGRES_JDBC_DRIVER_PATH.\")\n",
    "\n",
    "        logger.info(f\"Configuring Spark driver with extra classpath: {jdbc_jar_path}\")\n",
    "        spark_builder = spark_builder.config(\"spark.driver.extraClassPath\", jdbc_jar_path)\n",
    "        # =====================================================\n",
    "\n",
    "        # === Removed the spark.jars.packages configuration block ===\n",
    "        # jdbc_package = \"org.postgresql:postgresql:42.6.0\"\n",
    "        # logger.info(f\"Configuring Spark to use JDBC package: {jdbc_package}\")\n",
    "        # spark_builder = spark_builder.config(\"spark.jars.packages\", jdbc_package)\n",
    "        # =====================================================\n",
    "\n",
    "\n",
    "        logger.info(\"Attempting to get or create Spark session...\")\n",
    "        spark_session = spark_builder.getOrCreate()\n",
    "        logger.info(\"Spark session created successfully.\")\n",
    "        return spark_session\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create Spark session: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "# Initialize Spark\n",
    "spark = create_spark_session()\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Database schema definition\n",
    "Base = declarative_base()\n",
    "SCHEMA_NAME = \"GFRetail\" # Ensure this matches your database schema name\n",
    "\n",
    "# Define SQLAlchemy models for all tables, including expected columns after FE\n",
    "# These models are used by Base.metadata.create_all (for new tables)\n",
    "# Note: These models define the *target* schema. When reading from DB, Spark\n",
    "# will infer the schema, but these models ensure the target tables (especially\n",
    "# the new daily_store_sales and potentially new columns) are set up correctly.\n",
    "class Stores(Base):\n",
    "    __tablename__ = 'stores'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    store_id = Column(Integer, primary_key=True)\n",
    "    country = Column(String(100), nullable=True)\n",
    "    city = Column(String(100), nullable=True)\n",
    "    store_name = Column(String(100))\n",
    "    number_of_employees = Column(Integer)\n",
    "    zip_code = Column(String(20))\n",
    "    latitude = Column(Float, nullable=True)\n",
    "    longitude = Column(Float, nullable=True)\n",
    "\n",
    "class Employees(Base):\n",
    "    __tablename__ = 'employees'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    employee_id = Column(Integer, primary_key=True)\n",
    "    store_id = Column(Integer, ForeignKey(f'{SCHEMA_NAME}.stores.store_id'))\n",
    "    name = Column(String(255), nullable=True)\n",
    "    position = Column(String(100), nullable=True)\n",
    "\n",
    "class Customers(Base):\n",
    "    __tablename__ = 'customers'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    customer_id = Column(Integer, primary_key=True)\n",
    "    name = Column(String(255), nullable=True)\n",
    "    email = Column(String(255), nullable=True)\n",
    "    join_date = Column(Date, nullable=True)\n",
    "    telephone = Column(String(50), nullable=True)\n",
    "    city = Column(String(100), nullable=True)\n",
    "    country = Column(String(100), nullable=True)\n",
    "    gender = Column(String(20), nullable=True)\n",
    "    date_of_birth = Column(Date, nullable=True)\n",
    "    job_title = Column(String(100), nullable=True)\n",
    "    # New Feature Column - will be added to DB if running create_all and it's missing\n",
    "    days_since_birth = Column(Integer, nullable=True)\n",
    "\n",
    "class Products(Base):\n",
    "    __tablename__ = 'products'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    product_id = Column(Integer, primary_key=True)\n",
    "    category = Column(String(100), nullable=True)\n",
    "    sub_category = Column(String(100))\n",
    "    description_pt = Column(Text)\n",
    "    description_de = Column(Text)\n",
    "    description_fr = Column(Text)\n",
    "    description_es = Column(Text)\n",
    "    description_en = Column(Text)\n",
    "    description_zh = Column(Text)\n",
    "    color = Column(String(50), nullable=True)\n",
    "    sizes = Column(String(100), nullable=True)\n",
    "    production_cost = Column(Float)\n",
    "\n",
    "class Discounts(Base):\n",
    "    __tablename__ = 'discounts'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True) # Assuming this is the PK in DB\n",
    "    start_date = Column(Date)\n",
    "    end_date = Column(Date)\n",
    "    discount_rate = Column(Float)\n",
    "    description = Column(String(255), nullable=True)\n",
    "    category = Column(String(100), nullable=True)\n",
    "    sub_category = Column(String(100))\n",
    "\n",
    "class Transactions(Base):\n",
    "    __tablename__ = 'transactions'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    # Assuming 'id' is the primary key already in the DB\n",
    "    id = Column(Integer, primary_key=True) # Assuming this is pre-existing PK\n",
    "    invoice_id = Column(String(50), nullable=True)\n",
    "    line_number = Column(Integer)\n",
    "    customer_id = Column(Integer, ForeignKey(f'{SCHEMA_NAME}.customers.customer_id'))\n",
    "    product_id = Column(Integer, ForeignKey(f'{SCHEMA_NAME}.products.product_id'))\n",
    "    size = Column(String(10))\n",
    "    color = Column(String(50), nullable=True)\n",
    "    unit_price = Column(Float)\n",
    "    quantity = Column(Integer, nullable=True)\n",
    "    transaction_date = Column(DateTime) # Or Timestamp if that's the DB type\n",
    "    discount = Column(Float, nullable=True)\n",
    "    line_total = Column(Float)\n",
    "    store_id = Column(Integer, ForeignKey(f'{SCHEMA_NAME}.stores.store_id'))\n",
    "    employee_id = Column(Integer, ForeignKey(f'{SCHEMA_NAME}.employees.employee_id'))\n",
    "    currency = Column(String(10), nullable=True)\n",
    "    currency_symbol = Column(String(5))\n",
    "    sku = Column(String(100))\n",
    "    transaction_type = Column(String(20))\n",
    "    payment_method = Column(String(50))\n",
    "    invoice_total = Column(Float)\n",
    "    is_return = Column(Boolean)\n",
    "    # New Feature Columns - will be added to DB if running create_all and they are missing\n",
    "    net_line_total = Column(Float, nullable=True)\n",
    "    transaction_day_of_week = Column(Integer, nullable=True)\n",
    "\n",
    "class DailyStoreSales(Base):\n",
    "    __tablename__ = 'daily_store_sales'\n",
    "    __table_args__ = (\n",
    "        PrimaryKeyConstraint('store_id', 'sale_date'), # Using composite primary key\n",
    "        {'schema': SCHEMA_NAME}\n",
    "    )\n",
    "    store_id = Column(Integer, ForeignKey(f'{SCHEMA_NAME}.stores.store_id'), nullable=False)\n",
    "    sale_date = Column(Date, nullable=False)\n",
    "    transaction_count = Column(Integer, nullable=True)\n",
    "    total_sales = Column(Float, nullable=True)\n",
    "    average_unit_price = Column(Float, nullable=True)\n",
    "    total_quantity_sold = Column(Integer, nullable=True)\n",
    "    total_discount_given = Column(Float, nullable=True)\n",
    "    total_net_sales = Column(Float, nullable=True)\n",
    "\n",
    "\n",
    "# --- Data Processing Functions (Feature Engineering & Aggregation) ---\n",
    "# These functions now assume input DataFrames have column names matching SQLAlchemy models (snake_case)\n",
    "\n",
    "def add_customer_features(customers_df):\n",
    "    \"\"\"Adds feature engineered columns to the customers DataFrame.\"\"\"\n",
    "    logger.info(\"Adding customer features (days_since_birth)...\")\n",
    "    # Assumes 'date_of_birth' column exists and is DateType/TimestampType in DB\n",
    "    processed_df = customers_df.withColumn(\n",
    "        \"days_since_birth\",\n",
    "        when(col(\"date_of_birth\").isNotNull(), datediff(current_date(), col(\"date_of_birth\")))\n",
    "        .otherwise(lit(None)) # Handle cases where date_of_birth is null\n",
    "    )\n",
    "    logger.info(\"Customer feature engineering complete.\")\n",
    "    return processed_df\n",
    "\n",
    "def add_transaction_features(transactions_df):\n",
    "    \"\"\"Adds feature engineered columns to the transactions DataFrame.\"\"\"\n",
    "    logger.info(\"Adding transaction features (net_line_total, day_of_week)...\")\n",
    "    # Ensure required columns are cast to appropriate types before calculation/extraction\n",
    "    # Assuming columns are already in snake_case and appropriate types from DB read,\n",
    "    # but explicit casting here adds robustness if DB schema is slightly different.\n",
    "    transactions_df = transactions_df.withColumn(\"unit_price\", col(\"unit_price\").cast(FloatType()))\\\n",
    "                                     .withColumn(\"quantity\", col(\"quantity\").cast(IntegerType()))\\\n",
    "                                     .withColumn(\"discount\", col(\"discount\").cast(FloatType()))\\\n",
    "                                     .withColumn(\"transaction_date\", col(\"transaction_date\").cast(TimestampType())) # Ensure datetime type\n",
    "\n",
    "\n",
    "    processed_df = transactions_df.withColumn(\n",
    "        \"net_line_total\",\n",
    "        (col(\"unit_price\") * col(\"quantity\")) - col(\"discount\") # Calculation on cleaned columns\n",
    "    ).withColumn(\n",
    "        \"transaction_day_of_week\",\n",
    "         when(col(\"transaction_date\").isNotNull(), dayofweek(col(\"transaction_date\")))\n",
    "        .otherwise(lit(None))\n",
    "    )\n",
    "    logger.info(\"Transaction feature engineering complete.\")\n",
    "    return processed_df\n",
    "\n",
    "\n",
    "def aggregate_daily_store_sales(transactions_df):\n",
    "    \"\"\"Aggregates transaction data (assuming snake_case columns) to get daily sales summary per store.\"\"\"\n",
    "    logger.info(\"Starting daily store sales aggregation...\")\n",
    "\n",
    "    # Ensure transaction_date is a proper timestamp/datetime before casting to Date\n",
    "    # Assuming the input transactions_df already has 'transaction_date' as TimestampType\n",
    "    transactions_df = transactions_df.withColumn(\"sale_date\", to_date(col(\"transaction_date\")))\n",
    "\n",
    "    # Group by store_id and sale_date and perform aggregations\n",
    "    daily_sales_df = transactions_df.groupBy(\"store_id\", \"sale_date\").agg(\n",
    "        spark_count(\"*\").alias(\"transaction_count\"),\n",
    "        spark_sum(\"line_total\").alias(\"total_sales\"), # Using line_total as total sales per line\n",
    "        spark_avg(\"unit_price\").alias(\"average_unit_price\"),\n",
    "        spark_sum(\"quantity\").alias(\"total_quantity_sold\"),\n",
    "        spark_sum(\"discount\").alias(\"total_discount_given\"),\n",
    "        spark_sum(\"net_line_total\").alias(\"total_net_sales\") # Aggregate the new net_line_total\n",
    "    )\n",
    "\n",
    "    logger.info(\"Daily store sales aggregation complete.\")\n",
    "    return daily_sales_df.select(\n",
    "        \"store_id\",\n",
    "        \"sale_date\",\n",
    "        \"transaction_count\",\n",
    "        \"total_sales\",\n",
    "        \"average_unit_price\",\n",
    "        \"total_quantity_sold\",\n",
    "        \"total_discount_given\",\n",
    "        \"total_net_sales\"\n",
    "    )\n",
    "\n",
    "# --- Database Interaction Functions ---\n",
    "\n",
    "def setup_database(engine):\n",
    "    \"\"\"Create schema and set permissions if needed, then create tables (if they don't exist).\"\"\"\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            # Check if schema exists\n",
    "            schema_exists = conn.execute(\n",
    "                text(\"SELECT 1 FROM information_schema.schemata WHERE schema_name = :schema\"),\n",
    "                {'schema': SCHEMA_NAME}\n",
    "            ).scalar()\n",
    "\n",
    "            if not schema_exists:\n",
    "                logger.info(f\"Creating schema {SCHEMA_NAME}\")\n",
    "                conn.execute(text(f\"CREATE SCHEMA {SCHEMA_NAME}\"))\n",
    "                # Grant permissions - adjust user/role as needed\n",
    "                grant_user = os.getenv('DB_USER', 'postgres')\n",
    "                conn.execute(text(f\"\"\"\n",
    "                    GRANT ALL ON SCHEMA {SCHEMA_NAME} TO \"{grant_user}\";\n",
    "                    ALTER DEFAULT PRIVILEGES IN SCHEMA {SCHEMA_NAME}\n",
    "                    GRANT ALL ON TABLES TO \"{grant_user}\";\n",
    "                \"\"\"))\n",
    "                logger.info(f\"Schema {SCHEMA_NAME} created and permissions granted to {grant_user}.\")\n",
    "\n",
    "            # Create all tables if they don't exist (this is safe for existing tables)\n",
    "            # This will add new columns (days_since_birth, net_line_total, transaction_day_of_week)\n",
    "            # to existing tables if they are missing, and create the daily_store_sales table.\n",
    "            logger.info(\"Creating/Ensuring existence of database tables based on models...\")\n",
    "            Base.metadata.create_all(engine)\n",
    "            logger.info(\"Table creation/check complete.\")\n",
    "\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Database setup failed: {str(e)}\", exc_info=True)\n",
    "        return False\n",
    "\n",
    "def load_data_from_postgres_to_spark(spark, db_host, db_port, db_name, db_user, db_password, schema_name, table_name):\n",
    "    \"\"\"Reads a table from PostgreSQL into a Spark DataFrame using JDBC with optional partitioning.\"\"\"\n",
    "    logger.info(f\"Reading data from database table: {schema_name}.{table_name}\")\n",
    "    try:\n",
    "        jdbc_url = f'jdbc:postgresql://{db_host}:{db_port}/{db_name}'\n",
    "        db_properties = {\n",
    "            \"user\": db_user,\n",
    "            \"password\": db_password,\n",
    "            \"driver\": \"org.postgresql.Driver\",\n",
    "            \"fetchsize\": \"10000\" # Helps with large reads\n",
    "        }\n",
    "\n",
    "        # Construct the full table name including schema (quoted for case sensitivity/special chars)\n",
    "        full_table_name = f'\"{schema_name}\".\"{table_name}\"'\n",
    "\n",
    "        # === Add Partitioning Logic Here for Performance ===\n",
    "        # This is crucial for large tables like 'transactions'.\n",
    "        # You need to determine appropriate partitionColumn, lowerBound, upperBound, and numPartitions\n",
    "        partition_options = {}\n",
    "        if table_name == 'transactions':\n",
    "            logger.info(f\"Applying partitioning for {table_name}\")\n",
    "            logger.warning(f\"Partitioning options are NOT set for {table_name}. This will be slow for large tables. Consider adding partitionColumn, lowerBound, upperBound, and numPartitions.\")\n",
    "\n",
    "        spark_df = spark.read.jdbc(\n",
    "            url=jdbc_url,\n",
    "            table=full_table_name,\n",
    "            properties=db_properties,\n",
    "            **partition_options # Pass the partition options\n",
    "        )\n",
    "        logger.info(f\"Successfully read data from {schema_name}.{table_name}. Schema: {spark_df.printSchema()}\")\n",
    "        # logger.info(f\"Number of partitions after reading: {spark_df.rdd.getNumPartitions()}\") # Check partition count\n",
    "        return spark_df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to read data from {schema_name}.{table_name} into Spark: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "def upsert_data(session, table_class, records, primary_keys, batch_size=1000):\n",
    "    \"\"\"Upsert data into the database with batch processing using SQLAlchemy merge.\"\"\"\n",
    "    try:\n",
    "        total_records = len(records)\n",
    "        if total_records == 0:\n",
    "            logger.info(f\"No records to upsert for {table_class.__tablename__}.\")\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Starting to upsert {total_records} records for {table_class.__tablename__} in batches of {batch_size}\")\n",
    "\n",
    "        for i in range(0, total_records, batch_size):\n",
    "            batch = records[i:i + batch_size]\n",
    "            batch_num = (i // batch_size) + 1\n",
    "            total_batches = (total_records - 1) // batch_size + 1\n",
    "\n",
    "            try:\n",
    "                # Use merge for upsert logic, which handles existing primary keys (simple or composite)\n",
    "                for record in batch:\n",
    "                    # Convert Spark Row object to dictionary if needed, or ensure records are dicts\n",
    "                    record_dict = record if isinstance(record, dict) else record.asDict()\n",
    "                    instance = table_class(**record_dict)\n",
    "                    session.merge(instance) # merge handles both insert and update based on PK\n",
    "\n",
    "                session.commit()\n",
    "                logger.info(f\"Successfully processed batch {batch_num}/{total_batches} for {table_class.__tablename__}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                session.rollback()\n",
    "                logger.error(f\"Error processing batch {batch_num} for {table_class.__tablename__}: {str(e)}\", exc_info=True)\n",
    "                # Log affected primary key(s) if possible\n",
    "                try:\n",
    "                    pk_values = [\n",
    "                        {pk: record_dict.get(pk) for pk in primary_keys}\n",
    "                        for record_dict in (record if isinstance(record, dict) else record.asDict() for record in batch)\n",
    "                    ]\n",
    "                    logger.error(f\"Batch primary key values: {pk_values}\")\n",
    "                except Exception as log_e:\n",
    "                    logger.error(f\"Could not log batch primary keys: {log_e}\")\n",
    "\n",
    "                # Decide whether to re-raise or continue. Re-raising stops on first error.\n",
    "                raise # Stop on error\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        session.rollback()\n",
    "        logger.error(f\"Critical error in upsert_data for {table_class.__tablename__}: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "def write_spark_df_to_postgres(spark_df, table_name, engine):\n",
    "    \"\"\"Write Spark DataFrame to PostgreSQL using SQLAlchemy upsert (merge) logic.\"\"\"\n",
    "    try:\n",
    "        # Map table name to SQLAlchemy class name\n",
    "        # Handles 'daily_store_sales' -> 'DailyStoreSales'\n",
    "        table_class_name = ''.join(word.capitalize() for word in table_name.split('_'))\n",
    "        table_class = globals().get(table_class_name)\n",
    "\n",
    "        if not table_class:\n",
    "             logger.error(f\"SQLAlchemy class not found for table name: {table_name}\")\n",
    "             return\n",
    "\n",
    "        # Define primary keys for each table\n",
    "        primary_keys_map = {\n",
    "            'stores': ['store_id'],\n",
    "            'employees': ['employee_id'],\n",
    "            'customers': ['customer_id'],\n",
    "            'products': ['product_id'],\n",
    "            'discounts': ['id'],\n",
    "            'transactions': ['id'], # Assuming 'id' is the PK\n",
    "            'daily_store_sales': ['store_id', 'sale_date'] # Composite key\n",
    "        }\n",
    "        primary_keys = primary_keys_map.get(table_name)\n",
    "\n",
    "        if not primary_keys:\n",
    "            logger.error(f\"Primary key(s) not defined for table {table_name}\")\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Starting write process for table {table_name}...\")\n",
    "\n",
    "        # Convert Spark DataFrame to Pandas DataFrames iteratively\n",
    "        # Using toLocalIterator for larger dataframes to avoid driver memory issues\n",
    "        pandas_iter = spark_df.toLocalIterator()\n",
    "        records_buffer = []\n",
    "        batch_size = 5000 # Batch size for converting Spark rows to dicts and upserting\n",
    "\n",
    "        Session = sessionmaker(bind=engine)\n",
    "        session = Session()\n",
    "        try:\n",
    "            i = 0\n",
    "            for row in pandas_iter:\n",
    "                records_buffer.append(row.asDict()) # Convert Spark Row to dictionary\n",
    "                i += 1\n",
    "\n",
    "                if i % batch_size == 0:\n",
    "                    logger.info(f\"Buffer size reached {batch_size}, processing batch {i // batch_size}\")\n",
    "                    upsert_data(session, table_class, records_buffer, primary_keys)\n",
    "                    records_buffer = [] # Clear buffer\n",
    "                    # Commit is done inside upsert_data for each batch\n",
    "\n",
    "            # Process any remaining records in the buffer\n",
    "            if records_buffer:\n",
    "                logger.info(f\"Processing final batch of {len(records_buffer)} records\")\n",
    "                upsert_data(session, table_class, records_buffer, primary_keys)\n",
    "\n",
    "            # Verify count only for smaller tables or if specifically needed and feasible\n",
    "            # Note: Counting can be slow on large tables, especially after upserts\n",
    "            if table_name in ['stores', 'employees', 'discounts', 'daily_store_sales']: # Only count the tables we loaded/updated\n",
    "                 try:\n",
    "                    with engine.connect() as conn:\n",
    "                        # Use quoted identifiers for the schema and table name\n",
    "                        count = conn.execute(text(f'SELECT COUNT(*) FROM \"{SCHEMA_NAME}\".\"{table_name}\"')).scalar()\n",
    "                        logger.info(f\"Final count for {table_name}: {count} records\")\n",
    "                 except Exception as count_e:\n",
    "                    logger.warning(f\"Could not verify count for {table_name}: {count_e}\")\n",
    "            else:\n",
    "                 logger.info(f\"Skipped final count verification for table {table_name} (read from DB).\")\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "             logger.error(f\"Error during data processing and upsert for {table_name}: {str(e)}\", exc_info=True)\n",
    "             session.rollback() # Rollback any outstanding transaction if an error occurred before a commit\n",
    "             raise\n",
    "        finally:\n",
    "            session.close()\n",
    "            logger.info(f\"Session closed for table {table_name}.\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fatal error in write_spark_df_to_postgres for {table_name}: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "\n",
    "def process_and_aggregate_from_db():\n",
    "    \"\"\"Main function to read data from DB, process, aggregate, and load aggregated data.\"\"\"\n",
    "    # Load environment variables\n",
    "    DB_USER = os.getenv('DB_USER')\n",
    "    DB_PASSWORD = os.getenv('DB_PASSWORD')\n",
    "    DB_HOST = os.getenv('DB_HOST')\n",
    "    DB_PORT = os.getenv('DB_PORT')\n",
    "    DB_NAME = os.getenv('DB_NAME')\n",
    "\n",
    "    required_vars = {\n",
    "        'DB_USER': DB_USER,\n",
    "        'DB_PASSWORD': DB_PASSWORD,\n",
    "        'DB_HOST': DB_HOST,\n",
    "        'DB_NAME': DB_NAME,\n",
    "    }\n",
    "\n",
    "    for var_name, var_value in required_vars.items():\n",
    "        if not var_value:\n",
    "            raise ValueError(f\"Missing required environment variable: {var_name}\")\n",
    "\n",
    "    DB_PORT = DB_PORT if DB_PORT else '5432' # Set default for PORT\n",
    "\n",
    "    engine = None # Initialize engine outside try for finally block\n",
    "\n",
    "    try:\n",
    "        # Create SQLAlchemy engine for schema setup and writing (only for aggregated data)\n",
    "        engine = create_engine(\n",
    "            f'postgresql+psycopg2://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}',\n",
    "            pool_size=15, # Adjust pool size\n",
    "            max_overflow=25, # Adjust max overflow\n",
    "            pool_pre_ping=True,\n",
    "            pool_recycle=3600,\n",
    "            connect_args={'connect_timeout': 10}\n",
    "        )\n",
    "\n",
    "        # Setup Database: Create schema and tables if they don't exist\n",
    "        # This will ensure the daily_store_sales table exists and add FE columns if missing\n",
    "        if not setup_database(engine):\n",
    "            raise RuntimeError(\"Database setup failed\")\n",
    "\n",
    "        # --- Read Data from Database into Spark ---\n",
    "        logger.info(\"Starting to read data from database tables...\")\n",
    "        start_time_read_db = datetime.now()\n",
    "        try:\n",
    "            # Read all necessary tables from the database\n",
    "            customers_df = load_data_from_postgres_to_spark(spark, DB_HOST, DB_PORT, DB_NAME, DB_USER, DB_PASSWORD, SCHEMA_NAME, 'customers')\n",
    "            products_df = load_data_from_postgres_to_spark(spark, DB_HOST, DB_PORT, DB_NAME, DB_USER, DB_PASSWORD, SCHEMA_NAME, 'products')\n",
    "            transactions_df = load_data_from_postgres_to_spark(spark, DB_HOST, DB_PORT, DB_NAME, DB_USER, DB_PASSWORD, SCHEMA_NAME, 'transactions')\n",
    "            stores_df = load_data_from_postgres_to_spark(spark, DB_HOST, DB_PORT, DB_NAME, DB_USER, DB_PASSWORD, SCHEMA_NAME, 'stores')\n",
    "            employees_df = load_data_from_postgres_to_spark(spark, DB_HOST, DB_PORT, DB_NAME, DB_USER, DB_PASSWORD, SCHEMA_NAME, 'employees')\n",
    "            discounts_df = load_data_from_postgres_to_spark(spark, DB_HOST, DB_PORT, DB_NAME, DB_USER, DB_PASSWORD, SCHEMA_NAME, 'discounts')\n",
    "\n",
    "            logger.info(f\"Completed reading data from database in {datetime.now() - start_time_read_db}\")\n",
    "\n",
    "        except Exception as e:\n",
    "             logger.error(f\"Failed to read data from DB. Cannot proceed with processing. Error: {str(e)}\", exc_info=True)\n",
    "             # Reading core data from DB is fatal if it fails\n",
    "             raise\n",
    "\n",
    "        # --- Perform Feature Engineering on DB-read DataFrames ---\n",
    "        logger.info(\"Starting feature engineering on DB-read dataframes...\")\n",
    "        start_time_fe = datetime.now()\n",
    "        try:\n",
    "            # Apply feature engineering functions to the relevant DataFrames\n",
    "            # These functions now expect snake_case columns from the DB read\n",
    "            processed_customer_df = add_customer_features(customers_df)\n",
    "            processed_transaction_df = add_transaction_features(transactions_df)\n",
    "            # No specific FE planned for other tables in this iteration\n",
    "\n",
    "            logger.info(f\"Feature engineering complete in {datetime.now() - start_time_fe}\")\n",
    "\n",
    "        except Exception as e:\n",
    "             logger.error(f\"Error during feature engineering: {str(e)}\", exc_info=True)\n",
    "             # Feature engineering failure is likely fatal as aggregation depends on it\n",
    "             raise\n",
    "\n",
    "        # --- Perform Aggregation and Load New Tables ---\n",
    "        if processed_transaction_df is not None: # Check if transactions data was processed\n",
    "            logger.info(\"Starting aggregation for daily store sales...\")\n",
    "            start_time_agg = datetime.now()\n",
    "            try:\n",
    "                # Aggregate using the feature-engineered transactions DataFrame\n",
    "                daily_sales_df = aggregate_daily_store_sales(processed_transaction_df)\n",
    "                # Load the aggregated data into the new daily_store_sales table\n",
    "                write_spark_df_to_postgres(daily_sales_df, 'daily_store_sales', engine)\n",
    "                logger.info(f\"Completed daily store sales aggregation and loading in {datetime.now() - start_time_agg}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error during daily store sales aggregation or loading: {str(e)}\", exc_info=True)\n",
    "                # Decide whether to stop or continue on aggregation error\n",
    "                # For now, let's treat aggregation failure as fatal\n",
    "                raise\n",
    "\n",
    "        else:\n",
    "            logger.warning(\"No processed transaction data available for aggregation.\")\n",
    "\n",
    "\n",
    "        logger.info(\"Overall data processing and aggregation completed.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fatal error during overall process: {str(e)}\", exc_info=True)\n",
    "        # Re-raise the exception so the main block catches it for process exit code\n",
    "        raise\n",
    "\n",
    "    finally:\n",
    "        # Ensure database engine and Spark session are closed/stopped\n",
    "        if engine:\n",
    "            engine.dispose()\n",
    "            logger.info(\"Database engine disposed.\")\n",
    "        if spark:\n",
    "            spark.stop()\n",
    "            logger.info(\"Spark session stopped.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # This block runs when the script is executed directly\n",
    "    try:\n",
    "        logger.info(\"Starting data processing and aggregation script (reading from DB)...\")\n",
    "        overall_start_time = datetime.now()\n",
    "        process_and_aggregate_from_db() # Call the new main function\n",
    "        overall_end_time = datetime.now()\n",
    "        logger.info(f\"Script finished. Total elapsed time: {overall_end_time - overall_start_time}\")\n",
    "        sys.exit(0) # Exit successfully\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Script terminated due to fatal error: {str(e)}\", exc_info=True)\n",
    "        sys.exit(1) # Exit with a non-zero status code to indicate failure\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
