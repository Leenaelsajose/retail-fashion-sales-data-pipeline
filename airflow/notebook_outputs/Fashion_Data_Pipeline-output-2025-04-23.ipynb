{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeb4b011-d2aa-443a-a94a-0bb182363c9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T02:56:31.068362Z",
     "iopub.status.busy": "2025-04-23T02:56:31.068111Z",
     "iopub.status.idle": "2025-04-23T02:56:31.072153Z",
     "shell.execute_reply": "2025-04-23T02:56:31.071382Z"
    },
    "papermill": {
     "duration": 0.008927,
     "end_time": "2025-04-23T02:56:31.073402",
     "exception": false,
     "start_time": "2025-04-23T02:56:31.064475",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters cell (must be tagged as \"parameters\" in the notebook)\n",
    "execution_date = None\n",
    "dag_run_id = None\n",
    "ds = None\n",
    "ds_nodash = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "549b39fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T02:56:31.078314Z",
     "iopub.status.busy": "2025-04-23T02:56:31.077973Z",
     "iopub.status.idle": "2025-04-23T02:56:31.081305Z",
     "shell.execute_reply": "2025-04-23T02:56:31.080775Z"
    },
    "papermill": {
     "duration": 0.006365,
     "end_time": "2025-04-23T02:56:31.082143",
     "exception": false,
     "start_time": "2025-04-23T02:56:31.075778",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "execution_date = \"2025-04-23T02:56:25.099096+00:00\"\n",
    "dag_run_id = \"manual__2025-04-23T02:56:25.099096+00:00\"\n",
    "ds = \"2025-04-23\"\n",
    "ds_nodash = \"20250423\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22ffd22c-78ab-42db-bbd1-cee4f9b7483f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T02:56:31.086637Z",
     "iopub.status.busy": "2025-04-23T02:56:31.086426Z",
     "iopub.status.idle": "2025-04-23T02:56:32.122245Z",
     "shell.execute_reply": "2025-04-23T02:56:32.121293Z"
    },
    "papermill": {
     "duration": 1.03972,
     "end_time": "2025-04-23T02:56:32.123849",
     "exception": false,
     "start_time": "2025-04-23T02:56:31.084129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in ./megin_mathew/airflow_venv/lib/python3.12/site-packages (0.3.11)\r\n",
      "Requirement already satisfied: pandas in ./megin_mathew/airflow_venv/lib/python3.12/site-packages (2.2.3)\r\n",
      "Requirement already satisfied: psycopg2-binary in ./megin_mathew/airflow_venv/lib/python3.12/site-packages (2.9.10)\r\n",
      "Requirement already satisfied: sqlalchemy in ./megin_mathew/airflow_venv/lib/python3.12/site-packages (1.4.54)\r\n",
      "Requirement already satisfied: packaging in ./megin_mathew/airflow_venv/lib/python3.12/site-packages (from kagglehub) (24.2)\r\n",
      "Requirement already satisfied: pyyaml in ./megin_mathew/airflow_venv/lib/python3.12/site-packages (from kagglehub) (6.0.2)\r\n",
      "Requirement already satisfied: requests in ./megin_mathew/airflow_venv/lib/python3.12/site-packages (from kagglehub) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm in ./megin_mathew/airflow_venv/lib/python3.12/site-packages (from kagglehub) (4.67.1)\r\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./megin_mathew/airflow_venv/lib/python3.12/site-packages (from pandas) (2.2.5)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./megin_mathew/airflow_venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in ./megin_mathew/airflow_venv/lib/python3.12/site-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./megin_mathew/airflow_venv/lib/python3.12/site-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./megin_mathew/airflow_venv/lib/python3.12/site-packages (from sqlalchemy) (3.2.0)\r\n",
      "Requirement already satisfied: six>=1.5 in ./megin_mathew/airflow_venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./megin_mathew/airflow_venv/lib/python3.12/site-packages (from requests->kagglehub) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./megin_mathew/airflow_venv/lib/python3.12/site-packages (from requests->kagglehub) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./megin_mathew/airflow_venv/lib/python3.12/site-packages (from requests->kagglehub) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./megin_mathew/airflow_venv/lib/python3.12/site-packages (from requests->kagglehub) (2025.1.31)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install kagglehub pandas psycopg2-binary sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aebe01f6-0b27-4a7c-8339-4c84690278a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T02:56:32.130337Z",
     "iopub.status.busy": "2025-04-23T02:56:32.130058Z",
     "iopub.status.idle": "2025-04-23T02:56:32.229057Z",
     "shell.execute_reply": "2025-04-23T02:56:32.228350Z"
    },
    "papermill": {
     "duration": 0.103295,
     "end_time": "2025-04-23T02:56:32.230034",
     "exception": false,
     "start_time": "2025-04-23T02:56:32.126739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['KaggleDatasetAdapter', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', 'auth', 'cache', 'clients', 'colab_cache_resolver', 'competition', 'competition_download', 'config', 'dataset_download', 'dataset_load', 'dataset_upload', 'datasets', 'datasets_enums', 'datasets_helpers', 'env', 'exceptions', 'gcs_upload', 'get_package_asset_path', 'handle', 'http_resolver', 'integrity', 'kaggle_cache_resolver', 'kagglehub', 'load_dataset', 'logger', 'login', 'model_download', 'model_upload', 'models', 'models_helpers', 'notebook_output_download', 'notebooks', 'package_import', 'packages', 'registry', 'resolver', 'signing', 'tracker', 'utility_script_install', 'utility_scripts', 'whoami']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/megin_mathew/airflow_venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "print(dir(kagglehub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dc15916-c74b-4cd6-9e1f-79a82761becb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T02:56:32.236915Z",
     "iopub.status.busy": "2025-04-23T02:56:32.236407Z",
     "iopub.status.idle": "2025-04-23T02:56:33.213235Z",
     "shell.execute_reply": "2025-04-23T02:56:33.212153Z"
    },
    "papermill": {
     "duration": 0.981564,
     "end_time": "2025-04-23T02:56:33.214195",
     "exception": false,
     "start_time": "2025-04-23T02:56:32.232631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in ./megin_mathew/airflow_venv/lib/python3.12/site-packages (1.7.4.2)\r\n",
      "Requirement already satisfied: bleach in ./megin_mathew/airflow_venv/lib/python3.12/site-packages (from kaggle) (6.2.0)\r\n",
      "Requirement already satisfied: certifi>=14.05.14 in ./megin_mathew/airflow_venv/lib/python3.12/site-packages (from kaggle) (2025.1.31)\r\n",
      "Requirement already satisfied: charset-normalizer in ./megin_mathew/airflow_venv/lib/python3.12/site-packages (from kaggle) (3.4.1)\r\n",
      "Requirement already satisfied: idna in ./megin_mathew/airflow_venv/lib/python3.12/site-packages (from kaggle) (3.10)\r\n",
      "Requirement already satisfied: protobuf in ./megin_mathew/airflow_venv/lib/python3.12/site-packages (from kaggle) (5.29.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in ./megin_mathew/airflow_venv/lib/python3.12/site-packages (from kaggle) (2.9.0.post0)\r\n",
      "Requirement already satisfied: python-slugify in ./megin_mathew/airflow_venv/lib/python3.12/site-packages (from kaggle) (8.0.4)\r\n",
      "Requirement already satisfied: requests in ./megin_mathew/airflow_venv/lib/python3.12/site-packages (from kaggle) (2.32.3)\r\n",
      "Requirement already satisfied: setuptools>=21.0.0 in ./megin_mathew/airflow_venv/lib/python3.12/site-packages (from kaggle) (79.0.0)\r\n",
      "Requirement already satisfied: six>=1.10 in ./megin_mathew/airflow_venv/lib/python3.12/site-packages (from kaggle) (1.17.0)\r\n",
      "Requirement already satisfied: text-unidecode in ./megin_mathew/airflow_venv/lib/python3.12/site-packages (from kaggle) (1.3)\r\n",
      "Requirement already satisfied: tqdm in ./megin_mathew/airflow_venv/lib/python3.12/site-packages (from kaggle) (4.67.1)\r\n",
      "Requirement already satisfied: urllib3>=1.15.1 in ./megin_mathew/airflow_venv/lib/python3.12/site-packages (from kaggle) (2.4.0)\r\n",
      "Requirement already satisfied: webencodings in ./megin_mathew/airflow_venv/lib/python3.12/site-packages (from kaggle) (0.5.1)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaa93ad1-1263-4f1f-bdee-aa3b4116c65c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T02:56:33.222485Z",
     "iopub.status.busy": "2025-04-23T02:56:33.222114Z",
     "iopub.status.idle": "2025-04-23T02:56:33.297816Z",
     "shell.execute_reply": "2025-04-23T02:56:33.297133Z"
    },
    "papermill": {
     "duration": 0.081475,
     "end_time": "2025-04-23T02:56:33.298745",
     "exception": false,
     "start_time": "2025-04-23T02:56:33.217270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/megin_mathew/.config/kaggle/kaggle.json'\n",
      "Kaggle module is successfully installed!\n"
     ]
    }
   ],
   "source": [
    "import kaggle\n",
    "print(\"Kaggle module is successfully installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdd709ed-dd9d-4679-a689-5b4c22affbe7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T02:56:33.304052Z",
     "iopub.status.busy": "2025-04-23T02:56:33.303616Z",
     "iopub.status.idle": "2025-04-23T02:56:33.307537Z",
     "shell.execute_reply": "2025-04-23T02:56:33.306938Z"
    },
    "papermill": {
     "duration": 0.007194,
     "end_time": "2025-04-23T02:56:33.308214",
     "exception": false,
     "start_time": "2025-04-23T02:56:33.301020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/megin_mathew/.config/kaggle/kaggle.json'\n",
      "Kaggle API authentication successful!\n"
     ]
    }
   ],
   "source": [
    "kaggle.api.authenticate()\n",
    "print(\"Kaggle API authentication successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4db10617-1625-4d69-af65-18e7676ad3db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T02:56:33.313373Z",
     "iopub.status.busy": "2025-04-23T02:56:33.313156Z",
     "iopub.status.idle": "2025-04-23T02:58:02.406313Z",
     "shell.execute_reply": "2025-04-23T02:58:02.404745Z"
    },
    "papermill": {
     "duration": 89.099064,
     "end_time": "2025-04-23T02:58:02.409552",
     "exception": false,
     "start_time": "2025-04-23T02:56:33.310488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: transactions.csv → Stored in: /home/megin_mathew/fashion_dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/megin_mathew/airflow_venv/lib/python3.12/site-packages/kagglehub/pandas_datasets.py:91: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  result = read_function(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: customers.csv → Stored in: /home/megin_mathew/fashion_dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: discounts.csv → Stored in: /home/megin_mathew/fashion_dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: employees.csv → Stored in: /home/megin_mathew/fashion_dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: products.csv → Stored in: /home/megin_mathew/fashion_dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: stores.csv → Stored in: /home/megin_mathew/fashion_dataset\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Define dataset folder (custom cache location)\n",
    "dataset_path = \"/home/megin_mathew/fashion_dataset\"\n",
    "\n",
    "# Set KaggleHub cache override\n",
    "os.environ[\"KAGGLEHUB_CACHE\"] = dataset_path\n",
    "\n",
    "# File names in the dataset\n",
    "file_names = [\n",
    "    \"transactions.csv\", \"customers.csv\", \"discounts.csv\",\n",
    "    \"employees.csv\", \"products.csv\", \"stores.csv\"\n",
    "]\n",
    "\n",
    "# Ensure cache directory exists\n",
    "os.makedirs(dataset_path, exist_ok=True)\n",
    "\n",
    "# Download each file individually\n",
    "for file_name in file_names:\n",
    "    df = kagglehub.dataset_load(\n",
    "        KaggleDatasetAdapter.PANDAS,\n",
    "        \"ricgomes/global-fashion-retail-stores-dataset\",\n",
    "        file_name\n",
    "    )\n",
    "    \n",
    "    # Save to custom cache location\n",
    "    df.to_csv(os.path.join(dataset_path, file_name), index=False)\n",
    "    print(f\"Downloaded: {file_name} → Stored in: {dataset_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3be147cd-fa04-4969-9abf-c7d5147a5996",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T02:58:02.428809Z",
     "iopub.status.busy": "2025-04-23T02:58:02.428453Z",
     "iopub.status.idle": "2025-04-23T02:58:02.436278Z",
     "shell.execute_reply": "2025-04-23T02:58:02.435632Z"
    },
    "papermill": {
     "duration": 0.020943,
     "end_time": "2025-04-23T02:58:02.437067",
     "exception": false,
     "start_time": "2025-04-23T02:58:02.416124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set custom cache location\n",
    "os.environ[\"KAGGLEHUB_CACHE\"] = \"/home/megin_mathew/fashion_dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2d6b94-15e3-4213-a744-4ad1eb5a9de1",
   "metadata": {
    "papermill": {
     "duration": 0.033066,
     "end_time": "2025-04-23T02:58:02.492168",
     "exception": false,
     "start_time": "2025-04-23T02:58:02.459102",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Optimized And Cleansed and uses Hadoop along with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73d3ff1f-8a99-47c8-b7a2-ae4eac67cc1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T02:58:02.593786Z",
     "iopub.status.busy": "2025-04-23T02:58:02.593479Z",
     "iopub.status.idle": "2025-04-23T02:58:06.946311Z",
     "shell.execute_reply": "2025-04-23T02:58:06.945319Z"
    },
    "papermill": {
     "duration": 4.390567,
     "end_time": "2025-04-23T02:58:06.948226",
     "exception": false,
     "start_time": "2025-04-23T02:58:02.557659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/22 22:58:04 WARN Utils: Your hostname, BOOK-KTS1KMKSNJ resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/04/22 22:58:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/22 22:58:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/04/22 22:58:05 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 22:58:06,937 - INFO - Starting data load process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 22:58:06,939 - ERROR - Fatal error: Missing required environment variable: DB_USER\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_19999/3184093236.py\", line 531, in <module>\n",
      "    load_data_to_postgres()\n",
      "  File \"/tmp/ipykernel_19999/3184093236.py\", line 451, in load_data_to_postgres\n",
      "    raise ValueError(f\"Missing required environment variable: {var_name}\")\n",
      "ValueError: Missing required environment variable: DB_USER\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys \n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Float, Date, DateTime, Boolean, ForeignKey, Text, text\n",
    "from sqlalchemy.orm import declarative_base, sessionmaker, relationship\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, to_date, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType, BooleanType\n",
    "\n",
    "# Configure logging\n",
    "log_directory = '/home/megin_mathew/logs/' # Or another directory where the user has permissions\n",
    "log_file_path = os.path.join(log_directory, 'data_loader.log')\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file_path, encoding='utf-8'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Spark with Hadoop workaround for Windows\n",
    "def create_spark_session():\n",
    "    \"\"\"Create and configure Spark session\"\"\"\n",
    "    try:\n",
    "        # Set Hadoop home directory if on Windows\n",
    "        if os.name == 'nt':\n",
    "            os.environ['HADOOP_HOME'] = 'C:\\\\hadoop'\n",
    "            os.environ['PATH'] = os.environ['PATH'] + ';' + os.environ['HADOOP_HOME'] + '\\\\bin'\n",
    "            \n",
    "            # Set Python path explicitly\n",
    "            python_path = sys.executable\n",
    "            os.environ['PYSPARK_PYTHON'] = python_path\n",
    "            os.environ['PYSPARK_DRIVER_PYTHON'] = python_path\n",
    "        \n",
    "        # Create temp directory if it doesn't exist\n",
    "        temp_dir = \"/home/megin_mathew/spark_temp\"\n",
    "        if not os.path.exists(temp_dir):\n",
    "            os.makedirs(temp_dir)\n",
    "            \n",
    "        return SparkSession.builder \\\n",
    "            .appName(\"GlobalFashionRetailDataLoader\") \\\n",
    "            .config(\"spark.local.dir\", temp_dir) \\\n",
    "            .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "            .config(\"spark.executor.memory\", \"8g\") \\\n",
    "            .config(\"spark.driver.memory\", \"8g\") \\\n",
    "            .config(\"spark.memory.offHeap.enabled\", \"false\") \\\n",
    "            .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "            .config(\"spark.default.parallelism\", \"200\") \\\n",
    "            .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "            .config(\"spark.network.timeout\", \"600s\") \\\n",
    "            .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "            .config(\"spark.python.profile\", \"false\") \\\n",
    "            .config(\"spark.executor.instances\", \"4\") \\\n",
    "            .config(\"spark.executor.cores\", \"2\") \\\n",
    "            .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "            .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "            .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"true\") \\\n",
    "            .getOrCreate()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create Spark session: {str(e)}\")\n",
    "        raise\n",
    "# Initialize Spark\n",
    "spark = create_spark_session()\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Database schema definition\n",
    "Base = declarative_base()\n",
    "SCHEMA_NAME = \"GFRetail\"\n",
    "\n",
    "class Stores(Base):\n",
    "    __tablename__ = 'stores'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    store_id = Column(Integer, primary_key=True)\n",
    "    country = Column(String(100), nullable=True)\n",
    "    city = Column(String(100), nullable=True)\n",
    "    store_name = Column(String(100))\n",
    "    number_of_employees = Column(Integer)\n",
    "    zip_code = Column(String(20))\n",
    "    latitude = Column(Float, nullable=True)\n",
    "    longitude = Column(Float, nullable=True)\n",
    "\n",
    "class Employees(Base):\n",
    "    __tablename__ = 'employees'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    employee_id = Column(Integer, primary_key=True)\n",
    "    store_id = Column(Integer, ForeignKey(f'{SCHEMA_NAME}.stores.store_id'))\n",
    "    name = Column(String(255), nullable=True)\n",
    "    position = Column(String(100), nullable=True)\n",
    "\n",
    "class Customers(Base):\n",
    "    __tablename__ = 'customers'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    customer_id = Column(Integer, primary_key=True)\n",
    "    name = Column(String(255), nullable=True)\n",
    "    email = Column(String(255), nullable=True)\n",
    "    join_date = Column(Date, nullable=True)\n",
    "    telephone = Column(String(50), nullable=True)\n",
    "    city = Column(String(100), nullable=True)\n",
    "    country = Column(String(100), nullable=True)\n",
    "    gender = Column(String(20), nullable=True)\n",
    "    date_of_birth = Column(Date, nullable=True)\n",
    "    job_title = Column(String(100), nullable=True)\n",
    "\n",
    "class Products(Base):\n",
    "    __tablename__ = 'products'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    product_id = Column(Integer, primary_key=True)\n",
    "    category = Column(String(100), nullable=True)\n",
    "    sub_category = Column(String(100))\n",
    "    description_pt = Column(Text)\n",
    "    description_de = Column(Text)\n",
    "    description_fr = Column(Text)\n",
    "    description_es = Column(Text)\n",
    "    description_en = Column(Text)\n",
    "    description_zh = Column(Text)\n",
    "    color = Column(String(50), nullable=True)\n",
    "    sizes = Column(String(100), nullable=True)\n",
    "    production_cost = Column(Float)\n",
    "\n",
    "class Discounts(Base):\n",
    "    __tablename__ = 'discounts'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    start_date = Column(Date)\n",
    "    end_date = Column(Date)\n",
    "    discount_rate = Column(Float)\n",
    "    description = Column(String(255), nullable=True)\n",
    "    category = Column(String(100), nullable=True)\n",
    "    sub_category = Column(String(100))\n",
    "\n",
    "class Transactions(Base):\n",
    "    __tablename__ = 'transactions'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    invoice_id = Column(String(50), nullable=True)\n",
    "    line_number = Column(Integer)\n",
    "    customer_id = Column(Integer, ForeignKey(f'{SCHEMA_NAME}.customers.customer_id'))\n",
    "    product_id = Column(Integer, ForeignKey(f'{SCHEMA_NAME}.products.product_id'))\n",
    "    size = Column(String(10))\n",
    "    color = Column(String(50), nullable=True)\n",
    "    unit_price = Column(Float)\n",
    "    quantity = Column(Integer, nullable=True)\n",
    "    transaction_date = Column(DateTime)\n",
    "    discount = Column(Float, nullable=True)\n",
    "    line_total = Column(Float)\n",
    "    store_id = Column(Integer, ForeignKey(f'{SCHEMA_NAME}.stores.store_id'))\n",
    "    employee_id = Column(Integer, ForeignKey(f'{SCHEMA_NAME}.employees.employee_id'))\n",
    "    currency = Column(String(10), nullable=True)\n",
    "    currency_symbol = Column(String(5))\n",
    "    sku = Column(String(100))\n",
    "    transaction_type = Column(String(20))\n",
    "    payment_method = Column(String(50))\n",
    "    invoice_total = Column(Float)\n",
    "    is_return = Column(Boolean)\n",
    "\n",
    "# Data processing functions\n",
    "def process_stores(spark_df):\n",
    "    spark_df = spark_df.withColumn(\"country\", when(col(\"Country\").isNull(), lit(\"Unknown\")).otherwise(col(\"Country\"))) \\\n",
    "                      .withColumn(\"city\", when(col(\"City\").isNull(), lit(\"Unknown\")).otherwise(col(\"City\"))) \\\n",
    "                      .withColumn(\"latitude\", col(\"Latitude\").cast(FloatType())) \\\n",
    "                      .withColumn(\"longitude\", col(\"Longitude\").cast(FloatType()))\n",
    "    \n",
    "    return spark_df.select(\n",
    "        col(\"Store ID\").alias(\"store_id\"),\n",
    "        col(\"country\"),\n",
    "        col(\"city\"),\n",
    "        col(\"Store Name\").alias(\"store_name\"),\n",
    "        col(\"Number of Employees\").alias(\"number_of_employees\"),\n",
    "        col(\"ZIP Code\").alias(\"zip_code\"),\n",
    "        col(\"latitude\"),\n",
    "        col(\"longitude\")\n",
    "    )\n",
    "\n",
    "def process_employees(spark_df):\n",
    "    spark_df = spark_df.withColumn(\"name\", when(col(\"Name\").isNull(), lit(\"Unknown\")).otherwise(col(\"Name\"))) \\\n",
    "                      .withColumn(\"position\", when(col(\"Position\").isNull(), lit(\"Unknown\")).otherwise(col(\"Position\")))\n",
    "    \n",
    "    return spark_df.select(\n",
    "        col(\"Employee ID\").alias(\"employee_id\"),\n",
    "        col(\"Store ID\").alias(\"store_id\"),\n",
    "        col(\"name\"),\n",
    "        col(\"position\")\n",
    "    )\n",
    "\n",
    "def process_customers(spark_df):\n",
    "    spark_df = spark_df.withColumn(\"join_date\", to_date(col(\"Date Of Birth\"), \"yyyy-MM-dd\")) \\\n",
    "                      .withColumn(\"name\", when(col(\"Name\").isNull(), lit(\"Unknown\")).otherwise(col(\"Name\"))) \\\n",
    "                      .withColumn(\"email\", when(col(\"Email\").isNull(), lit(\"unknown@example.com\")).otherwise(col(\"Email\")))\n",
    "    \n",
    "    return spark_df.select(\n",
    "        col(\"Customer ID\").alias(\"customer_id\"),\n",
    "        col(\"name\"),\n",
    "        col(\"email\"),\n",
    "        col(\"join_date\"),\n",
    "        col(\"Telephone\").alias(\"telephone\"),\n",
    "        col(\"City\").alias(\"city\"),\n",
    "        col(\"Country\").alias(\"country\"),\n",
    "        col(\"Gender\").alias(\"gender\"),\n",
    "        to_date(col(\"Date Of Birth\"), \"yyyy-MM-dd\").alias(\"date_of_birth\"),\n",
    "        col(\"Job Title\").alias(\"job_title\")\n",
    "    )\n",
    "\n",
    "def process_products(spark_df):\n",
    "    spark_df = spark_df.withColumn(\"category\", when(col(\"Category\").isNull(), lit(\"Unknown\")).otherwise(col(\"Category\"))) \\\n",
    "                      .withColumn(\"color\", when(col(\"Color\").isNull(), lit(\"Unknown\")).otherwise(col(\"Color\"))) \\\n",
    "                      .withColumn(\"sizes\", when(col(\"Sizes\").isNull(), lit(\"Unknown\")).otherwise(col(\"Sizes\")))\n",
    "    \n",
    "    return spark_df.select(\n",
    "        col(\"Product ID\").alias(\"product_id\"),\n",
    "        col(\"category\"),\n",
    "        col(\"Sub Category\").alias(\"sub_category\"),\n",
    "        col(\"Description PT\").alias(\"description_pt\"),\n",
    "        col(\"Description DE\").alias(\"description_de\"),\n",
    "        col(\"Description FR\").alias(\"description_fr\"),\n",
    "        col(\"Description ES\").alias(\"description_es\"),\n",
    "        col(\"Description EN\").alias(\"description_en\"),\n",
    "        col(\"Description ZH\").alias(\"description_zh\"),\n",
    "        col(\"color\"),\n",
    "        col(\"sizes\"),\n",
    "        col(\"Production Cost\").alias(\"production_cost\")\n",
    "    )\n",
    "\n",
    "def process_discounts(spark_df):\n",
    "    spark_df = spark_df.withColumn(\"description\", when(col(\"Description\").isNull(), lit(\"No description\")).otherwise(col(\"Description\"))) \\\n",
    "                      .withColumn(\"category\", when(col(\"Category\").isNull(), lit(\"General\")).otherwise(col(\"Category\")))\n",
    "    \n",
    "    return spark_df.select(\n",
    "        to_date(col(\"Start\"), \"yyyy-MM-dd\").alias(\"start_date\"),\n",
    "        to_date(col(\"End\"), \"yyyy-MM-dd\").alias(\"end_date\"),\n",
    "        col(\"Discont\").alias(\"discount_rate\"),\n",
    "        col(\"description\"),\n",
    "        col(\"category\"),\n",
    "        col(\"Sub Category\").alias(\"sub_category\")\n",
    "    )\n",
    "\n",
    "def process_transactions(spark_df):\n",
    "    spark_df = spark_df.withColumn(\"invoice_id\", when(col(\"Invoice ID\").isNull(), lit(\"UNKNOWN\")).otherwise(col(\"Invoice ID\"))) \\\n",
    "                      .withColumn(\"quantity\", when(col(\"Quantity\").isNull(), lit(1)).otherwise(col(\"Quantity\"))) \\\n",
    "                      .withColumn(\"color\", when(col(\"Color\").isNull(), lit(\"Unknown\")).otherwise(col(\"Color\"))) \\\n",
    "                      .withColumn(\"discount\", when(col(\"Discount\").isNull(), lit(0.0)).otherwise(col(\"Discount\"))) \\\n",
    "                      .withColumn(\"currency\", when(col(\"Currency\").isNull(), lit(\"USD\")).otherwise(col(\"Currency\"))) \\\n",
    "                      .withColumn(\"is_return\", col(\"Transaction Type\") == \"Return\")\n",
    "    \n",
    "    return spark_df.select(\n",
    "        col(\"invoice_id\"),\n",
    "        col(\"Line\").alias(\"line_number\"),\n",
    "        col(\"Customer ID\").alias(\"customer_id\"),\n",
    "        col(\"Product ID\").alias(\"product_id\"),\n",
    "        col(\"Size\").alias(\"size\"),\n",
    "        col(\"color\"),\n",
    "        col(\"Unit Price\").alias(\"unit_price\"),\n",
    "        col(\"quantity\"),\n",
    "        col(\"Date\").alias(\"transaction_date\"),\n",
    "        col(\"discount\"),\n",
    "        col(\"Line Total\").alias(\"line_total\"),\n",
    "        col(\"Store ID\").alias(\"store_id\"),\n",
    "        col(\"Employee ID\").alias(\"employee_id\"),\n",
    "        col(\"currency\"),\n",
    "        col(\"Currency Symbol\").alias(\"currency_symbol\"),\n",
    "        col(\"SKU\").alias(\"sku\"),\n",
    "        col(\"Transaction Type\").alias(\"transaction_type\"),\n",
    "        col(\"Payment Method\").alias(\"payment_method\"),\n",
    "        col(\"Invoice Total\").alias(\"invoice_total\"),\n",
    "        col(\"is_return\")\n",
    "    )\n",
    "\n",
    "# In setup_database function, add table creation after schema creation\n",
    "def setup_database(engine):\n",
    "    \"\"\"Create schema and set permissions if needed\"\"\"\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            # Check if schema exists\n",
    "            schema_exists = conn.execute(\n",
    "                text(\"SELECT 1 FROM information_schema.schemata WHERE schema_name = :schema\"),\n",
    "                {'schema': SCHEMA_NAME}\n",
    "            ).scalar()\n",
    "            \n",
    "            if not schema_exists:\n",
    "                logger.info(f\"Creating schema {SCHEMA_NAME}\")\n",
    "                conn.execute(text(f\"CREATE SCHEMA {SCHEMA_NAME}\"))\n",
    "                conn.execute(text(f\"\"\"\n",
    "                    GRANT ALL ON SCHEMA {SCHEMA_NAME} TO {os.getenv('DB_USER', 'postgres')};\n",
    "                    ALTER DEFAULT PRIVILEGES IN SCHEMA {SCHEMA_NAME} \n",
    "                    GRANT ALL ON TABLES TO {os.getenv('DB_USER', 'postgres')};\n",
    "                \"\"\"))\n",
    "            \n",
    "            # Create all tables if they don't exist\n",
    "                Base.metadata.create_all(engine)\n",
    "            \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Database setup failed: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def upsert_data(session, table_class, records, primary_key, batch_size=1000):\n",
    "    \"\"\"Upsert data into the database with batch processing\"\"\"\n",
    "    try:\n",
    "        total_records = len(records)\n",
    "        logger.info(f\"Starting to upsert {total_records} records in batches of {batch_size}\")\n",
    "        \n",
    "        for i in range(0, total_records, batch_size):\n",
    "            batch = records[i:i + batch_size]\n",
    "            batch_num = (i // batch_size) + 1\n",
    "            total_batches = (total_records - 1) // batch_size + 1\n",
    "            \n",
    "            try:\n",
    "                session.bulk_insert_mappings(table_class, batch)\n",
    "                session.commit()\n",
    "                logger.info(f\"Successfully inserted batch {batch_num}/{total_batches}\")\n",
    "            except Exception as e:\n",
    "                session.rollback()\n",
    "                logger.info(f\"Insert failed for batch {batch_num}, switching to upsert mode\")\n",
    "                \n",
    "                for record in batch:\n",
    "                    try:\n",
    "                        existing = session.query(table_class).filter(\n",
    "                            getattr(table_class, primary_key) == record[primary_key]\n",
    "                        ).first()\n",
    "                        \n",
    "                        if existing:\n",
    "                            for key, value in record.items():\n",
    "                                setattr(existing, key, value)\n",
    "                        else:\n",
    "                            new_record = table_class(**record)\n",
    "                            session.add(new_record)\n",
    "                    except Exception as rec_error:\n",
    "                        logger.error(f\"Error processing record {record[primary_key]}: {str(rec_error)}\")\n",
    "                        continue\n",
    "                \n",
    "                try:\n",
    "                    session.commit()\n",
    "                    logger.info(f\"Successfully upserted batch {batch_num}/{total_batches}\")\n",
    "                except Exception as commit_error:\n",
    "                    session.rollback()\n",
    "                    logger.error(f\"Failed to commit batch {batch_num}: {str(commit_error)}\")\n",
    "                    continue\n",
    "                \n",
    "    except Exception as e:\n",
    "        session.rollback()\n",
    "        logger.error(f\"Error in upsert_data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def write_spark_df_to_postgres(spark_df, table_name, engine):\n",
    "    \"\"\"Write Spark DataFrame to PostgreSQL with optimized methods\"\"\"\n",
    "    try:\n",
    "        table_class = globals()[table_name.capitalize()]\n",
    "        primary_key = {\n",
    "            'stores': 'store_id',\n",
    "            'employees': 'employee_id',\n",
    "            'customers': 'customer_id',\n",
    "            'products': 'product_id',\n",
    "            'discounts': 'id',\n",
    "            'transactions': 'id'\n",
    "        }.get(table_name)\n",
    "        \n",
    "        if not primary_key:\n",
    "            logger.error(f\"Primary key not defined for table {table_name}\")\n",
    "            return\n",
    "            \n",
    "        # Skip count verification for large tables to avoid Spark worker crashes\n",
    "        if table_name in ['transactions']:  # Add other large tables if needed\n",
    "            total_rows = \"unknown (skipped count for large table)\"\n",
    "        else:\n",
    "            total_rows = spark_df.count()\n",
    "            \n",
    "        logger.info(f\"Processing {total_rows} records for table {table_name}\")\n",
    "        \n",
    "        # For small tables, use direct conversion\n",
    "        if isinstance(total_rows, int) and total_rows <= 10000:\n",
    "            logger.info(f\"Processing small {table_name} table with direct conversion\")\n",
    "            pandas_df = spark_df.toPandas()\n",
    "            records = pandas_df.to_dict('records')\n",
    "            \n",
    "            Session = sessionmaker(bind=engine)\n",
    "            session = Session()\n",
    "            try:\n",
    "                upsert_data(session, table_class, records, primary_key)\n",
    "                \n",
    "                # Verify count using SQLAlchemy which handles schema names correctly\n",
    "                count = session.query(table_class).count()\n",
    "                logger.info(f\"Verified {count} records in {table_name}\")\n",
    "                \n",
    "            finally:\n",
    "                session.close()\n",
    "            return\n",
    "        \n",
    "        # For larger tables\n",
    "        logger.info(f\"Processing large {table_name} table with optimized chunks\")\n",
    "        pandas_iter = spark_df.toLocalIterator() \n",
    "        records_buffer = []\n",
    "        \n",
    "        for i, row in enumerate(pandas_iter, 1):\n",
    "            records_buffer.append(row.asDict())\n",
    "            \n",
    "            if i % 10000 == 0 or (isinstance(total_rows, int) and i == total_rows):\n",
    "                Session = sessionmaker(bind=engine)\n",
    "                session = Session()\n",
    "                try:\n",
    "                    upsert_data(session, table_class, records_buffer, primary_key)\n",
    "                    logger.info(f\"Processed {i} records\")\n",
    "                    records_buffer = []\n",
    "                finally:\n",
    "                    session.close()\n",
    "        \n",
    "        # Skip final verification for large tables\n",
    "        if table_name not in ['transactions']:  # Add other large tables if needed\n",
    "            with engine.connect() as conn:\n",
    "                # Use quoted identifiers for the schema and table name\n",
    "                count = conn.execute(text(f'SELECT COUNT(*) FROM \"{SCHEMA_NAME}\".\"{table_name}\"')).scalar()\n",
    "                logger.info(f\"Final count for {table_name}: {count} records\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error writing to {table_name}: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "def load_data_to_postgres():\n",
    "    \"\"\"Main function to load data to PostgreSQL\"\"\"\n",
    "    #DB_USER = os.getenv('DB_USER', 'postgres')\n",
    "    #DB_PASSWORD = os.getenv('DB_PASSWORD', 'Grandvalley_2026')\n",
    "    #DB_HOST = os.getenv('DB_HOST', 'localhost')\n",
    "    #DB_PORT = os.getenv('DB_PORT', '5432')\n",
    "    #DB_NAME = os.getenv('DB_NAME', 'Global_Fashion_Retail_Stores')\n",
    "    #DATASET_PATH = os.getenv('DATASET_PATH', 'C:/Users/megin/fashion_dataset')\n",
    "    DB_USER = os.getenv('DB_USER')\n",
    "    DB_PASSWORD = os.getenv('DB_PASSWORD')\n",
    "    DB_HOST = os.getenv('DB_HOST')\n",
    "    DB_PORT = os.getenv('DB_PORT')\n",
    "    DB_NAME = os.getenv('DB_NAME')\n",
    "    DATASET_PATH = os.getenv('DATASET_PATH')\n",
    "\n",
    "    required_vars = {\n",
    "    'DB_USER': DB_USER,\n",
    "    'DB_PASSWORD': DB_PASSWORD,\n",
    "    'DB_HOST': DB_HOST,\n",
    "    'DB_NAME': DB_NAME,\n",
    "    'DATASET_PATH': DATASET_PATH\n",
    "    }\n",
    "    \n",
    "    for var_name, var_value in required_vars.items():\n",
    "        if not var_value:\n",
    "            raise ValueError(f\"Missing required environment variable: {var_name}\")\n",
    "    \n",
    "    # Set default for PORT only (since it often doesn't change)\n",
    "    DB_PORT = DB_PORT if DB_PORT else '5432'\n",
    "    try:\n",
    "        engine = create_engine(\n",
    "            f'postgresql+psycopg2://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}',\n",
    "            pool_size=20,\n",
    "            max_overflow=30,\n",
    "            pool_pre_ping=True,\n",
    "            pool_recycle=3600,\n",
    "            connect_args={'connect_timeout': 10}\n",
    "        )\n",
    "        \n",
    "        if not setup_database(engine):\n",
    "            raise RuntimeError(\"Database setup failed\")\n",
    "        \n",
    "        Base.metadata.create_all(engine)\n",
    "        \n",
    "        file_processors = [\n",
    "            ('stores.csv', process_stores, 'stores'),\n",
    "            ('employees.csv', process_employees, 'employees'),\n",
    "            ('products.csv', process_products, 'products'),\n",
    "            ('discounts.csv', process_discounts, 'discounts'),\n",
    "            ('customers.csv', process_customers, 'customers'),\n",
    "            ('transactions.csv', process_transactions, 'transactions')\n",
    "        ]\n",
    "        \n",
    "        for file_name, processor, table_name in file_processors:\n",
    "            file_path = os.path.join(DATASET_PATH, file_name)\n",
    "            if os.path.exists(file_path):\n",
    "                logger.info(f\"Processing {file_name}...\")\n",
    "                start_time = datetime.now()\n",
    "                \n",
    "                try:\n",
    "                    if file_name in ['customers.csv', 'transactions.csv']:\n",
    "                        sample_df = spark.read.csv(file_path, header=True, inferSchema=True, samplingRatio=0.1)\n",
    "                        schema = sample_df.schema\n",
    "                        spark_df = spark.read.csv(file_path, header=True, schema=schema)\n",
    "                    else:\n",
    "                        spark_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "                    \n",
    "                    processed_df = processor(spark_df)\n",
    "                    logger.info(f\"Processed DataFrame schema: {processed_df._jdf.schema().treeString()}\")\n",
    "                    \n",
    "                    write_spark_df_to_postgres(processed_df, table_name, engine)\n",
    "                    \n",
    "                    logger.info(f\"Completed {file_name} in {datetime.now() - start_time}\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing {file_name}: {str(e)}\", exc_info=True)\n",
    "                    \n",
    "                    if file_name == 'products.csv':\n",
    "                        logger.info(\"Attempting alternative approach for products.csv\")\n",
    "                        try:\n",
    "                            products_pd = pd.read_csv(file_path)\n",
    "                            products_spark = spark.createDataFrame(products_pd)\n",
    "                            processed_df = processor(products_spark)\n",
    "                            write_spark_df_to_postgres(processed_df, table_name, engine)\n",
    "                            logger.info(f\"Successfully processed products.csv with alternative approach\")\n",
    "                        except Exception as alt_e:\n",
    "                            logger.error(f\"Alternative approach also failed: {str(alt_e)}\")\n",
    "                            raise\n",
    "                    else:\n",
    "                        raise\n",
    "            else:\n",
    "                logger.warning(f\"File not found: {file_path}\")\n",
    "        \n",
    "        logger.info(\"Data load completed successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Database error: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "    finally:\n",
    "        engine.dispose()\n",
    "        spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        logger.info(\"Starting data load process...\")\n",
    "        start_time = datetime.now()\n",
    "        load_data_to_postgres()\n",
    "        logger.info(f\"Process completed in {datetime.now() - start_time}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fatal error: {str(e)}\", exc_info=True)\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1df72ed-687e-495d-88a1-2458e5ba3892",
   "metadata": {
    "papermill": {
     "duration": 0.003667,
     "end_time": "2025-04-23T02:58:06.957248",
     "exception": false,
     "start_time": "2025-04-23T02:58:06.953581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 99.423272,
   "end_time": "2025-04-23T02:58:09.604007",
   "environment_variables": {},
   "exception": null,
   "input_path": "/home/megin_mathew/airflow/notebooks/Fashion_Data_Pipeline.ipynb",
   "output_path": "/home/megin_mathew/airflow/notebook_outputs/Fashion_Data_Pipeline-output-2025-04-23.ipynb",
   "parameters": {
    "dag_run_id": "manual__2025-04-23T02:56:25.099096+00:00",
    "ds": "2025-04-23",
    "ds_nodash": "20250423",
    "execution_date": "2025-04-23T02:56:25.099096+00:00"
   },
   "start_time": "2025-04-23T02:56:30.180735",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}