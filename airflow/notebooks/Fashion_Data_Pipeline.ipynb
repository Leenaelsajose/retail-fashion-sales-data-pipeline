{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c89edbe2",
   "metadata": {},
   "source": [
    "Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ffd22c-78ab-42db-bbd1-cee4f9b7483f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kagglehub kaggle pandas psycopg2-binary sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b7e147",
   "metadata": {},
   "source": [
    "Import Kaggle and Authenticate to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebe01f6-0b27-4a7c-8339-4c84690278a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import kaggle\n",
    "print(dir(kagglehub))\n",
    "print(\"Kaggle module is successfully installed!\")\n",
    "kaggle.api.authenticate()\n",
    "print(\"Kaggle API authentication successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bead18a8",
   "metadata": {},
   "source": [
    "Pull Data from Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31780d0c",
   "metadata": {},
   "source": [
    "Import the Retail Fashion Dataset from Kaggle \n",
    "The dataset consists of 6 csv file\n",
    "- transactiions.csv - 773 MB in size and has 6.41 million records\n",
    "- customers.csv - 183 MB in size and has 1.64 million records\n",
    "- products.csv - 4.77 MB in size and has 17940 records\n",
    "- discounts.csv - 18 KB in size and has 3801 records\n",
    "- employees.csv - 15.2 KB in size and has 404 records\n",
    "- stores.csv - 3 KB in size and has 35 records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db10617-1625-4d69-af65-18e7676ad3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Define dataset folder (custom cache location)\n",
    "dataset_path = \"/home/megin_mathew/fashion_dataset\"\n",
    "\n",
    "# Set KaggleHub cache override\n",
    "os.environ[\"KAGGLEHUB_CACHE\"] = dataset_path\n",
    "\n",
    "# File names in the dataset\n",
    "file_names = [\n",
    "    \"transactions.csv\", \"customers.csv\", \"discounts.csv\",\n",
    "    \"employees.csv\", \"products.csv\", \"stores.csv\"\n",
    "]\n",
    "\n",
    "# Ensure cache directory exists\n",
    "os.makedirs(dataset_path, exist_ok=True)\n",
    "\n",
    "# Download each file individually\n",
    "for file_name in file_names:\n",
    "    df = kagglehub.dataset_load(\n",
    "        KaggleDatasetAdapter.PANDAS,\n",
    "        \"ricgomes/global-fashion-retail-stores-dataset\",\n",
    "        file_name\n",
    "    )\n",
    "    \n",
    "    # Save to custom cache location\n",
    "    df.to_csv(os.path.join(dataset_path, file_name), index=False)\n",
    "    print(f\"Downloaded: {file_name} → Stored in: {dataset_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f969f8",
   "metadata": {},
   "source": [
    "Set Kaggle Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be147cd-fa04-4969-9abf-c7d5147a5996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set custom cache location\n",
    "os.environ[\"KAGGLEHUB_CACHE\"] = \"/home/megin_mathew/fashion_dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2d6b94-15e3-4213-a744-4ad1eb5a9de1",
   "metadata": {},
   "source": [
    "# Global Fashion Retail Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146a6d84",
   "metadata": {},
   "source": [
    "## Initial Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc11508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('data_pipeline.log', encoding='utf-8'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(\"/home/megin_mathew/airflow/notebooks/.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f504e5",
   "metadata": {},
   "source": [
    "## Database Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496be88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import (\n",
    "    create_engine, Column, Integer, String, Float, Date, DateTime,\n",
    "    Boolean, ForeignKey, Text, text, PrimaryKeyConstraint, inspect\n",
    ")\n",
    "from sqlalchemy.orm import declarative_base, sessionmaker\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "Base = declarative_base()\n",
    "SCHEMA_NAME = \"GFRetail\"\n",
    "\n",
    "class Stores(Base):\n",
    "    __tablename__ = 'stores'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    store_id = Column(Integer, primary_key=True)\n",
    "    country = Column(String(100), nullable=True)\n",
    "    city = Column(String(100), nullable=True)\n",
    "    store_name = Column(String(100))\n",
    "    number_of_employees = Column(Integer)\n",
    "    zip_code = Column(String(20))\n",
    "    latitude = Column(Float, nullable=True)\n",
    "    longitude = Column(Float, nullable=True)\n",
    "\n",
    "class Employees(Base):\n",
    "    __tablename__ = 'employees'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    employee_id = Column(Integer, primary_key=True)\n",
    "    store_id = Column(Integer, ForeignKey(f'{SCHEMA_NAME}.stores.store_id'))\n",
    "    name = Column(String(255), nullable=True)\n",
    "    position = Column(String(100), nullable=True)\n",
    "\n",
    "class Customers(Base):\n",
    "    __tablename__ = 'customers'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    customer_id = Column(Integer, primary_key=True)\n",
    "    name = Column(String(255), nullable=True)\n",
    "    email = Column(String(255), nullable=True)\n",
    "    join_date = Column(Date, nullable=True)\n",
    "    telephone = Column(String(50), nullable=True)\n",
    "    city = Column(String(100), nullable=True)\n",
    "    country = Column(String(100), nullable=True)\n",
    "    gender = Column(String(20), nullable=True)\n",
    "    date_of_birth = Column(Date, nullable=True)\n",
    "    job_title = Column(String(100), nullable=True)\n",
    "    days_since_birth = Column(Integer, nullable=True)\n",
    "\n",
    "class Products(Base):\n",
    "    __tablename__ = 'products'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    product_id = Column(Integer, primary_key=True)\n",
    "    category = Column(String(100), nullable=True)\n",
    "    sub_category = Column(String(100))\n",
    "    description_pt = Column(Text)\n",
    "    description_de = Column(Text)\n",
    "    description_fr = Column(Text)\n",
    "    description_es = Column(Text)\n",
    "    description_en = Column(Text)\n",
    "    description_zh = Column(Text)\n",
    "    color = Column(String(50), nullable=True)\n",
    "    sizes = Column(String(100), nullable=True)\n",
    "    production_cost = Column(Float)\n",
    "\n",
    "class Discounts(Base):\n",
    "    __tablename__ = 'discounts'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    start_date = Column(Date)\n",
    "    end_date = Column(Date)\n",
    "    discount_rate = Column(Float)\n",
    "    description = Column(String(255), nullable=True)\n",
    "    category = Column(String(100), nullable=True)\n",
    "    sub_category = Column(String(100))\n",
    "\n",
    "class Transactions(Base):\n",
    "    __tablename__ = 'transactions'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    invoice_id = Column(String(50), nullable=True)\n",
    "    line_number = Column(Integer)\n",
    "    customer_id = Column(Integer, ForeignKey(f'{SCHEMA_NAME}.customers.customer_id'))\n",
    "    product_id = Column(Integer, ForeignKey(f'{SCHEMA_NAME}.products.product_id'))\n",
    "    size = Column(String(10))\n",
    "    color = Column(String(50), nullable=True)\n",
    "    unit_price = Column(Float)\n",
    "    quantity = Column(Integer, nullable=True)\n",
    "    transaction_date = Column(DateTime)\n",
    "    discount = Column(Float, nullable=True)\n",
    "    line_total = Column(Float)\n",
    "    store_id = Column(Integer, ForeignKey(f'{SCHEMA_NAME}.stores.store_id'))\n",
    "    employee_id = Column(Integer, ForeignKey(f'{SCHEMA_NAME}.employees.employee_id'))\n",
    "    currency = Column(String(10), nullable=True)\n",
    "    currency_symbol = Column(String(5))\n",
    "    sku = Column(String(100))\n",
    "    transaction_type = Column(String(20))\n",
    "    payment_method = Column(String(50))\n",
    "    invoice_total = Column(Float)\n",
    "    is_return = Column(Boolean)\n",
    "    net_line_total = Column(Float, nullable=True)\n",
    "    transaction_day_of_week = Column(Integer, nullable=True)\n",
    "\n",
    "class DailyStoreSales(Base):\n",
    "    __tablename__ = 'daily_store_sales'\n",
    "    __table_args__ = (\n",
    "        PrimaryKeyConstraint('store_id', 'sale_date'),\n",
    "        {'schema': SCHEMA_NAME}\n",
    "    )\n",
    "    store_id = Column(Integer, ForeignKey(f'{SCHEMA_NAME}.stores.store_id'), nullable=False)\n",
    "    sale_date = Column(Date, nullable=False)\n",
    "    transaction_count = Column(Integer, nullable=True)\n",
    "    total_sales = Column(Float, nullable=True)\n",
    "    average_unit_price = Column(Float, nullable=True)\n",
    "    total_quantity_sold = Column(Integer, nullable=True)\n",
    "    total_discount_given = Column(Float, nullable=True)\n",
    "    total_net_sales = Column(Float, nullable=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea3502d",
   "metadata": {},
   "source": [
    "## Spark Session Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60126efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def create_spark_session():\n",
    "    \"\"\"Create and configure Spark session.\"\"\"\n",
    "    try:\n",
    "        if os.name == 'nt':\n",
    "            hadoop_home = os.getenv('HADOOP_HOME_WIN', 'C:\\\\hadoop')\n",
    "            os.environ['HADOOP_HOME'] = hadoop_home\n",
    "            os.environ['PATH'] = os.environ['PATH'] + ';' + os.path.join(hadoop_home, 'bin')\n",
    "            os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "            os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "        temp_dir = os.getenv('SPARK_TEMP_DIR',  \"/home/megin_mathew/spark_temp\")\n",
    "        if not os.path.exists(temp_dir):\n",
    "            os.makedirs(temp_dir)\n",
    "\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"FashionRetailETL\") \\\n",
    "            .config(\"spark.local.dir\", temp_dir) \\\n",
    "            .config(\"spark.executor.memory\", \"8g\") \\\n",
    "            .config(\"spark.driver.memory\", \"8g\") \\\n",
    "            .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "        logger.info(\"Spark session created successfully\")\n",
    "        return spark\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create Spark session: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ce89ab",
   "metadata": {},
   "source": [
    "## Extraction Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b28f402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_csv(spark, file_path):\n",
    "    \"\"\"Extract data from CSV file.\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Extracting data from {file_path}\")\n",
    "        if 'transactions' in file_path.lower() or 'customers' in file_path.lower():\n",
    "            # For large files, use sampling to infer schema\n",
    "            sample_df = spark.read.csv(file_path, header=True, inferSchema=True, samplingRatio=0.1)\n",
    "            return spark.read.csv(file_path, header=True, schema=sample_df.schema)\n",
    "        return spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting {file_path}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def extract_postgres(spark, table_name):\n",
    "    \"\"\"Extract data from PostgreSQL.\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Extracting {table_name} from PostgreSQL\")\n",
    "        jdbc_url = f\"jdbc:postgresql://{os.getenv('DB_HOST')}:{os.getenv('DB_PORT', '5432')}/{os.getenv('DB_NAME')}\"\n",
    "        properties = {\n",
    "            \"user\": os.getenv('DB_USER'),\n",
    "            \"password\": os.getenv('DB_PASSWORD'),\n",
    "            \"driver\": \"org.postgresql.Driver\",\n",
    "            \"fetchsize\": \"10000\"\n",
    "        }\n",
    "        return spark.read.jdbc(\n",
    "            url=jdbc_url,\n",
    "            table=f'\"{SCHEMA_NAME}\".\"{table_name}\"',\n",
    "            properties=properties\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting {table_name}: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23a6d44",
   "metadata": {},
   "source": [
    "## Transformation Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9782fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, when, lit, to_date, datediff, current_date,\n",
    "    dayofweek, to_timestamp, count, sum, avg ,  floor, months_between\n",
    ")\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "def transform_stores(df):\n",
    "    \"\"\"Transform stores data.\"\"\"\n",
    "    return df.withColumn(\"country\", when(col(\"Country\").isNull(), lit(\"Unknown\")).otherwise(col(\"Country\"))) \\\n",
    "            .withColumn(\"city\", when(col(\"City\").isNull(), lit(\"Unknown\")).otherwise(col(\"City\"))) \\\n",
    "            .select(\n",
    "                col(\"Store ID\").alias(\"store_id\"),\n",
    "                col(\"country\"),\n",
    "                col(\"city\"),\n",
    "                col(\"Store Name\").alias(\"store_name\"),\n",
    "                col(\"Number of Employees\").alias(\"number_of_employees\"),\n",
    "                col(\"ZIP Code\").alias(\"zip_code\"),\n",
    "                col(\"Latitude\").cast(FloatType()).alias(\"latitude\"),\n",
    "                col(\"Longitude\").cast(FloatType()).alias(\"longitude\")\n",
    "            )\n",
    "\n",
    "def transform_employees(df):\n",
    "    \"\"\"Transform employees data.\"\"\"\n",
    "    return df.withColumn(\"name\", when(col(\"Name\").isNull(), lit(\"Unknown\")).otherwise(col(\"Name\"))) \\\n",
    "            .withColumn(\"position\", when(col(\"Position\").isNull(), lit(\"Unknown\")).otherwise(col(\"Position\"))) \\\n",
    "            .select(\n",
    "                col(\"Employee ID\").alias(\"employee_id\"),\n",
    "                col(\"Store ID\").alias(\"store_id\"),\n",
    "                col(\"name\"),\n",
    "                col(\"position\")\n",
    "            )\n",
    "\n",
    "\n",
    "def transform_customers(df):\n",
    "    \"\"\"Transform customers data with feature engineering.\"\"\"\n",
    "    # First check if required columns exist\n",
    "    for col_name in [\"Join Date\", \"Date Of Birth\", \"Email\", \"Name\"]:\n",
    "        if col_name not in df.columns:\n",
    "            df = df.withColumn(col_name, lit(None))\n",
    "    \n",
    "    return (\n",
    "        df\n",
    "        # Handle NULL names → \"Unknown\"\n",
    "        .withColumn(\n",
    "            \"name\",\n",
    "            when(col(\"Name\").isNull(), lit(\"Unknown\"))  \n",
    "            .otherwise(col(\"Name\"))\n",
    "        )\n",
    "        # Handle NULL emails → \"unknown@example.com\"\n",
    "        .withColumn(\n",
    "            \"email\",\n",
    "            when(col(\"Email\").isNull(), lit(\"unknown@example.com\"))\n",
    "            .otherwise(col(\"Email\"))\n",
    "        )\n",
    "        # Convert Join Date to date format\n",
    "        .withColumn(\n",
    "            \"join_date\",\n",
    "            to_date(col(\"Join Date\"), \"yyyy-MM-dd\")\n",
    "        )\n",
    "        # Convert Date Of Birth to date (default to 1900-01-01 if NULL)\n",
    "        .withColumn(\n",
    "            \"date_of_birth\",\n",
    "            when(\n",
    "                col(\"Date Of Birth\").isNotNull(),\n",
    "                to_date(col(\"Date Of Birth\"), \"yyyy-MM-dd\")\n",
    "            )\n",
    "            .otherwise(to_date(lit(\"1900-01-01\"), \"yyyy-MM-dd\"))\n",
    "        )\n",
    "        # Calculate age in years\n",
    "        .withColumn(\n",
    "            \"age\",\n",
    "            floor(months_between(current_date(), col(\"date_of_birth\")) / 12).cast(\"integer\")\n",
    "        )\n",
    "        # Select and rename columns\n",
    "        .select(\n",
    "            col(\"Customer ID\").alias(\"customer_id\"),\n",
    "            col(\"name\"),\n",
    "            col(\"email\"),\n",
    "            col(\"join_date\"),\n",
    "            col(\"Telephone\").alias(\"telephone\"),\n",
    "            col(\"City\").alias(\"city\"),\n",
    "            col(\"Country\").alias(\"country\"),\n",
    "            col(\"Gender\").alias(\"gender\"),\n",
    "            col(\"date_of_birth\"),\n",
    "            col(\"Job Title\").alias(\"job_title\"),\n",
    "            col(\"age\")\n",
    "        )\n",
    "    )\n",
    "            \n",
    "def transform_products(df):\n",
    "    \"\"\"Transform products data.\"\"\"\n",
    "    return df.withColumn(\"category\", when(col(\"Category\").isNull(), lit(\"Unknown\")).otherwise(col(\"Category\"))) \\\n",
    "            .withColumn(\"color\", when(col(\"Color\").isNull(), lit(\"Unknown\")).otherwise(col(\"Color\"))) \\\n",
    "            .select(\n",
    "                col(\"Product ID\").alias(\"product_id\"),\n",
    "                col(\"category\"),\n",
    "                col(\"Sub Category\").alias(\"sub_category\"),\n",
    "                col(\"Description PT\").alias(\"description_pt\"),\n",
    "                col(\"Description DE\").alias(\"description_de\"),\n",
    "                col(\"Description FR\").alias(\"description_fr\"),\n",
    "                col(\"Description ES\").alias(\"description_es\"),\n",
    "                col(\"Description EN\").alias(\"description_en\"),\n",
    "                col(\"Description ZH\").alias(\"description_zh\"),\n",
    "                col(\"color\"),\n",
    "                col(\"Sizes\").alias(\"sizes\"),\n",
    "                col(\"Production Cost\").alias(\"production_cost\")\n",
    "            )\n",
    "\n",
    "def transform_discounts(df):\n",
    "    \"\"\"Transform discounts data.\"\"\"\n",
    "    return df.withColumn(\"description\", when(col(\"Description\").isNull(), lit(\"No description\")).otherwise(col(\"Description\"))) \\\n",
    "            .select(\n",
    "                to_date(col(\"Start\"), \"yyyy-MM-dd\").alias(\"start_date\"),\n",
    "                to_date(col(\"End\"), \"yyyy-MM-dd\").alias(\"end_date\"),\n",
    "                col(\"Discont\").alias(\"discount_rate\"),\n",
    "                col(\"description\"),\n",
    "                col(\"Category\").alias(\"category\"),\n",
    "                col(\"Sub Category\").alias(\"sub_category\")\n",
    "            )\n",
    "\n",
    "def transform_transactions(df):\n",
    "    \"\"\"Transform transactions data with feature engineering.\"\"\"\n",
    "    return df.withColumn(\"invoice_id\", when(col(\"Invoice ID\").isNull(), lit(\"UNKNOWN\")).otherwise(col(\"Invoice ID\"))) \\\n",
    "            .withColumn(\"quantity\", when(col(\"Quantity\").isNull(), lit(1)).otherwise(col(\"Quantity\"))) \\\n",
    "            .withColumn(\"discount\", when(col(\"Discount\").isNull(), lit(0.0)).otherwise(col(\"Discount\"))) \\\n",
    "            .withColumn(\"is_return\", col(\"Transaction Type\") == \"Return\") \\\n",
    "            .withColumn(\"transaction_date\", to_timestamp(col(\"Date\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "            .withColumn(\"net_line_total\", (col(\"Unit Price\") * col(\"quantity\")) - col(\"discount\")) \\\n",
    "            .withColumn(\"transaction_day_of_week\", dayofweek(to_timestamp(col(\"Date\"), \"yyyy-MM-dd HH:mm:ss\"))) \\\n",
    "            .select(\n",
    "                col(\"invoice_id\"),\n",
    "                col(\"Line\").alias(\"line_number\"),\n",
    "                col(\"Customer ID\").alias(\"customer_id\"),\n",
    "                col(\"Product ID\").alias(\"product_id\"),\n",
    "                col(\"Size\").alias(\"size\"),\n",
    "                col(\"Color\").alias(\"color\"),\n",
    "                col(\"Unit Price\").alias(\"unit_price\"),\n",
    "                col(\"quantity\"),\n",
    "                col(\"transaction_date\"),\n",
    "                col(\"discount\"),\n",
    "                col(\"Line Total\").alias(\"line_total\"),\n",
    "                col(\"Store ID\").alias(\"store_id\"),\n",
    "                col(\"Employee ID\").alias(\"employee_id\"),\n",
    "                col(\"Currency\").alias(\"currency\"),\n",
    "                col(\"Currency Symbol\").alias(\"currency_symbol\"),\n",
    "                col(\"SKU\").alias(\"sku\"),\n",
    "                col(\"Transaction Type\").alias(\"transaction_type\"),\n",
    "                col(\"Payment Method\").alias(\"payment_method\"),\n",
    "                col(\"Invoice Total\").alias(\"invoice_total\"),\n",
    "                col(\"is_return\"),\n",
    "                col(\"net_line_total\"),\n",
    "                col(\"transaction_day_of_week\")\n",
    "            )\n",
    "\n",
    "def aggregate_daily_sales(df):\n",
    "    \"\"\"Aggregate transactions to daily store sales.\"\"\"\n",
    "    return df.withColumn(\"sale_date\", to_date(col(\"transaction_date\"))) \\\n",
    "            .groupBy(\"store_id\", \"sale_date\") \\\n",
    "            .agg(\n",
    "                count(\"*\").alias(\"transaction_count\"),\n",
    "                sum(\"line_total\").alias(\"total_sales\"),\n",
    "                avg(\"unit_price\").alias(\"average_unit_price\"),\n",
    "                sum(\"quantity\").alias(\"total_quantity_sold\"),\n",
    "                sum(\"discount\").alias(\"total_discount_given\"),\n",
    "                sum(\"net_line_total\").alias(\"total_net_sales\")\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3b2b26",
   "metadata": {},
   "source": [
    "## Database Module(Load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f34a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_database(engine):\n",
    "    \"\"\"Initialize database schema and tables.\"\"\"\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            # Create schema if not exists\n",
    "            if not conn.execute(\n",
    "                text(\"SELECT 1 FROM information_schema.schemata WHERE schema_name = :schema\"),\n",
    "                {'schema': SCHEMA_NAME}\n",
    "            ).scalar():\n",
    "                logger.info(f\"Creating schema {SCHEMA_NAME}\")\n",
    "                conn.execute(text(f\"CREATE SCHEMA {SCHEMA_NAME}\"))\n",
    "                grant_user = os.getenv('DB_USER', 'postgres')\n",
    "                conn.execute(text(f\"\"\"\n",
    "                    GRANT ALL ON SCHEMA {SCHEMA_NAME} TO \"{grant_user}\";\n",
    "                    ALTER DEFAULT PRIVILEGES IN SCHEMA {SCHEMA_NAME}\n",
    "                    GRANT ALL ON TABLES TO \"{grant_user}\";\n",
    "                \"\"\"))\n",
    "\n",
    "            # Create tables and add missing columns\n",
    "            Base.metadata.create_all(engine)\n",
    "            inspector = inspect(engine)\n",
    "            \n",
    "            for table_name in Base.metadata.tables.keys():\n",
    "                if not inspector.has_table(table_name, schema=SCHEMA_NAME):\n",
    "                    continue\n",
    "                    \n",
    "                table = Base.metadata.tables[f\"{SCHEMA_NAME}.{table_name}\"]\n",
    "                columns = inspector.get_columns(table_name, schema=SCHEMA_NAME)\n",
    "                existing_columns = {c['name'] for c in columns}\n",
    "                \n",
    "                for column in table.columns:\n",
    "                    if column.name not in existing_columns:\n",
    "                        logger.info(f\"Adding column {column.name} to {table_name}\")\n",
    "                        column_type = column.type.compile(engine.dialect)\n",
    "                        conn.execute(text(\n",
    "                            f'ALTER TABLE \"{SCHEMA_NAME}\".\"{table_name}\" '\n",
    "                            f'ADD COLUMN \"{column.name}\" {column_type}'\n",
    "                        ))\n",
    "            \n",
    "        logger.info(\"Database setup completed\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Database setup failed: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def create_db_engine():\n",
    "    \"\"\"Create SQLAlchemy engine.\"\"\"\n",
    "    return create_engine(\n",
    "        f'postgresql+psycopg2://{os.getenv(\"DB_USER\")}:{os.getenv(\"DB_PASSWORD\")}@'\n",
    "        f'{os.getenv(\"DB_HOST\")}:{os.getenv(\"DB_PORT\", \"5432\")}/{os.getenv(\"DB_NAME\")}',\n",
    "        pool_size=20,\n",
    "        max_overflow=30,\n",
    "        pool_pre_ping=True,\n",
    "        pool_recycle=3600,\n",
    "        connect_args={'connect_timeout': 10}\n",
    "    )\n",
    "\n",
    "def load_data_to_postgres(spark_df, table_name, engine):\n",
    "    \"\"\"Load Spark DataFrame to PostgreSQL with proper batch processing.\"\"\"\n",
    "    try:\n",
    "        table_class = globals().get(table_name.capitalize())\n",
    "        if not table_class:\n",
    "            logger.error(f\"No model class found for {table_name}\")\n",
    "            return\n",
    "\n",
    "        primary_keys = {\n",
    "            'stores': ['store_id'],\n",
    "            'employees': ['employee_id'],\n",
    "            'customers': ['customer_id'],\n",
    "            'products': ['product_id'],\n",
    "            'discounts': ['id'],\n",
    "            'transactions': ['id'],\n",
    "            'daily_store_sales': ['store_id', 'sale_date']\n",
    "        }.get(table_name)\n",
    "\n",
    "        if not primary_keys:\n",
    "            logger.error(f\"No primary key defined for {table_name}\")\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Starting to load data to {table_name}...\")\n",
    "        \n",
    "        # Convert to Pandas DataFrame in chunks\n",
    "        pandas_df = spark_df.toPandas()\n",
    "        \n",
    "        # Process in batches\n",
    "        batch_size = 1000\n",
    "        total_records = len(pandas_df)\n",
    "        num_batches = (total_records // batch_size) + 1\n",
    "        \n",
    "        Session = sessionmaker(bind=engine)\n",
    "        session = Session()\n",
    "        \n",
    "        try:\n",
    "            for i in range(0, total_records, batch_size):\n",
    "                batch = pandas_df.iloc[i:i + batch_size]\n",
    "                records = batch.to_dict('records')\n",
    "                \n",
    "                try:\n",
    "                    # First try bulk insert for performance\n",
    "                    session.bulk_insert_mappings(table_class, records)\n",
    "                    session.commit()\n",
    "                    logger.info(f\"Processed batch {i//batch_size + 1}/{num_batches}\")\n",
    "                except Exception as bulk_error:\n",
    "                    session.rollback()\n",
    "                    logger.warning(f\"Bulk insert failed, switching to individual upserts: {str(bulk_error)}\")\n",
    "                    \n",
    "                    for record in records:\n",
    "                        try:\n",
    "                            existing = session.query(table_class).filter_by(**{\n",
    "                                pk: record[pk] for pk in primary_keys\n",
    "                            }).first()\n",
    "                            \n",
    "                            if existing:\n",
    "                                for key, value in record.items():\n",
    "                                    setattr(existing, key, value)\n",
    "                            else:\n",
    "                                session.add(table_class(**record))\n",
    "                            session.commit()\n",
    "                        except Exception as rec_error:\n",
    "                            session.rollback()\n",
    "                            logger.error(f\"Error processing record: {str(rec_error)}\")\n",
    "                            continue\n",
    "        \n",
    "            # Verify count for non-large tables\n",
    "            if table_name not in ['transactions', 'daily_store_sales']:\n",
    "                count = session.query(table_class).count()\n",
    "                logger.info(f\"Verified {count} records in {table_name}\")\n",
    "                \n",
    "        finally:\n",
    "            session.close()\n",
    "            \n",
    "        logger.info(f\"Successfully loaded data to {table_name}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading to {table_name}: {str(e)}\", exc_info=True)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dc4741",
   "metadata": {},
   "source": [
    "## Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2a7a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline():\n",
    "    \"\"\"Main ETL pipeline execution.\"\"\"\n",
    "    spark = None\n",
    "    engine = None\n",
    "    \n",
    "    try:\n",
    "        # Initialize\n",
    "        spark = create_spark_session()\n",
    "        engine = create_db_engine()\n",
    "        \n",
    "        if not setup_database(engine):\n",
    "            raise RuntimeError(\"Database setup failed\")\n",
    "        \n",
    "        # Process each data file\n",
    "        data_files = [\n",
    "            ('stores.csv', transform_stores, 'stores'),\n",
    "            ('employees.csv', transform_employees, 'employees'),\n",
    "            ('products.csv', transform_products, 'products'),\n",
    "            ('discounts.csv', transform_discounts, 'discounts'),\n",
    "            ('customers.csv', transform_customers, 'customers'),\n",
    "            ('transactions.csv', transform_transactions, 'transactions')\n",
    "        ]\n",
    "        \n",
    "        for file_name, transform_func, table_name in data_files:\n",
    "            file_path = os.path.join(os.getenv('DATASET_PATH'), file_name)\n",
    "            if not os.path.exists(file_path):\n",
    "                logger.warning(f\"File not found: {file_path}\")\n",
    "                continue\n",
    "                \n",
    "            logger.info(f\"Processing {file_name}...\")\n",
    "            start_time = datetime.now()\n",
    "            \n",
    "            try:\n",
    "                df = extract_csv(spark, file_path)\n",
    "                transformed_df = transform_func(df)\n",
    "                load_data_to_postgres(transformed_df, table_name, engine)\n",
    "                logger.info(f\"Completed {file_name} in {datetime.now() - start_time}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing {file_name}: {str(e)}\")\n",
    "                if file_name == 'products.csv':\n",
    "                    logger.info(\"Trying alternative approach for products\")\n",
    "                    try:\n",
    "                        products_pd = pd.read_csv(file_path)\n",
    "                        products_spark = spark.createDataFrame(products_pd)\n",
    "                        transformed_df = transform_func(products_spark)\n",
    "                        load_data_to_postgres(transformed_df, table_name, engine)\n",
    "                    except Exception as alt_e:\n",
    "                        logger.error(f\"Alternative approach failed: {str(alt_e)}\")\n",
    "                        raise\n",
    "                else:\n",
    "                    raise\n",
    "        \n",
    "        # Generate and load daily sales aggregations\n",
    "        logger.info(\"Generating daily sales aggregations...\")\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        transactions_df = extract_postgres(spark, 'transactions')\n",
    "        daily_sales_df = aggregate_daily_sales(transactions_df)\n",
    "        load_data_to_postgres(daily_sales_df, 'daily_store_sales', engine)\n",
    "        \n",
    "        logger.info(f\"Daily sales completed in {datetime.now() - start_time}\")\n",
    "        logger.info(\"Pipeline completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Pipeline failed: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "    finally:\n",
    "        if engine:\n",
    "            engine.dispose()\n",
    "        if spark:\n",
    "            spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16ffb58",
   "metadata": {},
   "source": [
    "## Execute Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032ecd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"Starting pipeline execution...\")\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        run_pipeline()\n",
    "        logger.info(f\"Pipeline completed in {datetime.now() - start_time}\")\n",
    "        sys.exit(0)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Pipeline failed: {str(e)}\", exc_info=True)\n",
    "        sys.exit(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
