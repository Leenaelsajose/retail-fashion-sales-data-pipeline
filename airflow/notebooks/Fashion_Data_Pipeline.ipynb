{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c89edbe2",
   "metadata": {},
   "source": [
    "Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ffd22c-78ab-42db-bbd1-cee4f9b7483f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kagglehub kaggle pandas psycopg2-binary sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b7e147",
   "metadata": {},
   "source": [
    "Import Kaggle and Authenticate to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebe01f6-0b27-4a7c-8339-4c84690278a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import kaggle\n",
    "print(dir(kagglehub))\n",
    "print(\"Kaggle module is successfully installed!\")\n",
    "kaggle.api.authenticate()\n",
    "print(\"Kaggle API authentication successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bead18a8",
   "metadata": {},
   "source": [
    "Pull Data from Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31780d0c",
   "metadata": {},
   "source": [
    "Import the Retail Fashion Dataset from Kaggle \n",
    "The dataset consists of 6 csv file\n",
    "- transactiions.csv - 773 MB in size and has 6.41 million records\n",
    "- customers.csv - 183 MB in size and has 1.64 million records\n",
    "- products.csv - 4.77 MB in size and has 17940 records\n",
    "- discounts.csv - 18 KB in size and has 3801 records\n",
    "- employees.csv - 15.2 KB in size and has 404 records\n",
    "- stores.csv - 3 KB in size and has 35 records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db10617-1625-4d69-af65-18e7676ad3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Define dataset folder (custom cache location)\n",
    "dataset_path = \"/home/megin_mathew/fashion_dataset\"\n",
    "\n",
    "# Set KaggleHub cache override\n",
    "os.environ[\"KAGGLEHUB_CACHE\"] = dataset_path\n",
    "\n",
    "# File names in the dataset\n",
    "file_names = [\n",
    "    \"transactions.csv\", \"customers.csv\", \"discounts.csv\",\n",
    "    \"employees.csv\", \"products.csv\", \"stores.csv\"\n",
    "]\n",
    "\n",
    "# Ensure cache directory exists\n",
    "os.makedirs(dataset_path, exist_ok=True)\n",
    "\n",
    "# Download each file individually\n",
    "for file_name in file_names:\n",
    "    df = kagglehub.dataset_load(\n",
    "        KaggleDatasetAdapter.PANDAS,\n",
    "        \"ricgomes/global-fashion-retail-stores-dataset\",\n",
    "        file_name\n",
    "    )\n",
    "    \n",
    "    # Save to custom cache location\n",
    "    df.to_csv(os.path.join(dataset_path, file_name), index=False)\n",
    "    print(f\"Downloaded: {file_name} â†’ Stored in: {dataset_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f969f8",
   "metadata": {},
   "source": [
    "Set Kaggle Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be147cd-fa04-4969-9abf-c7d5147a5996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set custom cache location\n",
    "os.environ[\"KAGGLEHUB_CACHE\"] = \"/home/megin_mathew/fashion_dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2d6b94-15e3-4213-a744-4ad1eb5a9de1",
   "metadata": {},
   "source": [
    "Optimized And Cleansed and uses Hadoop along with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3c4c2d",
   "metadata": {},
   "source": [
    "Cleaning /Preprocessing and Normaliztion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100e364d",
   "metadata": {},
   "source": [
    "Cleansing Preprocessing Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d3ff1f-8a99-47c8-b7a2-ae4eac67cc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys \n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Float, Date, DateTime, Boolean, ForeignKey, Text, text\n",
    "from sqlalchemy.orm import declarative_base, sessionmaker, relationship\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, to_date, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType, BooleanType\n",
    "\n",
    "# Configure logging\n",
    "log_directory = '/home/megin_mathew/logs/' # Or another directory where the user has permissions\n",
    "log_file_path = os.path.join(log_directory, 'data_loader.log')\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file_path, encoding='utf-8'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Spark with Hadoop workaround for Windows\n",
    "def create_spark_session():\n",
    "    \"\"\"Create and configure Spark session\"\"\"\n",
    "    try:\n",
    "        # Set Hadoop home directory if on Windows\n",
    "        if os.name == 'nt':\n",
    "            os.environ['HADOOP_HOME'] = 'C:\\\\hadoop'\n",
    "            os.environ['PATH'] = os.environ['PATH'] + ';' + os.environ['HADOOP_HOME'] + '\\\\bin'\n",
    "            \n",
    "            # Set Python path explicitly\n",
    "            python_path = sys.executable\n",
    "            os.environ['PYSPARK_PYTHON'] = python_path\n",
    "            os.environ['PYSPARK_DRIVER_PYTHON'] = python_path\n",
    "        \n",
    "        # Create temp directory if it doesn't exist\n",
    "        temp_dir = \"/home/megin_mathew/spark_temp\"\n",
    "        if not os.path.exists(temp_dir):\n",
    "            os.makedirs(temp_dir)\n",
    "            \n",
    "        return SparkSession.builder \\\n",
    "            .appName(\"GlobalFashionRetailDataLoader\") \\\n",
    "            .config(\"spark.local.dir\", temp_dir) \\\n",
    "            .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "            .config(\"spark.executor.memory\", \"8g\") \\\n",
    "            .config(\"spark.driver.memory\", \"8g\") \\\n",
    "            .config(\"spark.memory.offHeap.enabled\", \"false\") \\\n",
    "            .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "            .config(\"spark.default.parallelism\", \"200\") \\\n",
    "            .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "            .config(\"spark.network.timeout\", \"600s\") \\\n",
    "            .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "            .config(\"spark.python.profile\", \"false\") \\\n",
    "            .config(\"spark.executor.instances\", \"4\") \\\n",
    "            .config(\"spark.executor.cores\", \"2\") \\\n",
    "            .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "            .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "            .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"true\") \\\n",
    "            .getOrCreate()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create Spark session: {str(e)}\")\n",
    "        raise\n",
    "# Initialize Spark\n",
    "spark = create_spark_session()\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(\"/home/megin_mathew/airflow/notebooks/.env\")\n",
    "\n",
    "# Database schema definition\n",
    "Base = declarative_base()\n",
    "SCHEMA_NAME = \"GFRetail\"\n",
    "\n",
    "class Stores(Base):\n",
    "    __tablename__ = 'stores'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    store_id = Column(Integer, primary_key=True)\n",
    "    country = Column(String(100), nullable=True)\n",
    "    city = Column(String(100), nullable=True)\n",
    "    store_name = Column(String(100))\n",
    "    number_of_employees = Column(Integer)\n",
    "    zip_code = Column(String(20))\n",
    "    latitude = Column(Float, nullable=True)\n",
    "    longitude = Column(Float, nullable=True)\n",
    "\n",
    "class Employees(Base):\n",
    "    __tablename__ = 'employees'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    employee_id = Column(Integer, primary_key=True)\n",
    "    store_id = Column(Integer, ForeignKey(f'{SCHEMA_NAME}.stores.store_id'))\n",
    "    name = Column(String(255), nullable=True)\n",
    "    position = Column(String(100), nullable=True)\n",
    "\n",
    "class Customers(Base):\n",
    "    __tablename__ = 'customers'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    customer_id = Column(Integer, primary_key=True)\n",
    "    name = Column(String(255), nullable=True)\n",
    "    email = Column(String(255), nullable=True)\n",
    "    join_date = Column(Date, nullable=True)\n",
    "    telephone = Column(String(50), nullable=True)\n",
    "    city = Column(String(100), nullable=True)\n",
    "    country = Column(String(100), nullable=True)\n",
    "    gender = Column(String(20), nullable=True)\n",
    "    date_of_birth = Column(Date, nullable=True)\n",
    "    job_title = Column(String(100), nullable=True)\n",
    "\n",
    "class Products(Base):\n",
    "    __tablename__ = 'products'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    product_id = Column(Integer, primary_key=True)\n",
    "    category = Column(String(100), nullable=True)\n",
    "    sub_category = Column(String(100))\n",
    "    description_pt = Column(Text)\n",
    "    description_de = Column(Text)\n",
    "    description_fr = Column(Text)\n",
    "    description_es = Column(Text)\n",
    "    description_en = Column(Text)\n",
    "    description_zh = Column(Text)\n",
    "    color = Column(String(50), nullable=True)\n",
    "    sizes = Column(String(100), nullable=True)\n",
    "    production_cost = Column(Float)\n",
    "\n",
    "class Discounts(Base):\n",
    "    __tablename__ = 'discounts'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    start_date = Column(Date)\n",
    "    end_date = Column(Date)\n",
    "    discount_rate = Column(Float)\n",
    "    description = Column(String(255), nullable=True)\n",
    "    category = Column(String(100), nullable=True)\n",
    "    sub_category = Column(String(100))\n",
    "\n",
    "class Transactions(Base):\n",
    "    __tablename__ = 'transactions'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    invoice_id = Column(String(50), nullable=True)\n",
    "    line_number = Column(Integer)\n",
    "    customer_id = Column(Integer, ForeignKey(f'{SCHEMA_NAME}.customers.customer_id'))\n",
    "    product_id = Column(Integer, ForeignKey(f'{SCHEMA_NAME}.products.product_id'))\n",
    "    size = Column(String(10))\n",
    "    color = Column(String(50), nullable=True)\n",
    "    unit_price = Column(Float)\n",
    "    quantity = Column(Integer, nullable=True)\n",
    "    transaction_date = Column(DateTime)\n",
    "    discount = Column(Float, nullable=True)\n",
    "    line_total = Column(Float)\n",
    "    store_id = Column(Integer, ForeignKey(f'{SCHEMA_NAME}.stores.store_id'))\n",
    "    employee_id = Column(Integer, ForeignKey(f'{SCHEMA_NAME}.employees.employee_id'))\n",
    "    currency = Column(String(10), nullable=True)\n",
    "    currency_symbol = Column(String(5))\n",
    "    sku = Column(String(100))\n",
    "    transaction_type = Column(String(20))\n",
    "    payment_method = Column(String(50))\n",
    "    invoice_total = Column(Float)\n",
    "    is_return = Column(Boolean)\n",
    "\n",
    "# Data processing functions\n",
    "def process_stores(spark_df):\n",
    "    spark_df = spark_df.withColumn(\"country\", when(col(\"Country\").isNull(), lit(\"Unknown\")).otherwise(col(\"Country\"))) \\\n",
    "                      .withColumn(\"city\", when(col(\"City\").isNull(), lit(\"Unknown\")).otherwise(col(\"City\"))) \\\n",
    "                      .withColumn(\"latitude\", col(\"Latitude\").cast(FloatType())) \\\n",
    "                      .withColumn(\"longitude\", col(\"Longitude\").cast(FloatType()))\n",
    "    \n",
    "    return spark_df.select(\n",
    "        col(\"Store ID\").alias(\"store_id\"),\n",
    "        col(\"country\"),\n",
    "        col(\"city\"),\n",
    "        col(\"Store Name\").alias(\"store_name\"),\n",
    "        col(\"Number of Employees\").alias(\"number_of_employees\"),\n",
    "        col(\"ZIP Code\").alias(\"zip_code\"),\n",
    "        col(\"latitude\"),\n",
    "        col(\"longitude\")\n",
    "    )\n",
    "\n",
    "def process_employees(spark_df):\n",
    "    spark_df = spark_df.withColumn(\"name\", when(col(\"Name\").isNull(), lit(\"Unknown\")).otherwise(col(\"Name\"))) \\\n",
    "                      .withColumn(\"position\", when(col(\"Position\").isNull(), lit(\"Unknown\")).otherwise(col(\"Position\")))\n",
    "    \n",
    "    return spark_df.select(\n",
    "        col(\"Employee ID\").alias(\"employee_id\"),\n",
    "        col(\"Store ID\").alias(\"store_id\"),\n",
    "        col(\"name\"),\n",
    "        col(\"position\")\n",
    "    )\n",
    "\n",
    "def process_customers(spark_df):\n",
    "    spark_df = spark_df.withColumn(\"join_date\", to_date(col(\"Date Of Birth\"), \"yyyy-MM-dd\")) \\\n",
    "                      .withColumn(\"name\", when(col(\"Name\").isNull(), lit(\"Unknown\")).otherwise(col(\"Name\"))) \\\n",
    "                      .withColumn(\"email\", when(col(\"Email\").isNull(), lit(\"unknown@example.com\")).otherwise(col(\"Email\")))\n",
    "    \n",
    "    return spark_df.select(\n",
    "        col(\"Customer ID\").alias(\"customer_id\"),\n",
    "        col(\"name\"),\n",
    "        col(\"email\"),\n",
    "        col(\"join_date\"),\n",
    "        col(\"Telephone\").alias(\"telephone\"),\n",
    "        col(\"City\").alias(\"city\"),\n",
    "        col(\"Country\").alias(\"country\"),\n",
    "        col(\"Gender\").alias(\"gender\"),\n",
    "        to_date(col(\"Date Of Birth\"), \"yyyy-MM-dd\").alias(\"date_of_birth\"),\n",
    "        col(\"Job Title\").alias(\"job_title\")\n",
    "    )\n",
    "\n",
    "def process_products(spark_df):\n",
    "    spark_df = spark_df.withColumn(\"category\", when(col(\"Category\").isNull(), lit(\"Unknown\")).otherwise(col(\"Category\"))) \\\n",
    "                      .withColumn(\"color\", when(col(\"Color\").isNull(), lit(\"Unknown\")).otherwise(col(\"Color\"))) \\\n",
    "                      .withColumn(\"sizes\", when(col(\"Sizes\").isNull(), lit(\"Unknown\")).otherwise(col(\"Sizes\")))\n",
    "    \n",
    "    return spark_df.select(\n",
    "        col(\"Product ID\").alias(\"product_id\"),\n",
    "        col(\"category\"),\n",
    "        col(\"Sub Category\").alias(\"sub_category\"),\n",
    "        col(\"Description PT\").alias(\"description_pt\"),\n",
    "        col(\"Description DE\").alias(\"description_de\"),\n",
    "        col(\"Description FR\").alias(\"description_fr\"),\n",
    "        col(\"Description ES\").alias(\"description_es\"),\n",
    "        col(\"Description EN\").alias(\"description_en\"),\n",
    "        col(\"Description ZH\").alias(\"description_zh\"),\n",
    "        col(\"color\"),\n",
    "        col(\"sizes\"),\n",
    "        col(\"Production Cost\").alias(\"production_cost\")\n",
    "    )\n",
    "\n",
    "def process_discounts(spark_df):\n",
    "    spark_df = spark_df.withColumn(\"description\", when(col(\"Description\").isNull(), lit(\"No description\")).otherwise(col(\"Description\"))) \\\n",
    "                      .withColumn(\"category\", when(col(\"Category\").isNull(), lit(\"General\")).otherwise(col(\"Category\")))\n",
    "    \n",
    "    return spark_df.select(\n",
    "        to_date(col(\"Start\"), \"yyyy-MM-dd\").alias(\"start_date\"),\n",
    "        to_date(col(\"End\"), \"yyyy-MM-dd\").alias(\"end_date\"),\n",
    "        col(\"Discont\").alias(\"discount_rate\"),\n",
    "        col(\"description\"),\n",
    "        col(\"category\"),\n",
    "        col(\"Sub Category\").alias(\"sub_category\")\n",
    "    )\n",
    "\n",
    "def process_transactions(spark_df):\n",
    "    spark_df = spark_df.withColumn(\"invoice_id\", when(col(\"Invoice ID\").isNull(), lit(\"UNKNOWN\")).otherwise(col(\"Invoice ID\"))) \\\n",
    "                      .withColumn(\"quantity\", when(col(\"Quantity\").isNull(), lit(1)).otherwise(col(\"Quantity\"))) \\\n",
    "                      .withColumn(\"color\", when(col(\"Color\").isNull(), lit(\"Unknown\")).otherwise(col(\"Color\"))) \\\n",
    "                      .withColumn(\"discount\", when(col(\"Discount\").isNull(), lit(0.0)).otherwise(col(\"Discount\"))) \\\n",
    "                      .withColumn(\"currency\", when(col(\"Currency\").isNull(), lit(\"USD\")).otherwise(col(\"Currency\"))) \\\n",
    "                      .withColumn(\"is_return\", col(\"Transaction Type\") == \"Return\")\n",
    "    \n",
    "    return spark_df.select(\n",
    "        col(\"invoice_id\"),\n",
    "        col(\"Line\").alias(\"line_number\"),\n",
    "        col(\"Customer ID\").alias(\"customer_id\"),\n",
    "        col(\"Product ID\").alias(\"product_id\"),\n",
    "        col(\"Size\").alias(\"size\"),\n",
    "        col(\"color\"),\n",
    "        col(\"Unit Price\").alias(\"unit_price\"),\n",
    "        col(\"quantity\"),\n",
    "        col(\"Date\").alias(\"transaction_date\"),\n",
    "        col(\"discount\"),\n",
    "        col(\"Line Total\").alias(\"line_total\"),\n",
    "        col(\"Store ID\").alias(\"store_id\"),\n",
    "        col(\"Employee ID\").alias(\"employee_id\"),\n",
    "        col(\"currency\"),\n",
    "        col(\"Currency Symbol\").alias(\"currency_symbol\"),\n",
    "        col(\"SKU\").alias(\"sku\"),\n",
    "        col(\"Transaction Type\").alias(\"transaction_type\"),\n",
    "        col(\"Payment Method\").alias(\"payment_method\"),\n",
    "        col(\"Invoice Total\").alias(\"invoice_total\"),\n",
    "        col(\"is_return\")\n",
    "    )\n",
    "\n",
    "# In setup_database function, add table creation after schema creation\n",
    "def setup_database(engine):\n",
    "    \"\"\"Create schema and set permissions if needed\"\"\"\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            # Check if schema exists\n",
    "            schema_exists = conn.execute(\n",
    "                text(\"SELECT 1 FROM information_schema.schemata WHERE schema_name = :schema\"),\n",
    "                {'schema': SCHEMA_NAME}\n",
    "            ).scalar()\n",
    "            \n",
    "            if not schema_exists:\n",
    "                logger.info(f\"Creating schema {SCHEMA_NAME}\")\n",
    "                conn.execute(text(f\"CREATE SCHEMA {SCHEMA_NAME}\"))\n",
    "                conn.execute(text(f\"\"\"\n",
    "                    GRANT ALL ON SCHEMA {SCHEMA_NAME} TO {os.getenv('DB_USER', 'postgres')};\n",
    "                    ALTER DEFAULT PRIVILEGES IN SCHEMA {SCHEMA_NAME} \n",
    "                    GRANT ALL ON TABLES TO {os.getenv('DB_USER', 'postgres')};\n",
    "                \"\"\"))\n",
    "            \n",
    "            # Create all tables if they don't exist\n",
    "                Base.metadata.create_all(engine)\n",
    "            \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Database setup failed: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def upsert_data(session, table_class, records, primary_key, batch_size=1000):\n",
    "    \"\"\"Upsert data into the database with batch processing\"\"\"\n",
    "    try:\n",
    "        total_records = len(records)\n",
    "        logger.info(f\"Starting to upsert {total_records} records in batches of {batch_size}\")\n",
    "        \n",
    "        for i in range(0, total_records, batch_size):\n",
    "            batch = records[i:i + batch_size]\n",
    "            batch_num = (i // batch_size) + 1\n",
    "            total_batches = (total_records - 1) // batch_size + 1\n",
    "            \n",
    "            try:\n",
    "                session.bulk_insert_mappings(table_class, batch)\n",
    "                session.commit()\n",
    "                logger.info(f\"Successfully inserted batch {batch_num}/{total_batches}\")\n",
    "            except Exception as e:\n",
    "                session.rollback()\n",
    "                logger.info(f\"Insert failed for batch {batch_num}, switching to upsert mode\")\n",
    "                \n",
    "                for record in batch:\n",
    "                    try:\n",
    "                        existing = session.query(table_class).filter(\n",
    "                            getattr(table_class, primary_key) == record[primary_key]\n",
    "                        ).first()\n",
    "                        \n",
    "                        if existing:\n",
    "                            for key, value in record.items():\n",
    "                                setattr(existing, key, value)\n",
    "                        else:\n",
    "                            new_record = table_class(**record)\n",
    "                            session.add(new_record)\n",
    "                    except Exception as rec_error:\n",
    "                        logger.error(f\"Error processing record {record[primary_key]}: {str(rec_error)}\")\n",
    "                        continue\n",
    "                \n",
    "                try:\n",
    "                    session.commit()\n",
    "                    logger.info(f\"Successfully upserted batch {batch_num}/{total_batches}\")\n",
    "                except Exception as commit_error:\n",
    "                    session.rollback()\n",
    "                    logger.error(f\"Failed to commit batch {batch_num}: {str(commit_error)}\")\n",
    "                    continue\n",
    "                \n",
    "    except Exception as e:\n",
    "        session.rollback()\n",
    "        logger.error(f\"Error in upsert_data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def write_spark_df_to_postgres(spark_df, table_name, engine):\n",
    "    \"\"\"Write Spark DataFrame to PostgreSQL with optimized methods\"\"\"\n",
    "    try:\n",
    "        table_class = globals()[table_name.capitalize()]\n",
    "        primary_key = {\n",
    "            'stores': 'store_id',\n",
    "            'employees': 'employee_id',\n",
    "            'customers': 'customer_id',\n",
    "            'products': 'product_id',\n",
    "            'discounts': 'id',\n",
    "            'transactions': 'id'\n",
    "        }.get(table_name)\n",
    "        \n",
    "        if not primary_key:\n",
    "            logger.error(f\"Primary key not defined for table {table_name}\")\n",
    "            return\n",
    "            \n",
    "        # Skip count verification for large tables to avoid Spark worker crashes\n",
    "        if table_name in ['transactions']:  # Add other large tables if needed\n",
    "            total_rows = \"unknown (skipped count for large table)\"\n",
    "        else:\n",
    "            total_rows = spark_df.count()\n",
    "            \n",
    "        logger.info(f\"Processing {total_rows} records for table {table_name}\")\n",
    "        \n",
    "        # For small tables, use direct conversion\n",
    "        if isinstance(total_rows, int) and total_rows <= 10000:\n",
    "            logger.info(f\"Processing small {table_name} table with direct conversion\")\n",
    "            pandas_df = spark_df.toPandas()\n",
    "            records = pandas_df.to_dict('records')\n",
    "            \n",
    "            Session = sessionmaker(bind=engine)\n",
    "            session = Session()\n",
    "            try:\n",
    "                upsert_data(session, table_class, records, primary_key)\n",
    "                \n",
    "                # Verify count using SQLAlchemy which handles schema names correctly\n",
    "                count = session.query(table_class).count()\n",
    "                logger.info(f\"Verified {count} records in {table_name}\")\n",
    "                \n",
    "            finally:\n",
    "                session.close()\n",
    "            return\n",
    "        \n",
    "        # For larger tables\n",
    "        logger.info(f\"Processing large {table_name} table with optimized chunks\")\n",
    "        pandas_iter = spark_df.toLocalIterator() \n",
    "        records_buffer = []\n",
    "        \n",
    "        for i, row in enumerate(pandas_iter, 1):\n",
    "            records_buffer.append(row.asDict())\n",
    "            \n",
    "            if i % 10000 == 0 or (isinstance(total_rows, int) and i == total_rows):\n",
    "                Session = sessionmaker(bind=engine)\n",
    "                session = Session()\n",
    "                try:\n",
    "                    upsert_data(session, table_class, records_buffer, primary_key)\n",
    "                    logger.info(f\"Processed {i} records\")\n",
    "                    records_buffer = []\n",
    "                finally:\n",
    "                    session.close()\n",
    "        \n",
    "        # Skip final verification for large tables\n",
    "        if table_name not in ['transactions']:  # Add other large tables if needed\n",
    "            with engine.connect() as conn:\n",
    "                # Use quoted identifiers for the schema and table name\n",
    "                count = conn.execute(text(f'SELECT COUNT(*) FROM \"{SCHEMA_NAME}\".\"{table_name}\"')).scalar()\n",
    "                logger.info(f\"Final count for {table_name}: {count} records\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error writing to {table_name}: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "def load_data_to_postgres():\n",
    "    \"\"\"Main function to load data to PostgreSQL\"\"\"\n",
    "    DB_USER = os.getenv('DB_USER')\n",
    "    DB_PASSWORD = os.getenv('DB_PASSWORD')\n",
    "    DB_HOST = os.getenv('DB_HOST')\n",
    "    DB_PORT = os.getenv('DB_PORT')\n",
    "    DB_NAME = os.getenv('DB_NAME')\n",
    "    DATASET_PATH = os.getenv('DATASET_PATH')\n",
    "\n",
    "    required_vars = {\n",
    "    'DB_USER': DB_USER,\n",
    "    'DB_PASSWORD': DB_PASSWORD,\n",
    "    'DB_HOST': DB_HOST,\n",
    "    'DB_NAME': DB_NAME,\n",
    "    'DATASET_PATH': DATASET_PATH\n",
    "    }\n",
    "    \n",
    "    for var_name, var_value in required_vars.items():\n",
    "        if not var_value:\n",
    "            raise ValueError(f\"Missing required environment variable: {var_name}\")\n",
    "    \n",
    "    # Set default for PORT only (since it often doesn't change)\n",
    "    DB_PORT = DB_PORT if DB_PORT else '5432'\n",
    "    try:\n",
    "        engine = create_engine(\n",
    "            f'postgresql+psycopg2://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}',\n",
    "            pool_size=20,\n",
    "            max_overflow=30,\n",
    "            pool_pre_ping=True,\n",
    "            pool_recycle=3600,\n",
    "            connect_args={'connect_timeout': 10}\n",
    "        )\n",
    "        \n",
    "        if not setup_database(engine):\n",
    "            raise RuntimeError(\"Database setup failed\")\n",
    "        \n",
    "        Base.metadata.create_all(engine)\n",
    "        \n",
    "        file_processors = [\n",
    "            ('stores.csv', process_stores, 'stores'),\n",
    "            ('employees.csv', process_employees, 'employees'),\n",
    "            ('products.csv', process_products, 'products'),\n",
    "            ('discounts.csv', process_discounts, 'discounts'),\n",
    "            ('customers.csv', process_customers, 'customers'),\n",
    "            ('transactions.csv', process_transactions, 'transactions')\n",
    "        ]\n",
    "        \n",
    "        for file_name, processor, table_name in file_processors:\n",
    "            file_path = os.path.join(DATASET_PATH, file_name)\n",
    "            if os.path.exists(file_path):\n",
    "                logger.info(f\"Processing {file_name}...\")\n",
    "                start_time = datetime.now()\n",
    "                \n",
    "                try:\n",
    "                    if file_name in ['customers.csv', 'transactions.csv']:\n",
    "                        sample_df = spark.read.csv(file_path, header=True, inferSchema=True, samplingRatio=0.1)\n",
    "                        schema = sample_df.schema\n",
    "                        spark_df = spark.read.csv(file_path, header=True, schema=schema)\n",
    "                    else:\n",
    "                        spark_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "                    \n",
    "                    processed_df = processor(spark_df)\n",
    "                    logger.info(f\"Processed DataFrame schema: {processed_df._jdf.schema().treeString()}\")\n",
    "                    \n",
    "                    write_spark_df_to_postgres(processed_df, table_name, engine)\n",
    "                    \n",
    "                    logger.info(f\"Completed {file_name} in {datetime.now() - start_time}\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing {file_name}: {str(e)}\", exc_info=True)\n",
    "                    \n",
    "                    if file_name == 'products.csv':\n",
    "                        logger.info(\"Attempting alternative approach for products.csv\")\n",
    "                        try:\n",
    "                            products_pd = pd.read_csv(file_path)\n",
    "                            products_spark = spark.createDataFrame(products_pd)\n",
    "                            processed_df = processor(products_spark)\n",
    "                            write_spark_df_to_postgres(processed_df, table_name, engine)\n",
    "                            logger.info(f\"Successfully processed products.csv with alternative approach\")\n",
    "                        except Exception as alt_e:\n",
    "                            logger.error(f\"Alternative approach also failed: {str(alt_e)}\")\n",
    "                            raise\n",
    "                    else:\n",
    "                        raise\n",
    "            else:\n",
    "                logger.warning(f\"File not found: {file_path}\")\n",
    "        \n",
    "        logger.info(\"Data load completed successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Database error: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "    finally:\n",
    "        engine.dispose()\n",
    "        spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        logger.info(\"Starting data load process...\")\n",
    "        start_time = datetime.now()\n",
    "        load_data_to_postgres()\n",
    "        logger.info(f\"Process completed in {datetime.now() - start_time}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fatal error: {str(e)}\", exc_info=True)\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ce89ab",
   "metadata": {},
   "source": [
    "Cleanse and Aggregate for Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf53cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Float, Date, DateTime, Boolean, ForeignKey, Text, text, PrimaryKeyConstraint\n",
    "from sqlalchemy.orm import declarative_base, sessionmaker, relationship\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, to_date, lit, datediff, current_date, dayofweek, sum as spark_sum, avg as spark_avg, count as spark_count, to_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType, BooleanType, TimestampType\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('data_processor.log', encoding='utf-8'), # Changed log file name\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Spark with Hadoop workaround for Windows and JDBC driver config\n",
    "def create_spark_session():\n",
    "    \"\"\"Create and configure Spark session, including JDBC driver via spark.driver.extraClassPath.\"\"\"\n",
    "    try:\n",
    "        # Set Hadoop home directory if on Windows\n",
    "        # This is often needed for Spark to function correctly on Windows, even without HDFS\n",
    "        if os.name == 'nt':\n",
    "            # Ensure these paths are correct for your Windows setup if needed\n",
    "            # Consider adding HADOOP_HOME_WIN to your .env or set it here if consistent\n",
    "            hadoop_home = os.getenv('HADOOP_HOME_WIN', 'C:\\\\hadoop')\n",
    "            os.environ['HADOOP_HOME'] = hadoop_home\n",
    "            # Add Hadoop bin to PATH if it exists\n",
    "            hadoop_bin = os.path.join(hadoop_home, 'bin')\n",
    "            if os.path.exists(hadoop_bin):\n",
    "                 os.environ['PATH'] = os.environ['PATH'] + ';' + hadoop_bin\n",
    "\n",
    "            # Set Python path explicitly for PySpark\n",
    "            python_path = sys.executable\n",
    "            os.environ['PYSPARK_PYTHON'] = python_path\n",
    "            os.environ['PYSPARK_DRIVER_PYTHON'] = python_path\n",
    "\n",
    "        # Create temp directory if it doesn't exist\n",
    "        # Use environment variable with default for flexibility\n",
    "        temp_dir = os.getenv('SPARK_TEMP_DIR', 'D:/spark_temp')\n",
    "        if not os.path.exists(temp_dir):\n",
    "            os.makedirs(temp_dir)\n",
    "            logger.info(f\"Created Spark temporary directory: {temp_dir}\")\n",
    "\n",
    "        spark_builder = SparkSession.builder \\\n",
    "            .appName(\"GlobalFashionRetailDataProcessor\") \\\n",
    "            .config(\"spark.local.dir\", temp_dir) \\\n",
    "            .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "            .config(\"spark.executor.memory\", \"8g\") \\\n",
    "            .config(\"spark.driver.memory\", \"8g\") \\\n",
    "            .config(\"spark.memory.offHeap.enabled\", \"false\") \\\n",
    "            .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "            .config(\"spark.default.parallelism\", \"200\") \\\n",
    "            .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "            .config(\"spark.network.timeout\", \"600s\") \\\n",
    "            .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "            .config(\"spark.python.profile\", \"false\") \\\n",
    "            .config(\"spark.executor.instances\", \"4\") \\\n",
    "            .config(\"spark.executor.cores\", \"2\") \\\n",
    "            .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "            .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "            .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"true\") \\\n",
    "            .config(\"spark.sql.debug.maxToStringFields\", 100) # Increase for wider dataframes in logging\n",
    "\n",
    "        # === Configure JDBC driver via spark.driver.extraClassPath ===\n",
    "        # Manually specify the path to the downloaded JAR file using the POSTGRES_JDBC_DRIVER_PATH env var\n",
    "        # This config is specifically for the driver program (where your Python script runs)\n",
    "        jdbc_jar_path = os.getenv('POSTGRES_JDBC_DRIVER_PATH')\n",
    "\n",
    "        if not jdbc_jar_path:\n",
    "            logger.error(\"POSTGRES_JDBC_DRIVER_PATH environment variable is not set.\")\n",
    "            raise ValueError(\"POSTGRES_JDBC_DRIVER_PATH environment variable must be set to the path of the PostgreSQL JDBC driver JAR.\")\n",
    "\n",
    "        if not os.path.exists(jdbc_jar_path):\n",
    "             logger.error(f\"PostgreSQL JDBC driver not found at specified path: {jdbc_jar_path}\")\n",
    "             raise FileNotFoundError(f\"PostgreSQL JDBC driver not found at {jdbc_jar_path}. Please download the JAR and set POSTGRES_JDBC_DRIVER_PATH.\")\n",
    "\n",
    "        logger.info(f\"Configuring Spark driver with extra classpath: {jdbc_jar_path}\")\n",
    "        spark_builder = spark_builder.config(\"spark.driver.extraClassPath\", jdbc_jar_path)\n",
    "        # =====================================================\n",
    "\n",
    "        # === Removed the spark.jars.packages configuration block ===\n",
    "        # jdbc_package = \"org.postgresql:postgresql:42.6.0\"\n",
    "        # logger.info(f\"Configuring Spark to use JDBC package: {jdbc_package}\")\n",
    "        # spark_builder = spark_builder.config(\"spark.jars.packages\", jdbc_package)\n",
    "        # =====================================================\n",
    "\n",
    "\n",
    "        logger.info(\"Attempting to get or create Spark session...\")\n",
    "        spark_session = spark_builder.getOrCreate()\n",
    "        logger.info(\"Spark session created successfully.\")\n",
    "        return spark_session\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create Spark session: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "# Initialize Spark\n",
    "spark = create_spark_session()\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Database schema definition\n",
    "Base = declarative_base()\n",
    "SCHEMA_NAME = \"GFRetail\" # Ensure this matches your database schema name\n",
    "\n",
    "\n",
    "class Stores(Base):\n",
    "    __tablename__ = 'stores'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    store_id = Column(Integer, primary_key=True)\n",
    "    country = Column(String(100), nullable=True)\n",
    "    city = Column(String(100), nullable=True)\n",
    "    store_name = Column(String(100))\n",
    "    number_of_employees = Column(Integer)\n",
    "    zip_code = Column(String(20))\n",
    "    latitude = Column(Float, nullable=True)\n",
    "    longitude = Column(Float, nullable=True)\n",
    "\n",
    "class Employees(Base):\n",
    "    __tablename__ = 'employees'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    employee_id = Column(Integer, primary_key=True)\n",
    "    store_id = Column(Integer, ForeignKey(f'{SCHEMA_NAME}.stores.store_id'))\n",
    "    name = Column(String(255), nullable=True)\n",
    "    position = Column(String(100), nullable=True)\n",
    "\n",
    "class Customers(Base):\n",
    "    __tablename__ = 'customers'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    customer_id = Column(Integer, primary_key=True)\n",
    "    name = Column(String(255), nullable=True)\n",
    "    email = Column(String(255), nullable=True)\n",
    "    join_date = Column(Date, nullable=True)\n",
    "    telephone = Column(String(50), nullable=True)\n",
    "    city = Column(String(100), nullable=True)\n",
    "    country = Column(String(100), nullable=True)\n",
    "    gender = Column(String(20), nullable=True)\n",
    "    date_of_birth = Column(Date, nullable=True)\n",
    "    job_title = Column(String(100), nullable=True)\n",
    "    # New Feature Column - will be added to DB if running create_all and it's missing\n",
    "    days_since_birth = Column(Integer, nullable=True)\n",
    "\n",
    "class Products(Base):\n",
    "    __tablename__ = 'products'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    product_id = Column(Integer, primary_key=True)\n",
    "    category = Column(String(100), nullable=True)\n",
    "    sub_category = Column(String(100))\n",
    "    description_pt = Column(Text)\n",
    "    description_de = Column(Text)\n",
    "    description_fr = Column(Text)\n",
    "    description_es = Column(Text)\n",
    "    description_en = Column(Text)\n",
    "    description_zh = Column(Text)\n",
    "    color = Column(String(50), nullable=True)\n",
    "    sizes = Column(String(100), nullable=True)\n",
    "    production_cost = Column(Float)\n",
    "\n",
    "class Discounts(Base):\n",
    "    __tablename__ = 'discounts'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True) # Assuming this is the PK in DB\n",
    "    start_date = Column(Date)\n",
    "    end_date = Column(Date)\n",
    "    discount_rate = Column(Float)\n",
    "    description = Column(String(255), nullable=True)\n",
    "    category = Column(String(100), nullable=True)\n",
    "    sub_category = Column(String(100))\n",
    "\n",
    "class Transactions(Base):\n",
    "    __tablename__ = 'transactions'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    # Assuming 'id' is the primary key already in the DB\n",
    "    id = Column(Integer, primary_key=True) # Assuming this is pre-existing PK\n",
    "    invoice_id = Column(String(50), nullable=True)\n",
    "    line_number = Column(Integer)\n",
    "    customer_id = Column(Integer, ForeignKey(f'{SCHEMA_NAME}.customers.customer_id'))\n",
    "    product_id = Column(Integer, ForeignKey(f'{SCHEMA_NAME}.products.product_id'))\n",
    "    size = Column(String(10))\n",
    "    color = Column(String(50), nullable=True)\n",
    "    unit_price = Column(Float)\n",
    "    quantity = Column(Integer, nullable=True)\n",
    "    transaction_date = Column(DateTime) # Or Timestamp if that's the DB type\n",
    "    discount = Column(Float, nullable=True)\n",
    "    line_total = Column(Float)\n",
    "    store_id = Column(Integer, ForeignKey(f'{SCHEMA_NAME}.stores.store_id'))\n",
    "    employee_id = Column(Integer, ForeignKey(f'{SCHEMA_NAME}.employees.employee_id'))\n",
    "    currency = Column(String(10), nullable=True)\n",
    "    currency_symbol = Column(String(5))\n",
    "    sku = Column(String(100))\n",
    "    transaction_type = Column(String(20))\n",
    "    payment_method = Column(String(50))\n",
    "    invoice_total = Column(Float)\n",
    "    is_return = Column(Boolean)\n",
    "    # New Feature Columns - will be added to DB if running create_all and they are missing\n",
    "    net_line_total = Column(Float, nullable=True)\n",
    "    transaction_day_of_week = Column(Integer, nullable=True)\n",
    "\n",
    "class DailyStoreSales(Base):\n",
    "    __tablename__ = 'daily_store_sales'\n",
    "    __table_args__ = (\n",
    "        PrimaryKeyConstraint('store_id', 'sale_date'), # Using composite primary key\n",
    "        {'schema': SCHEMA_NAME}\n",
    "    )\n",
    "    store_id = Column(Integer, ForeignKey(f'{SCHEMA_NAME}.stores.store_id'), nullable=False)\n",
    "    sale_date = Column(Date, nullable=False)\n",
    "    transaction_count = Column(Integer, nullable=True)\n",
    "    total_sales = Column(Float, nullable=True)\n",
    "    average_unit_price = Column(Float, nullable=True)\n",
    "    total_quantity_sold = Column(Integer, nullable=True)\n",
    "    total_discount_given = Column(Float, nullable=True)\n",
    "    total_net_sales = Column(Float, nullable=True)\n",
    "\n",
    "\n",
    "# --- Data Processing Functions (Feature Engineering & Aggregation) ---\n",
    "# These functions now assume input DataFrames have column names matching SQLAlchemy models (snake_case)\n",
    "\n",
    "def add_customer_features(customers_df):\n",
    "    \"\"\"Adds feature engineered columns to the customers DataFrame.\"\"\"\n",
    "    logger.info(\"Adding customer features (days_since_birth)...\")\n",
    "    # Assumes 'date_of_birth' column exists and is DateType/TimestampType in DB\n",
    "    processed_df = customers_df.withColumn(\n",
    "        \"days_since_birth\",\n",
    "        when(col(\"date_of_birth\").isNotNull(), datediff(current_date(), col(\"date_of_birth\")))\n",
    "        .otherwise(lit(None)) # Handle cases where date_of_birth is null\n",
    "    )\n",
    "    logger.info(\"Customer feature engineering complete.\")\n",
    "    return processed_df\n",
    "\n",
    "def add_transaction_features(transactions_df):\n",
    "    \"\"\"Adds feature engineered columns to the transactions DataFrame.\"\"\"\n",
    "    logger.info(\"Adding transaction features (net_line_total, day_of_week)...\")\n",
    "    # Ensure required columns are cast to appropriate types before calculation/extraction\n",
    "    # Assuming columns are already in snake_case and appropriate types from DB read,\n",
    "    # but explicit casting here adds robustness if DB schema is slightly different.\n",
    "    transactions_df = transactions_df.withColumn(\"unit_price\", col(\"unit_price\").cast(FloatType()))\\\n",
    "                                     .withColumn(\"quantity\", col(\"quantity\").cast(IntegerType()))\\\n",
    "                                     .withColumn(\"discount\", col(\"discount\").cast(FloatType()))\\\n",
    "                                     .withColumn(\"transaction_date\", col(\"transaction_date\").cast(TimestampType())) # Ensure datetime type\n",
    "\n",
    "\n",
    "    processed_df = transactions_df.withColumn(\n",
    "        \"net_line_total\",\n",
    "        (col(\"unit_price\") * col(\"quantity\")) - col(\"discount\") # Calculation on cleaned columns\n",
    "    ).withColumn(\n",
    "        \"transaction_day_of_week\",\n",
    "         when(col(\"transaction_date\").isNotNull(), dayofweek(col(\"transaction_date\")))\n",
    "        .otherwise(lit(None))\n",
    "    )\n",
    "    logger.info(\"Transaction feature engineering complete.\")\n",
    "    return processed_df\n",
    "\n",
    "\n",
    "def aggregate_daily_store_sales(transactions_df):\n",
    "    \"\"\"Aggregates transaction data (assuming snake_case columns) to get daily sales summary per store.\"\"\"\n",
    "    logger.info(\"Starting daily store sales aggregation...\")\n",
    "\n",
    "    # Ensure transaction_date is a proper timestamp/datetime before casting to Date\n",
    "    # Assuming the input transactions_df already has 'transaction_date' as TimestampType\n",
    "    transactions_df = transactions_df.withColumn(\"sale_date\", to_date(col(\"transaction_date\")))\n",
    "\n",
    "    # Group by store_id and sale_date and perform aggregations\n",
    "    daily_sales_df = transactions_df.groupBy(\"store_id\", \"sale_date\").agg(\n",
    "        spark_count(\"*\").alias(\"transaction_count\"),\n",
    "        spark_sum(\"line_total\").alias(\"total_sales\"), # Using line_total as total sales per line\n",
    "        spark_avg(\"unit_price\").alias(\"average_unit_price\"),\n",
    "        spark_sum(\"quantity\").alias(\"total_quantity_sold\"),\n",
    "        spark_sum(\"discount\").alias(\"total_discount_given\"),\n",
    "        spark_sum(\"net_line_total\").alias(\"total_net_sales\") # Aggregate the new net_line_total\n",
    "    )\n",
    "\n",
    "    logger.info(\"Daily store sales aggregation complete.\")\n",
    "    return daily_sales_df.select(\n",
    "        \"store_id\",\n",
    "        \"sale_date\",\n",
    "        \"transaction_count\",\n",
    "        \"total_sales\",\n",
    "        \"average_unit_price\",\n",
    "        \"total_quantity_sold\",\n",
    "        \"total_discount_given\",\n",
    "        \"total_net_sales\"\n",
    "    )\n",
    "\n",
    "# --- Database Interaction Functions ---\n",
    "\n",
    "def setup_database(engine):\n",
    "    \"\"\"Create schema and set permissions if needed, then create tables (if they don't exist).\"\"\"\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            # Check if schema exists\n",
    "            schema_exists = conn.execute(\n",
    "                text(\"SELECT 1 FROM information_schema.schemata WHERE schema_name = :schema\"),\n",
    "                {'schema': SCHEMA_NAME}\n",
    "            ).scalar()\n",
    "\n",
    "            if not schema_exists:\n",
    "                logger.info(f\"Creating schema {SCHEMA_NAME}\")\n",
    "                conn.execute(text(f\"CREATE SCHEMA {SCHEMA_NAME}\"))\n",
    "                # Grant permissions - adjust user/role as needed\n",
    "                grant_user = os.getenv('DB_USER', 'postgres')\n",
    "                conn.execute(text(f\"\"\"\n",
    "                    GRANT ALL ON SCHEMA {SCHEMA_NAME} TO \"{grant_user}\";\n",
    "                    ALTER DEFAULT PRIVILEGES IN SCHEMA {SCHEMA_NAME}\n",
    "                    GRANT ALL ON TABLES TO \"{grant_user}\";\n",
    "                \"\"\"))\n",
    "                logger.info(f\"Schema {SCHEMA_NAME} created and permissions granted to {grant_user}.\")\n",
    "\n",
    "            # Create all tables if they don't exist (this is safe for existing tables)\n",
    "            # This will add new columns (days_since_birth, net_line_total, transaction_day_of_week)\n",
    "            # to existing tables if they are missing, and create the daily_store_sales table.\n",
    "            logger.info(\"Creating/Ensuring existence of database tables based on models...\")\n",
    "            Base.metadata.create_all(engine)\n",
    "            logger.info(\"Table creation/check complete.\")\n",
    "\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Database setup failed: {str(e)}\", exc_info=True)\n",
    "        return False\n",
    "\n",
    "def load_data_from_postgres_to_spark(spark, db_host, db_port, db_name, db_user, db_password, schema_name, table_name):\n",
    "    \"\"\"Reads a table from PostgreSQL into a Spark DataFrame using JDBC with optional partitioning.\"\"\"\n",
    "    logger.info(f\"Reading data from database table: {schema_name}.{table_name}\")\n",
    "    try:\n",
    "        jdbc_url = f'jdbc:postgresql://{db_host}:{db_port}/{db_name}'\n",
    "        db_properties = {\n",
    "            \"user\": db_user,\n",
    "            \"password\": db_password,\n",
    "            \"driver\": \"org.postgresql.Driver\",\n",
    "            \"fetchsize\": \"10000\" # Helps with large reads\n",
    "        }\n",
    "\n",
    "        # Construct the full table name including schema (quoted for case sensitivity/special chars)\n",
    "        full_table_name = f'\"{schema_name}\".\"{table_name}\"'\n",
    "\n",
    "        # === Add Partitioning Logic Here for Performance ===\n",
    "        # This is crucial for large tables like 'transactions'.\n",
    "        # You need to determine appropriate partitionColumn, lowerBound, upperBound, and numPartitions\n",
    "        partition_options = {}\n",
    "        if table_name == 'transactions':\n",
    "            logger.info(f\"Applying partitioning for {table_name}\")\n",
    "            logger.warning(f\"Partitioning options are NOT set for {table_name}. This will be slow for large tables. Consider adding partitionColumn, lowerBound, upperBound, and numPartitions.\")\n",
    "\n",
    "        spark_df = spark.read.jdbc(\n",
    "            url=jdbc_url,\n",
    "            table=full_table_name,\n",
    "            properties=db_properties,\n",
    "            **partition_options # Pass the partition options\n",
    "        )\n",
    "        logger.info(f\"Successfully read data from {schema_name}.{table_name}. Schema: {spark_df.printSchema()}\")\n",
    "        # logger.info(f\"Number of partitions after reading: {spark_df.rdd.getNumPartitions()}\") # Check partition count\n",
    "        return spark_df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to read data from {schema_name}.{table_name} into Spark: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "def upsert_data(session, table_class, records, primary_keys, batch_size=1000):\n",
    "    \"\"\"Upsert data into the database with batch processing using SQLAlchemy merge.\"\"\"\n",
    "    try:\n",
    "        total_records = len(records)\n",
    "        if total_records == 0:\n",
    "            logger.info(f\"No records to upsert for {table_class.__tablename__}.\")\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Starting to upsert {total_records} records for {table_class.__tablename__} in batches of {batch_size}\")\n",
    "\n",
    "        for i in range(0, total_records, batch_size):\n",
    "            batch = records[i:i + batch_size]\n",
    "            batch_num = (i // batch_size) + 1\n",
    "            total_batches = (total_records - 1) // batch_size + 1\n",
    "\n",
    "            try:\n",
    "                # Use merge for upsert logic, which handles existing primary keys (simple or composite)\n",
    "                for record in batch:\n",
    "                    # Convert Spark Row object to dictionary if needed, or ensure records are dicts\n",
    "                    record_dict = record if isinstance(record, dict) else record.asDict()\n",
    "                    instance = table_class(**record_dict)\n",
    "                    session.merge(instance) # merge handles both insert and update based on PK\n",
    "\n",
    "                session.commit()\n",
    "                logger.info(f\"Successfully processed batch {batch_num}/{total_batches} for {table_class.__tablename__}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                session.rollback()\n",
    "                logger.error(f\"Error processing batch {batch_num} for {table_class.__tablename__}: {str(e)}\", exc_info=True)\n",
    "                # Log affected primary key(s) if possible\n",
    "                try:\n",
    "                    pk_values = [\n",
    "                        {pk: record_dict.get(pk) for pk in primary_keys}\n",
    "                        for record_dict in (record if isinstance(record, dict) else record.asDict() for record in batch)\n",
    "                    ]\n",
    "                    logger.error(f\"Batch primary key values: {pk_values}\")\n",
    "                except Exception as log_e:\n",
    "                    logger.error(f\"Could not log batch primary keys: {log_e}\")\n",
    "\n",
    "                # Decide whether to re-raise or continue. Re-raising stops on first error.\n",
    "                raise # Stop on error\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        session.rollback()\n",
    "        logger.error(f\"Critical error in upsert_data for {table_class.__tablename__}: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "def write_spark_df_to_postgres(spark_df, table_name, engine):\n",
    "    \"\"\"Write Spark DataFrame to PostgreSQL using SQLAlchemy upsert (merge) logic.\"\"\"\n",
    "    try:\n",
    "        # Map table name to SQLAlchemy class name\n",
    "        # Handles 'daily_store_sales' -> 'DailyStoreSales'\n",
    "        table_class_name = ''.join(word.capitalize() for word in table_name.split('_'))\n",
    "        table_class = globals().get(table_class_name)\n",
    "\n",
    "        if not table_class:\n",
    "             logger.error(f\"SQLAlchemy class not found for table name: {table_name}\")\n",
    "             return\n",
    "\n",
    "        # Define primary keys for each table\n",
    "        primary_keys_map = {\n",
    "            'stores': ['store_id'],\n",
    "            'employees': ['employee_id'],\n",
    "            'customers': ['customer_id'],\n",
    "            'products': ['product_id'],\n",
    "            'discounts': ['id'],\n",
    "            'transactions': ['id'], # Assuming 'id' is the PK\n",
    "            'daily_store_sales': ['store_id', 'sale_date'] # Composite key\n",
    "        }\n",
    "        primary_keys = primary_keys_map.get(table_name)\n",
    "\n",
    "        if not primary_keys:\n",
    "            logger.error(f\"Primary key(s) not defined for table {table_name}\")\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Starting write process for table {table_name}...\")\n",
    "\n",
    "        # Convert Spark DataFrame to Pandas DataFrames iteratively\n",
    "        # Using toLocalIterator for larger dataframes to avoid driver memory issues\n",
    "        pandas_iter = spark_df.toLocalIterator()\n",
    "        records_buffer = []\n",
    "        batch_size = 5000 # Batch size for converting Spark rows to dicts and upserting\n",
    "\n",
    "        Session = sessionmaker(bind=engine)\n",
    "        session = Session()\n",
    "        try:\n",
    "            i = 0\n",
    "            for row in pandas_iter:\n",
    "                records_buffer.append(row.asDict()) # Convert Spark Row to dictionary\n",
    "                i += 1\n",
    "\n",
    "                if i % batch_size == 0:\n",
    "                    logger.info(f\"Buffer size reached {batch_size}, processing batch {i // batch_size}\")\n",
    "                    upsert_data(session, table_class, records_buffer, primary_keys)\n",
    "                    records_buffer = [] # Clear buffer\n",
    "                    # Commit is done inside upsert_data for each batch\n",
    "\n",
    "            # Process any remaining records in the buffer\n",
    "            if records_buffer:\n",
    "                logger.info(f\"Processing final batch of {len(records_buffer)} records\")\n",
    "                upsert_data(session, table_class, records_buffer, primary_keys)\n",
    "\n",
    "            # Verify count only for smaller tables or if specifically needed and feasible\n",
    "            # Note: Counting can be slow on large tables, especially after upserts\n",
    "            if table_name in ['stores', 'employees', 'discounts', 'daily_store_sales']: # Only count the tables we loaded/updated\n",
    "                 try:\n",
    "                    with engine.connect() as conn:\n",
    "                        # Use quoted identifiers for the schema and table name\n",
    "                        count = conn.execute(text(f'SELECT COUNT(*) FROM \"{SCHEMA_NAME}\".\"{table_name}\"')).scalar()\n",
    "                        logger.info(f\"Final count for {table_name}: {count} records\")\n",
    "                 except Exception as count_e:\n",
    "                    logger.warning(f\"Could not verify count for {table_name}: {count_e}\")\n",
    "            else:\n",
    "                 logger.info(f\"Skipped final count verification for table {table_name} (read from DB).\")\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "             logger.error(f\"Error during data processing and upsert for {table_name}: {str(e)}\", exc_info=True)\n",
    "             session.rollback() # Rollback any outstanding transaction if an error occurred before a commit\n",
    "             raise\n",
    "        finally:\n",
    "            session.close()\n",
    "            logger.info(f\"Session closed for table {table_name}.\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fatal error in write_spark_df_to_postgres for {table_name}: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "\n",
    "def process_and_aggregate_from_db():\n",
    "    \"\"\"Main function to read data from DB, process, aggregate, and load aggregated data.\"\"\"\n",
    "    # Load environment variables\n",
    "    DB_USER = os.getenv('DB_USER')\n",
    "    DB_PASSWORD = os.getenv('DB_PASSWORD')\n",
    "    DB_HOST = os.getenv('DB_HOST')\n",
    "    DB_PORT = os.getenv('DB_PORT')\n",
    "    DB_NAME = os.getenv('DB_NAME')\n",
    "\n",
    "    required_vars = {\n",
    "        'DB_USER': DB_USER,\n",
    "        'DB_PASSWORD': DB_PASSWORD,\n",
    "        'DB_HOST': DB_HOST,\n",
    "        'DB_NAME': DB_NAME,\n",
    "    }\n",
    "\n",
    "    for var_name, var_value in required_vars.items():\n",
    "        if not var_value:\n",
    "            raise ValueError(f\"Missing required environment variable: {var_name}\")\n",
    "\n",
    "    DB_PORT = DB_PORT if DB_PORT else '5432' # Set default for PORT\n",
    "\n",
    "    engine = None # Initialize engine outside try for finally block\n",
    "\n",
    "    try:\n",
    "        # Create SQLAlchemy engine for schema setup and writing (only for aggregated data)\n",
    "        engine = create_engine(\n",
    "            f'postgresql+psycopg2://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}',\n",
    "            pool_size=15, # Adjust pool size\n",
    "            max_overflow=25, # Adjust max overflow\n",
    "            pool_pre_ping=True,\n",
    "            pool_recycle=3600,\n",
    "            connect_args={'connect_timeout': 10}\n",
    "        )\n",
    "\n",
    "        # Setup Database: Create schema and tables if they don't exist\n",
    "        # This will ensure the daily_store_sales table exists and add FE columns if missing\n",
    "        if not setup_database(engine):\n",
    "            raise RuntimeError(\"Database setup failed\")\n",
    "\n",
    "        # --- Read Data from Database into Spark ---\n",
    "        logger.info(\"Starting to read data from database tables...\")\n",
    "        start_time_read_db = datetime.now()\n",
    "        try:\n",
    "            # Read all necessary tables from the database\n",
    "            customers_df = load_data_from_postgres_to_spark(spark, DB_HOST, DB_PORT, DB_NAME, DB_USER, DB_PASSWORD, SCHEMA_NAME, 'customers')\n",
    "            products_df = load_data_from_postgres_to_spark(spark, DB_HOST, DB_PORT, DB_NAME, DB_USER, DB_PASSWORD, SCHEMA_NAME, 'products')\n",
    "            transactions_df = load_data_from_postgres_to_spark(spark, DB_HOST, DB_PORT, DB_NAME, DB_USER, DB_PASSWORD, SCHEMA_NAME, 'transactions')\n",
    "            stores_df = load_data_from_postgres_to_spark(spark, DB_HOST, DB_PORT, DB_NAME, DB_USER, DB_PASSWORD, SCHEMA_NAME, 'stores')\n",
    "            employees_df = load_data_from_postgres_to_spark(spark, DB_HOST, DB_PORT, DB_NAME, DB_USER, DB_PASSWORD, SCHEMA_NAME, 'employees')\n",
    "            discounts_df = load_data_from_postgres_to_spark(spark, DB_HOST, DB_PORT, DB_NAME, DB_USER, DB_PASSWORD, SCHEMA_NAME, 'discounts')\n",
    "\n",
    "            logger.info(f\"Completed reading data from database in {datetime.now() - start_time_read_db}\")\n",
    "\n",
    "        except Exception as e:\n",
    "             logger.error(f\"Failed to read data from DB. Cannot proceed with processing. Error: {str(e)}\", exc_info=True)\n",
    "             # Reading core data from DB is fatal if it fails\n",
    "             raise\n",
    "\n",
    "        # --- Perform Feature Engineering on DB-read DataFrames ---\n",
    "        logger.info(\"Starting feature engineering on DB-read dataframes...\")\n",
    "        start_time_fe = datetime.now()\n",
    "        try:\n",
    "            # Apply feature engineering functions to the relevant DataFrames\n",
    "            # These functions now expect snake_case columns from the DB read\n",
    "            processed_customer_df = add_customer_features(customers_df)\n",
    "            processed_transaction_df = add_transaction_features(transactions_df)\n",
    "            # No specific FE planned for other tables in this iteration\n",
    "\n",
    "            logger.info(f\"Feature engineering complete in {datetime.now() - start_time_fe}\")\n",
    "\n",
    "        except Exception as e:\n",
    "             logger.error(f\"Error during feature engineering: {str(e)}\", exc_info=True)\n",
    "             # Feature engineering failure is likely fatal as aggregation depends on it\n",
    "             raise\n",
    "\n",
    "        # --- Perform Aggregation and Load New Tables ---\n",
    "        if processed_transaction_df is not None: # Check if transactions data was processed\n",
    "            logger.info(\"Starting aggregation for daily store sales...\")\n",
    "            start_time_agg = datetime.now()\n",
    "            try:\n",
    "                # Aggregate using the feature-engineered transactions DataFrame\n",
    "                daily_sales_df = aggregate_daily_store_sales(processed_transaction_df)\n",
    "                # Load the aggregated data into the new daily_store_sales table\n",
    "                write_spark_df_to_postgres(daily_sales_df, 'daily_store_sales', engine)\n",
    "                logger.info(f\"Completed daily store sales aggregation and loading in {datetime.now() - start_time_agg}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error during daily store sales aggregation or loading: {str(e)}\", exc_info=True)\n",
    "                # Decide whether to stop or continue on aggregation error\n",
    "                # For now, let's treat aggregation failure as fatal\n",
    "                raise\n",
    "\n",
    "        else:\n",
    "            logger.warning(\"No processed transaction data available for aggregation.\")\n",
    "\n",
    "\n",
    "        logger.info(\"Overall data processing and aggregation completed.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fatal error during overall process: {str(e)}\", exc_info=True)\n",
    "        # Re-raise the exception so the main block catches it for process exit code\n",
    "        raise\n",
    "\n",
    "    finally:\n",
    "        # Ensure database engine and Spark session are closed/stopped\n",
    "        if engine:\n",
    "            engine.dispose()\n",
    "            logger.info(\"Database engine disposed.\")\n",
    "        if spark:\n",
    "            spark.stop()\n",
    "            logger.info(\"Spark session stopped.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # This block runs when the script is executed directly\n",
    "    try:\n",
    "        logger.info(\"Starting data processing and aggregation script (reading from DB)...\")\n",
    "        overall_start_time = datetime.now()\n",
    "        process_and_aggregate_from_db() # Call the new main function\n",
    "        overall_end_time = datetime.now()\n",
    "        logger.info(f\"Script finished. Total elapsed time: {overall_end_time - overall_start_time}\")\n",
    "        sys.exit(0) # Exit successfully\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Script terminated due to fatal error: {str(e)}\", exc_info=True)\n",
    "        sys.exit(1) # Exit with a non-zero status code to indicate failure\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
