{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c89edbe2",
   "metadata": {},
   "source": [
    "Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ffd22c-78ab-42db-bbd1-cee4f9b7483f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kagglehub kaggle pandas psycopg2-binary sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b7e147",
   "metadata": {},
   "source": [
    "Import Kaggle and Authenticate to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebe01f6-0b27-4a7c-8339-4c84690278a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import kaggle\n",
    "print(dir(kagglehub))\n",
    "print(\"Kaggle module is successfully installed!\")\n",
    "kaggle.api.authenticate()\n",
    "print(\"Kaggle API authentication successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bead18a8",
   "metadata": {},
   "source": [
    "Pull Data from Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31780d0c",
   "metadata": {},
   "source": [
    "Import the Retail Fashion Dataset from Kaggle \n",
    "The dataset consists of 6 csv file\n",
    "- transactiions.csv - 773 MB in size and has 6.41 million records\n",
    "- customers.csv - 183 MB in size and has 1.64 million records\n",
    "- products.csv - 4.77 MB in size and has 17940 records\n",
    "- discounts.csv - 18 KB in size and has 3801 records\n",
    "- employees.csv - 15.2 KB in size and has 404 records\n",
    "- stores.csv - 3 KB in size and has 35 records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db10617-1625-4d69-af65-18e7676ad3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Define dataset folder (custom cache location)\n",
    "dataset_path = \"/home/megin_mathew/fashion_dataset\"\n",
    "\n",
    "# Set KaggleHub cache override\n",
    "os.environ[\"KAGGLEHUB_CACHE\"] = dataset_path\n",
    "\n",
    "# File names in the dataset\n",
    "file_names = [\n",
    "    \"transactions.csv\", \"customers.csv\", \"discounts.csv\",\n",
    "    \"employees.csv\", \"products.csv\", \"stores.csv\"\n",
    "]\n",
    "\n",
    "# Ensure cache directory exists\n",
    "os.makedirs(dataset_path, exist_ok=True)\n",
    "\n",
    "# Download each file individually\n",
    "for file_name in file_names:\n",
    "    df = kagglehub.dataset_load(\n",
    "        KaggleDatasetAdapter.PANDAS,\n",
    "        \"ricgomes/global-fashion-retail-stores-dataset\",\n",
    "        file_name\n",
    "    )\n",
    "    \n",
    "    # Save to custom cache location\n",
    "    df.to_csv(os.path.join(dataset_path, file_name), index=False)\n",
    "    print(f\"Downloaded: {file_name} â†’ Stored in: {dataset_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f969f8",
   "metadata": {},
   "source": [
    "Set Kaggle Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be147cd-fa04-4969-9abf-c7d5147a5996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set custom cache location\n",
    "os.environ[\"KAGGLEHUB_CACHE\"] = \"/home/megin_mathew/fashion_dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2d6b94-15e3-4213-a744-4ad1eb5a9de1",
   "metadata": {},
   "source": [
    "Optimized And Cleansed and uses Hadoop along with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3c4c2d",
   "metadata": {},
   "source": [
    "Cleaning /Preprocessing and Normaliztion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100e364d",
   "metadata": {},
   "source": [
    "Cleansing Preprocessing Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d3ff1f-8a99-47c8-b7a2-ae4eac67cc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys\n",
    "from typing import Optional, List, Dict, Any, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Float, Date, DateTime, Boolean, ForeignKey, Text, text\n",
    "from sqlalchemy.orm import declarative_base, sessionmaker\n",
    "from sqlalchemy.engine import Engine\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, when, to_date, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType, BooleanType\n",
    "\n",
    "# --- Configuration and Setup ---\n",
    "\n",
    "# Configure logging\n",
    "LOG_DIRECTORY = '/home/megin_mathew/logs/'\n",
    "LOG_FILE_PATH = os.path.join(LOG_DIRECTORY, 'data_loader.log')\n",
    "\n",
    "# Ensure log directory exists\n",
    "os.makedirs(LOG_DIRECTORY, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(LOG_FILE_PATH, encoding='utf-8'),\n",
    "        logging.StreamHandler(sys.stdout)  # Use sys.stdout for stream handler\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables\n",
    "DOTENV_PATH = \"/home/megin_mathew/airflow/notebooks/.env\"\n",
    "load_dotenv(DOTENV_PATH)\n",
    "\n",
    "\n",
    "def create_spark_session() -> Optional[SparkSession]:\n",
    "    \"\"\"Create and configure Spark session.\"\"\"\n",
    "    try:\n",
    "        # Set Hadoop home directory if on Windows\n",
    "        if os.name == 'nt':\n",
    "            hadoop_home = 'C:\\\\hadoop'\n",
    "            if not os.path.exists(hadoop_home):\n",
    "                 logger.warning(f\"HADOOP_HOME directory not found: {hadoop_home}. Spark might fail.\")\n",
    "            os.environ['HADOOP_HOME'] = hadoop_home\n",
    "            os.environ['PATH'] = os.environ.get('PATH', '') + ';' + os.environ['HADOOP_HOME'] + '\\\\bin'\n",
    "\n",
    "            # Set Python path explicitly\n",
    "            python_path = sys.executable\n",
    "            os.environ['PYSPARK_PYTHON'] = python_path\n",
    "            os.environ['PYSPARK_DRIVER_PYTHON'] = python_path\n",
    "\n",
    "        # Create temp directory if it doesn't exist\n",
    "        temp_dir = \"/home/megin_mathew/spark_temp\"\n",
    "        os.makedirs(temp_dir, exist_ok=True) # Use exist_ok=True\n",
    "\n",
    "        spark_session = SparkSession.builder \\\n",
    "            .appName(\"GlobalFashionRetailDataLoader\") \\\n",
    "            .config(\"spark.local.dir\", temp_dir) \\\n",
    "            .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "            .config(\"spark.executor.memory\", \"8g\") \\\n",
    "            .config(\"spark.driver.memory\", \"8g\") \\\n",
    "            .config(\"spark.memory.offHeap.enabled\", \"false\") \\\n",
    "            .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "            .config(\"spark.default.parallelism\", \"200\") \\\n",
    "            .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "            .config(\"spark.network.timeout\", \"600s\") \\\n",
    "            .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "            .config(\"spark.python.profile\", \"false\") \\\n",
    "            .config(\"spark.executor.instances\", \"4\") \\\n",
    "            .config(\"spark.executor.cores\", \"2\") \\\n",
    "            .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "            .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "            .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"true\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "        logger.info(\"Spark session created successfully.\")\n",
    "        return spark_session\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create Spark session: {str(e)}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "# --- Database Schema Definition ---\n",
    "\n",
    "Base = declarative_base()\n",
    "SCHEMA_NAME = \"GFRetail\"\n",
    "\n",
    "class Stores(Base):\n",
    "    __tablename__ = 'stores'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    store_id = Column(Integer, primary_key=True)\n",
    "    country = Column(String(100), nullable=True)\n",
    "    city = Column(String(100), nullable=True)\n",
    "    store_name = Column(String(100))\n",
    "    number_of_employees = Column(Integer)\n",
    "    zip_code = Column(String(20))\n",
    "    latitude = Column(Float, nullable=True)\n",
    "    longitude = Column(Float, nullable=True)\n",
    "\n",
    "class Employees(Base):\n",
    "    __tablename__ = 'employees'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    employee_id = Column(Integer, primary_key=True)\n",
    "    store_id = Column(Integer, ForeignKey(f'{SCHEMA_NAME}.stores.store_id'))\n",
    "    name = Column(String(255), nullable=True)\n",
    "    position = Column(String(100), nullable=True)\n",
    "\n",
    "class Customers(Base):\n",
    "    __tablename__ = 'customers'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    customer_id = Column(Integer, primary_key=True)\n",
    "    name = Column(String(255), nullable=True)\n",
    "    email = Column(String(255), nullable=True)\n",
    "    join_date = Column(Date, nullable=True)\n",
    "    telephone = Column(String(50), nullable=True)\n",
    "    city = Column(String(100), nullable=True)\n",
    "    country = Column(String(100), nullable=True)\n",
    "    gender = Column(String(20), nullable=True)\n",
    "    date_of_birth = Column(Date, nullable=True)\n",
    "    job_title = Column(String(100), nullable=True)\n",
    "\n",
    "class Products(Base):\n",
    "    __tablename__ = 'products'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    product_id = Column(Integer, primary_key=True)\n",
    "    category = Column(String(100), nullable=True)\n",
    "    sub_category = Column(String(100))\n",
    "    description_pt = Column(Text)\n",
    "    description_de = Column(Text)\n",
    "    description_fr = Column(Text)\n",
    "    description_es = Column(Text)\n",
    "    description_en = Column(Text)\n",
    "    description_zh = Column(Text)\n",
    "    color = Column(String(50), nullable=True)\n",
    "    sizes = Column(String(100), nullable=True)\n",
    "    production_cost = Column(Float)\n",
    "\n",
    "class Discounts(Base):\n",
    "    __tablename__ = 'discounts'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    start_date = Column(Date)\n",
    "    end_date = Column(Date)\n",
    "    discount_rate = Column(Float)\n",
    "    description = Column(String(255), nullable=True)\n",
    "    category = Column(String(100), nullable=True)\n",
    "    sub_category = Column(String(100))\n",
    "\n",
    "class Transactions(Base):\n",
    "    __tablename__ = 'transactions'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    invoice_id = Column(String(50), nullable=True)\n",
    "    line_number = Column(Integer)\n",
    "    customer_id = Column(Integer, ForeignKey(f'{SCHEMA_NAME}.customers.customer_id'))\n",
    "    product_id = Column(Integer, ForeignKey(f'{SCHEMA_NAME}.products.product_id'))\n",
    "    size = Column(String(10))\n",
    "    color = Column(String(50), nullable=True)\n",
    "    unit_price = Column(Float)\n",
    "    quantity = Column(Integer, nullable=True)\n",
    "    transaction_date = Column(DateTime)\n",
    "    discount = Column(Float, nullable=True)\n",
    "    line_total = Column(Float)\n",
    "    store_id = Column(Integer, ForeignKey(f'{SCHEMA_NAME}.stores.store_id'))\n",
    "    employee_id = Column(Integer, ForeignKey(f'{SCHEMA_NAME}.employees.employee_id'))\n",
    "    currency = Column(String(10), nullable=True)\n",
    "    currency_symbol = Column(String(5))\n",
    "    sku = Column(String(100))\n",
    "    transaction_type = Column(String(20))\n",
    "    payment_method = Column(String(50))\n",
    "    invoice_total = Column(Float)\n",
    "    is_return = Column(Boolean)\n",
    "\n",
    "# --- Extract ---\n",
    "\n",
    "def extract_data(spark: SparkSession, file_path: str, table_name: str) -> Optional[DataFrame]:\n",
    "    \"\"\"\n",
    "    Extracts data from a CSV file into a Spark DataFrame.\n",
    "    Attempts schema inference, using sampling for potentially large files.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        logger.warning(f\"File not found: {file_path}. Skipping extraction for {table_name}.\")\n",
    "        return None\n",
    "\n",
    "    logger.info(f\"Extracting data from {file_path}...\")\n",
    "    try:\n",
    "        # For large files like customers and transactions, use sampling for schema inference\n",
    "        if table_name in ['customers', 'transactions']:\n",
    "            sample_ratio = 0.1\n",
    "            logger.info(f\"Using sampling ratio {sample_ratio} for schema inference on {file_name}.\")\n",
    "            sample_df = spark.read.csv(file_path, header=True, inferSchema=True, samplingRatio=sample_ratio)\n",
    "            schema = sample_df.schema\n",
    "            spark_df = spark.read.csv(file_path, header=True, schema=schema)\n",
    "            logger.info(f\"Extracted {table_name} with schema from sample.\")\n",
    "        else:\n",
    "            # For smaller files, infer schema directly\n",
    "            spark_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "            logger.info(f\"Extracted {table_name} with inferred schema.\")\n",
    "\n",
    "        logger.info(f\"Extracted DataFrame schema for {table_name}:\")\n",
    "        spark_df.printSchema()\n",
    "        return spark_df\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting data from {file_path}: {str(e)}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "# --- Transform ---\n",
    "\n",
    "def process_stores(spark_df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Transforms the stores DataFrame.\"\"\"\n",
    "    logger.info(\"Transforming stores data...\")\n",
    "    transformed_df = spark_df.select(\n",
    "        col(\"Store ID\").alias(\"store_id\").cast(IntegerType()),\n",
    "        when(col(\"Country\").isNull(), lit(\"Unknown\")).otherwise(col(\"Country\")).alias(\"country\"),\n",
    "        when(col(\"City\").isNull(), lit(\"Unknown\")).otherwise(col(\"City\")).alias(\"city\"),\n",
    "        col(\"Store Name\").alias(\"store_name\").cast(StringType()),\n",
    "        col(\"Number of Employees\").alias(\"number_of_employees\").cast(IntegerType()),\n",
    "        col(\"ZIP Code\").alias(\"zip_code\").cast(StringType()),\n",
    "        col(\"Latitude\").alias(\"latitude\").cast(FloatType()),\n",
    "        col(\"Longitude\").alias(\"longitude\").cast(FloatType())\n",
    "    )\n",
    "    logger.info(\"Stores data transformation complete.\")\n",
    "    return transformed_df\n",
    "\n",
    "def process_employees(spark_df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Transforms the employees DataFrame.\"\"\"\n",
    "    logger.info(\"Transforming employees data...\")\n",
    "    transformed_df = spark_df.select(\n",
    "        col(\"Employee ID\").alias(\"employee_id\").cast(IntegerType()),\n",
    "        col(\"Store ID\").alias(\"store_id\").cast(IntegerType()),\n",
    "        when(col(\"Name\").isNull(), lit(\"Unknown\")).otherwise(col(\"Name\")).alias(\"name\"),\n",
    "        when(col(\"Position\").isNull(), lit(\"Unknown\")).otherwise(col(\"Position\")).alias(\"position\")\n",
    "    )\n",
    "    logger.info(\"Employees data transformation complete.\")\n",
    "    return transformed_df\n",
    "\n",
    "def process_customers(spark_df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Transforms the customers DataFrame.\"\"\"\n",
    "    logger.info(\"Transforming customers data...\")\n",
    "    transformed_df = spark_df.select(\n",
    "        col(\"Customer ID\").alias(\"customer_id\").cast(IntegerType()),\n",
    "        when(col(\"Name\").isNull(), lit(\"Unknown\")).otherwise(col(\"Name\")).alias(\"name\"),\n",
    "        when(col(\"Email\").isNull(), lit(\"unknown@example.com\")).otherwise(col(\"Email\")).alias(\"email\"),\n",
    "        to_date(col(\"Join Date\"), \"yyyy-MM-dd\").alias(\"join_date\"), # Assuming 'Join Date' in input\n",
    "        col(\"Telephone\").alias(\"telephone\"),\n",
    "        col(\"City\").alias(\"city\"),\n",
    "        col(\"Country\").alias(\"country\"),\n",
    "        col(\"Gender\").alias(\"gender\"),\n",
    "        to_date(col(\"Date Of Birth\"), \"yyyy-MM-dd\").alias(\"date_of_birth\"),\n",
    "        col(\"Job Title\").alias(\"job_title\")\n",
    "    )\n",
    "    logger.info(\"Customers data transformation complete.\")\n",
    "    return transformed_df\n",
    "\n",
    "\n",
    "def process_products(spark_df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Transforms the products DataFrame.\"\"\"\n",
    "    logger.info(\"Transforming products data...\")\n",
    "    transformed_df = spark_df.select(\n",
    "        col(\"Product ID\").alias(\"product_id\").cast(IntegerType()),\n",
    "        when(col(\"Category\").isNull(), lit(\"Unknown\")).otherwise(col(\"Category\")).alias(\"category\"),\n",
    "        col(\"Sub Category\").alias(\"sub_category\"),\n",
    "        col(\"Description PT\").alias(\"description_pt\").cast(StringType()),\n",
    "        col(\"Description DE\").alias(\"description_de\").cast(StringType()),\n",
    "        col(\"Description FR\").alias(\"description_fr\").cast(StringType()),\n",
    "        col(\"Description ES\").alias(\"description_es\").cast(StringType()),\n",
    "        col(\"Description EN\").alias(\"description_en\").cast(StringType()),\n",
    "        col(\"Description ZH\").alias(\"description_zh\").cast(StringType()),\n",
    "        when(col(\"Color\").isNull(), lit(\"Unknown\")).otherwise(col(\"Color\")).alias(\"color\"),\n",
    "        when(col(\"Sizes\").isNull(), lit(\"Unknown\")).otherwise(col(\"Sizes\")).alias(\"sizes\"),\n",
    "        col(\"Production Cost\").alias(\"production_cost\").cast(FloatType())\n",
    "    )\n",
    "    logger.info(\"Products data transformation complete.\")\n",
    "    return transformed_df\n",
    "\n",
    "def process_discounts(spark_df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Transforms the discounts DataFrame.\"\"\"\n",
    "    logger.info(\"Transforming discounts data...\")\n",
    "    transformed_df = spark_df.select(\n",
    "        # id is auto-incrementing in DB, so we don't select it from source\n",
    "        to_date(col(\"Start\"), \"yyyy-MM-dd\").alias(\"start_date\"), # Assuming 'Start' in input\n",
    "        to_date(col(\"End\"), \"yyyy-MM-dd\").alias(\"end_date\"),     # Assuming 'End' in input\n",
    "        col(\"Discont\").alias(\"discount_rate\").cast(FloatType()), # Assuming 'Discont' typo in input\n",
    "        when(col(\"Description\").isNull(), lit(\"No description\")).otherwise(col(\"Description\")).alias(\"description\"),\n",
    "        when(col(\"Category\").isNull(), lit(\"General\")).otherwise(col(\"Category\")).alias(\"category\"),\n",
    "        col(\"Sub Category\").alias(\"sub_category\")\n",
    "    )\n",
    "    logger.info(\"Discounts data transformation complete.\")\n",
    "    return transformed_df\n",
    "\n",
    "def process_transactions(spark_df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Transforms the transactions DataFrame.\"\"\"\n",
    "    logger.info(\"Transforming transactions data...\")\n",
    "    transformed_df = spark_df.select(\n",
    "        # id is auto-incrementing in DB, so we don't select it from source\n",
    "        when(col(\"Invoice ID\").isNull(), lit(\"UNKNOWN\")).otherwise(col(\"Invoice ID\")).alias(\"invoice_id\"),\n",
    "        col(\"Line\").alias(\"line_number\").cast(IntegerType()),\n",
    "        col(\"Customer ID\").alias(\"customer_id\").cast(IntegerType()),\n",
    "        col(\"Product ID\").alias(\"product_id\").cast(IntegerType()),\n",
    "        col(\"Size\").alias(\"size\").cast(StringType()),\n",
    "        when(col(\"Color\").isNull(), lit(\"Unknown\")).otherwise(col(\"Color\")).alias(\"color\"),\n",
    "        col(\"Unit Price\").alias(\"unit_price\").cast(FloatType()),\n",
    "        when(col(\"Quantity\").isNull(), lit(1)).otherwise(col(\"Quantity\")).alias(\"quantity\").cast(IntegerType()),\n",
    "        col(\"Date\").alias(\"transaction_date\").cast(DateTime()), # Assuming 'Date' in input is datetime\n",
    "        when(col(\"Discount\").isNull(), lit(0.0)).otherwise(col(\"Discount\")).alias(\"discount\").cast(FloatType()),\n",
    "        col(\"Line Total\").alias(\"line_total\").cast(FloatType()),\n",
    "        col(\"Store ID\").alias(\"store_id\").cast(IntegerType()),\n",
    "        col(\"Employee ID\").alias(\"employee_id\").cast(IntegerType()),\n",
    "        when(col(\"Currency\").isNull(), lit(\"USD\")).otherwise(col(\"Currency\")).alias(\"currency\"),\n",
    "        col(\"Currency Symbol\").alias(\"currency_symbol\"),\n",
    "        col(\"SKU\").alias(\"sku\"),\n",
    "        col(\"Transaction Type\").alias(\"transaction_type\"),\n",
    "        col(\"Payment Method\").alias(\"payment_method\"),\n",
    "        col(\"Invoice Total\").alias(\"invoice_total\").cast(FloatType()),\n",
    "        (col(\"Transaction Type\") == \"Return\").alias(\"is_return\").cast(BooleanType())\n",
    "    )\n",
    "    logger.info(\"Transactions data transformation complete.\")\n",
    "    return transformed_df\n",
    "\n",
    "\n",
    "# Mapping table names to their processing functions and primary keys\n",
    "TABLE_PROCESSING_MAP: Dict[str, Tuple[callable, str]] = {\n",
    "    'stores': (process_stores, 'store_id'),\n",
    "    'employees': (process_employees, 'employee_id'),\n",
    "    'customers': (process_customers, 'customer_id'),\n",
    "    'products': (process_products, 'product_id'),\n",
    "    'discounts': (process_discounts, 'id'), # Use 'id' for discounts PK\n",
    "    'transactions': (process_transactions, 'id'), # Use 'id' for transactions PK\n",
    "}\n",
    "\n",
    "# --- Load ---\n",
    "\n",
    "def setup_database(engine: Engine) -> bool:\n",
    "    \"\"\"Create schema and tables if they don't exist, and set permissions.\"\"\"\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            # Check if schema exists\n",
    "            schema_exists = conn.execute(\n",
    "                text(\"SELECT 1 FROM information_schema.schemata WHERE schema_name = :schema\"),\n",
    "                {'schema': SCHEMA_NAME}\n",
    "            ).scalar()\n",
    "\n",
    "            if not schema_exists:\n",
    "                logger.info(f\"Creating schema {SCHEMA_NAME}\")\n",
    "                conn.execute(text(f'CREATE SCHEMA \"{SCHEMA_NAME}\"')) # Quote schema name\n",
    "                db_user = os.getenv('DB_USER', 'postgres')\n",
    "                conn.execute(text(f'''\n",
    "                    GRANT ALL ON SCHEMA \"{SCHEMA_NAME}\" TO \"{db_user}\";\n",
    "                    ALTER DEFAULT PRIVILEGES IN SCHEMA \"{SCHEMA_NAME}\"\n",
    "                    GRANT ALL ON TABLES TO \"{db_user}\";\n",
    "                ''')) # Quote user name\n",
    "\n",
    "            # Create all tables defined by the Base metadata\n",
    "            # This will create tables only if they don't exist\n",
    "            logger.info(f\"Creating tables in schema {SCHEMA_NAME} if they don't exist...\")\n",
    "            Base.metadata.create_all(engine)\n",
    "            logger.info(\"Table creation process completed.\")\n",
    "\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Database setup failed: {str(e)}\", exc_info=True)\n",
    "        return False\n",
    "\n",
    "\n",
    "def upsert_data(\n",
    "    session: any, # Use `sqlalchemy.orm.Session` in a real project, but 'any' for simplicity here\n",
    "    table_class: type,\n",
    "    records: List[Dict[str, Any]],\n",
    "    primary_key: str,\n",
    "    batch_size: int = 1000\n",
    "):\n",
    "    \"\"\"Upsert data into the database with batch processing.\"\"\"\n",
    "    total_records = len(records)\n",
    "    if total_records == 0:\n",
    "        logger.info(f\"No records to upsert for {table_class.__tablename__}.\")\n",
    "        return\n",
    "\n",
    "    table_name = table_class.__tablename__\n",
    "    logger.info(f\"Starting to upsert {total_records} records into {SCHEMA_NAME}.{table_name} in batches of {batch_size}\")\n",
    "\n",
    "    for i in range(0, total_records, batch_size):\n",
    "        batch = records[i:i + batch_size]\n",
    "        batch_num = (i // batch_size) + 1\n",
    "        total_batches = (total_records + batch_size - 1) // batch_size # Correct total batch calculation\n",
    "\n",
    "        try:\n",
    "            # Attempt bulk insert first\n",
    "            session.bulk_insert_mappings(table_class, batch)\n",
    "            session.commit()\n",
    "            logger.debug(f\"Successfully bulk inserted batch {batch_num}/{total_batches}\") # Use debug for high volume\n",
    "        except Exception:\n",
    "            session.rollback()\n",
    "            logger.info(f\"Bulk insert failed for batch {batch_num}. Switching to individual upsert.\")\n",
    "\n",
    "            # Fallback to individual upsert if bulk insert fails (e.g., due to conflicts)\n",
    "            for record in batch:\n",
    "                try:\n",
    "                    # Check if the record exists by primary key\n",
    "                    existing = session.query(table_class).filter(\n",
    "                        getattr(table_class, primary_key) == record.get(primary_key) # Use get for safety\n",
    "                    ).first()\n",
    "\n",
    "                    if existing:\n",
    "                        # Update existing record\n",
    "                        for key, value in record.items():\n",
    "                            setattr(existing, key, value)\n",
    "                        # logger.debug(f\"Updated record with {primary_key}={record.get(primary_key)}\")\n",
    "                    else:\n",
    "                        # Insert new record\n",
    "                        new_record = table_class(**record)\n",
    "                        session.add(new_record)\n",
    "                        # logger.debug(f\"Added new record with {primary_key}={record.get(primary_key)}\")\n",
    "\n",
    "                except Exception as rec_error:\n",
    "                    logger.error(\n",
    "                        f\"Error processing record {record.get(primary_key, 'N/A')} \"\n",
    "                        f\"in batch {batch_num}: {str(rec_error)}\", exc_info=True\n",
    "                    )\n",
    "                    session.rollback() # Rollback individual record's transaction if it fails\n",
    "\n",
    "            try:\n",
    "                session.commit()\n",
    "                logger.info(f\"Successfully upserted (bulk or individual) batch {batch_num}/{total_batches}\")\n",
    "            except Exception as commit_error:\n",
    "                session.rollback()\n",
    "                logger.error(f\"Failed to commit batch {batch_num}: {str(commit_error)}\", exc_info=True)\n",
    "                # Depending on severity, you might want to raise the exception here\n",
    "                # raise commit_error # Uncomment to fail the whole process on commit error\n",
    "                continue # Continue to the next batch if commit fails\n",
    "\n",
    "\n",
    "def write_spark_df_to_postgres(spark_df: DataFrame, table_name: str, engine: Engine):\n",
    "    \"\"\"Writes Spark DataFrame to PostgreSQL with optimized methods.\"\"\"\n",
    "    try:\n",
    "        # Dynamically get the SQLAlchemy model class and primary key\n",
    "        table_class = globals().get(table_name.capitalize())\n",
    "        primary_key = TABLE_PROCESSING_MAP.get(table_name, (None, None))[1]\n",
    "\n",
    "        if not table_class or not primary_key:\n",
    "            logger.error(f\"Database model or primary key not defined for table '{table_name}'. Skipping load.\")\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Starting load process for table: {SCHEMA_NAME}.{table_name}\")\n",
    "\n",
    "        # Use toLocalIterator for large DataFrames to avoid OOM on driver\n",
    "        # Convert Spark DataFrame to an iterator of Pandas DataFrames or records\n",
    "        # Using toLocalIterator with asDict() to get dictionaries\n",
    "        records_iterator = (row.asDict() for row in spark_df.toLocalIterator())\n",
    "\n",
    "        records_buffer: List[Dict[str, Any]] = []\n",
    "        buffer_size = 5000 # Adjust buffer size as needed\n",
    "\n",
    "        Session = sessionmaker(bind=engine)\n",
    "\n",
    "        for i, record in enumerate(records_iterator, 1):\n",
    "            records_buffer.append(record)\n",
    "\n",
    "            if len(records_buffer) >= buffer_size:\n",
    "                session = Session()\n",
    "                try:\n",
    "                    upsert_data(session, table_class, records_buffer, primary_key)\n",
    "                    logger.info(f\"Processed and upserted {i} records for {table_name}.\")\n",
    "                    records_buffer = [] # Clear buffer after successful upsert\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error during upsert for batch ending at record {i}: {str(e)}\")\n",
    "                    session.rollback() # Ensure session is rolled back on error\n",
    "                    # Decide whether to continue or break on error\n",
    "                    # break # Uncomment to stop processing on first upsert error\n",
    "                finally:\n",
    "                    session.close()\n",
    "\n",
    "        # Upsert any remaining records in the buffer\n",
    "        if records_buffer:\n",
    "            session = Session()\n",
    "            try:\n",
    "                upsert_data(session, table_class, records_buffer, primary_key)\n",
    "                logger.info(f\"Processed and upserted final batch for {table_name}.\")\n",
    "            except Exception as e:\n",
    "                 logger.error(f\"Error during final upsert for {table_name}: {str(e)}\")\n",
    "                 session.rollback()\n",
    "            finally:\n",
    "                session.close()\n",
    "\n",
    "\n",
    "        # Optional: Verify count for smaller tables or periodically for large ones\n",
    "        # Skipping final count verification for very large tables like transactions\n",
    "        if table_name not in ['transactions']:\n",
    "             with engine.connect() as conn:\n",
    "                 # Use quoted identifiers for the schema and table name\n",
    "                 count_query = text(f'SELECT COUNT(*) FROM \"{SCHEMA_NAME}\".\"{table_name}\"')\n",
    "                 count = conn.execute(count_query).scalar()\n",
    "                 logger.info(f\"Final record count in {SCHEMA_NAME}.{table_name}: {count}\")\n",
    "\n",
    "        logger.info(f\"Load process completed for table: {SCHEMA_NAME}.{table_name}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error writing to {table_name}: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "\n",
    "# --- Main Pipeline Execution ---\n",
    "\n",
    "def run_etl_pipeline():\n",
    "    \"\"\"Orchestrates the ETL process for all datasets.\"\"\"\n",
    "    db_user = os.getenv('DB_USER')\n",
    "    db_password = os.getenv('DB_PASSWORD')\n",
    "    db_host = os.getenv('DB_HOST')\n",
    "    db_port = os.getenv('DB_PORT', '5432') # Use default port if not set\n",
    "    db_name = os.getenv('DB_NAME')\n",
    "    dataset_path = os.getenv('DATASET_PATH')\n",
    "\n",
    "    required_vars = {\n",
    "        'DB_USER': db_user,\n",
    "        'DB_PASSWORD': db_password,\n",
    "        'DB_HOST': db_host,\n",
    "        'DB_NAME': db_name,\n",
    "        'DATASET_PATH': dataset_path\n",
    "    }\n",
    "\n",
    "    for var_name, var_value in required_vars.items():\n",
    "        if not var_value:\n",
    "            logger.error(f\"Missing required environment variable: {var_name}\")\n",
    "            # Do not log password value\n",
    "            sys.exit(f\"Error: Missing required environment variable: {var_name}\") # Exit if essential vars are missing\n",
    "\n",
    "    engine: Optional[Engine] = None\n",
    "    spark_session: Optional[SparkSession] = None\n",
    "\n",
    "    try:\n",
    "        # 1. Create Spark Session\n",
    "        spark_session = create_spark_session()\n",
    "        if not spark_session:\n",
    "            raise RuntimeError(\"Failed to create Spark session.\")\n",
    "\n",
    "        # 2. Setup Database Connection and Schema/Tables\n",
    "        db_url = f'postgresql+psycopg2://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}'\n",
    "        logger.info(f\"Attempting to connect to database: {db_host}:{db_port}/{db_name}\")\n",
    "        engine = create_engine(\n",
    "            db_url,\n",
    "            pool_size=20,\n",
    "            max_overflow=30,\n",
    "            pool_pre_ping=True,\n",
    "            pool_recycle=3600,\n",
    "            connect_args={'connect_timeout': 10}\n",
    "        )\n",
    "\n",
    "        # Test connection\n",
    "        with engine.connect() as connection:\n",
    "            connection.execute(text(\"SELECT 1\"))\n",
    "        logger.info(\"Database connection successful.\")\n",
    "\n",
    "        if not setup_database(engine):\n",
    "            raise RuntimeError(\"Database setup failed.\")\n",
    "\n",
    "        # 3. ETL Process for each file\n",
    "        # The order of processing might matter if there are foreign key constraints,\n",
    "        # but upsert handles existing keys, so order is less critical for upsert.\n",
    "        # However, loading parent tables before children is generally safer.\n",
    "        # Revised order based on dependencies: Stores, Employees, Customers, Products, Discounts, Transactions\n",
    "        file_configs = [\n",
    "            ('stores.csv', 'stores'),\n",
    "            ('employees.csv', 'employees'),\n",
    "            ('customers.csv', 'customers'),\n",
    "            ('products.csv', 'products'),\n",
    "            ('discounts.csv', 'discounts'),\n",
    "            ('transactions.csv', 'transactions'),\n",
    "        ]\n",
    "\n",
    "        for file_name, table_name in file_configs:\n",
    "            file_path = os.path.join(dataset_path, file_name)\n",
    "            processor, primary_key = TABLE_PROCESSING_MAP.get(table_name, (None, None))\n",
    "\n",
    "            if processor is None:\n",
    "                 logger.error(f\"No processing function defined for table '{table_name}'. Skipping.\")\n",
    "                 continue\n",
    "\n",
    "            logger.info(f\"--- Starting processing for {file_name} (Table: {table_name}) ---\")\n",
    "            process_start_time = datetime.now()\n",
    "\n",
    "            # Extract\n",
    "            spark_df = extract_data(spark_session, file_path, table_name)\n",
    "            if spark_df is None:\n",
    "                logger.warning(f\"Skipping transformation and load for {file_name} due to extraction failure.\")\n",
    "                continue\n",
    "\n",
    "            # Transform\n",
    "            try:\n",
    "                 transformed_df = processor(spark_df)\n",
    "                 logger.info(f\"Transformation complete for {file_name}.\")\n",
    "            except Exception as e:\n",
    "                 logger.error(f\"Error during transformation for {file_name}: {str(e)}\", exc_info=True)\n",
    "                 logger.warning(f\"Skipping load for {file_name} due to transformation failure.\")\n",
    "                 continue # Skip load if transformation fails\n",
    "\n",
    "            # Load\n",
    "            try:\n",
    "                write_spark_df_to_postgres(transformed_df, table_name, engine)\n",
    "                logger.info(f\"Load complete for {file_name}.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error during load for {file_name}: {str(e)}\", exc_info=True)\n",
    "                # Decide if a load error should stop the entire pipeline or just skip this file\n",
    "                # raise # Uncomment to stop the pipeline on any load error\n",
    "\n",
    "            logger.info(f\"--- Finished processing for {file_name} in {datetime.now() - process_start_time} ---\")\n",
    "\n",
    "        logger.info(\"ETL pipeline completed successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during the ETL pipeline: {str(e)}\", exc_info=True)\n",
    "        # Consider adding a cleanup step here for partial loads if needed\n",
    "        raise # Re-raise the exception to indicate failure\n",
    "\n",
    "    finally:\n",
    "        # 4. Cleanup\n",
    "        if engine:\n",
    "            engine.dispose()\n",
    "            logger.info(\"Database engine disposed.\")\n",
    "        if spark_session:\n",
    "            spark_session.stop()\n",
    "            logger.info(\"Spark session stopped.\")\n",
    "\n",
    "# --- Script Entry Point ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"Starting Global Fashion Retail Data Load process...\")\n",
    "    start_time = datetime.now()\n",
    "    try:\n",
    "        run_etl_pipeline()\n",
    "        end_time = datetime.now()\n",
    "        logger.info(f\"Global Fashion Retail Data Load process completed successfully in {end_time - start_time}.\")\n",
    "        sys.exit(0) # Indicate success\n",
    "    except Exception:\n",
    "        end_time = datetime.now()\n",
    "        logger.error(f\"Global Fashion Retail Data Load process failed after {end_time - start_time}.\")\n",
    "        sys.exit(1) # Indicate failure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ce89ab",
   "metadata": {},
   "source": [
    "Cleanse and Aggregate for Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf53cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Float, Date, DateTime, Boolean, ForeignKey, Text, text, PrimaryKeyConstraint\n",
    "from sqlalchemy.orm import declarative_base, sessionmaker, relationship\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, to_date, lit, datediff, current_date, dayofweek, sum as spark_sum, avg as spark_avg, count as spark_count, to_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType, BooleanType, TimestampType\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('data_processor.log', encoding='utf-8'), # Changed log file name\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Spark with Hadoop workaround for Windows and JDBC driver config\n",
    "def create_spark_session():\n",
    "    \"\"\"Create and configure Spark session, including JDBC driver via spark.driver.extraClassPath.\"\"\"\n",
    "    try:\n",
    "        # Set Hadoop home directory if on Windows\n",
    "        # This is often needed for Spark to function correctly on Windows, even without HDFS\n",
    "        if os.name == 'nt':\n",
    "            # Ensure these paths are correct for your Windows setup if needed\n",
    "            # Consider adding HADOOP_HOME_WIN to your .env or set it here if consistent\n",
    "            hadoop_home = os.getenv('HADOOP_HOME_WIN', 'C:\\\\hadoop')\n",
    "            os.environ['HADOOP_HOME'] = hadoop_home\n",
    "            # Add Hadoop bin to PATH if it exists\n",
    "            hadoop_bin = os.path.join(hadoop_home, 'bin')\n",
    "            if os.path.exists(hadoop_bin):\n",
    "                 os.environ['PATH'] = os.environ['PATH'] + ';' + hadoop_bin\n",
    "\n",
    "            # Set Python path explicitly for PySpark\n",
    "            python_path = sys.executable\n",
    "            os.environ['PYSPARK_PYTHON'] = python_path\n",
    "            os.environ['PYSPARK_DRIVER_PYTHON'] = python_path\n",
    "\n",
    "        # Create temp directory if it doesn't exist\n",
    "        # Use environment variable with default for flexibility\n",
    "        temp_dir = os.getenv('SPARK_TEMP_DIR', 'D:/spark_temp')\n",
    "        if not os.path.exists(temp_dir):\n",
    "            os.makedirs(temp_dir)\n",
    "            logger.info(f\"Created Spark temporary directory: {temp_dir}\")\n",
    "\n",
    "        spark_builder = SparkSession.builder \\\n",
    "            .appName(\"GlobalFashionRetailDataProcessor\") \\\n",
    "            .config(\"spark.local.dir\", temp_dir) \\\n",
    "            .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "            .config(\"spark.executor.memory\", \"8g\") \\\n",
    "            .config(\"spark.driver.memory\", \"8g\") \\\n",
    "            .config(\"spark.memory.offHeap.enabled\", \"false\") \\\n",
    "            .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "            .config(\"spark.default.parallelism\", \"200\") \\\n",
    "            .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "            .config(\"spark.network.timeout\", \"600s\") \\\n",
    "            .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "            .config(\"spark.python.profile\", \"false\") \\\n",
    "            .config(\"spark.executor.instances\", \"4\") \\\n",
    "            .config(\"spark.executor.cores\", \"2\") \\\n",
    "            .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "            .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "            .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"true\") \\\n",
    "            .config(\"spark.sql.debug.maxToStringFields\", 100) # Increase for wider dataframes in logging\n",
    "\n",
    "        # === Configure JDBC driver via spark.driver.extraClassPath ===\n",
    "        # Manually specify the path to the downloaded JAR file using the POSTGRES_JDBC_DRIVER_PATH env var\n",
    "        # This config is specifically for the driver program (where your Python script runs)\n",
    "        jdbc_jar_path = os.getenv('POSTGRES_JDBC_DRIVER_PATH')\n",
    "\n",
    "        if not jdbc_jar_path:\n",
    "            logger.error(\"POSTGRES_JDBC_DRIVER_PATH environment variable is not set.\")\n",
    "            raise ValueError(\"POSTGRES_JDBC_DRIVER_PATH environment variable must be set to the path of the PostgreSQL JDBC driver JAR.\")\n",
    "\n",
    "        if not os.path.exists(jdbc_jar_path):\n",
    "             logger.error(f\"PostgreSQL JDBC driver not found at specified path: {jdbc_jar_path}\")\n",
    "             raise FileNotFoundError(f\"PostgreSQL JDBC driver not found at {jdbc_jar_path}. Please download the JAR and set POSTGRES_JDBC_DRIVER_PATH.\")\n",
    "\n",
    "        logger.info(f\"Configuring Spark driver with extra classpath: {jdbc_jar_path}\")\n",
    "        spark_builder = spark_builder.config(\"spark.driver.extraClassPath\", jdbc_jar_path)\n",
    "        # =====================================================\n",
    "\n",
    "        # === Removed the spark.jars.packages configuration block ===\n",
    "        # jdbc_package = \"org.postgresql:postgresql:42.6.0\"\n",
    "        # logger.info(f\"Configuring Spark to use JDBC package: {jdbc_package}\")\n",
    "        # spark_builder = spark_builder.config(\"spark.jars.packages\", jdbc_package)\n",
    "        # =====================================================\n",
    "\n",
    "\n",
    "        logger.info(\"Attempting to get or create Spark session...\")\n",
    "        spark_session = spark_builder.getOrCreate()\n",
    "        logger.info(\"Spark session created successfully.\")\n",
    "        return spark_session\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create Spark session: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "# Initialize Spark\n",
    "spark = create_spark_session()\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Database schema definition\n",
    "Base = declarative_base()\n",
    "SCHEMA_NAME = \"GFRetail\" # Ensure this matches your database schema name\n",
    "\n",
    "\n",
    "class Stores(Base):\n",
    "    __tablename__ = 'stores'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    store_id = Column(Integer, primary_key=True)\n",
    "    country = Column(String(100), nullable=True)\n",
    "    city = Column(String(100), nullable=True)\n",
    "    store_name = Column(String(100))\n",
    "    number_of_employees = Column(Integer)\n",
    "    zip_code = Column(String(20))\n",
    "    latitude = Column(Float, nullable=True)\n",
    "    longitude = Column(Float, nullable=True)\n",
    "\n",
    "class Employees(Base):\n",
    "    __tablename__ = 'employees'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    employee_id = Column(Integer, primary_key=True)\n",
    "    store_id = Column(Integer, ForeignKey(f'{SCHEMA_NAME}.stores.store_id'))\n",
    "    name = Column(String(255), nullable=True)\n",
    "    position = Column(String(100), nullable=True)\n",
    "\n",
    "class Customers(Base):\n",
    "    __tablename__ = 'customers'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    customer_id = Column(Integer, primary_key=True)\n",
    "    name = Column(String(255), nullable=True)\n",
    "    email = Column(String(255), nullable=True)\n",
    "    join_date = Column(Date, nullable=True)\n",
    "    telephone = Column(String(50), nullable=True)\n",
    "    city = Column(String(100), nullable=True)\n",
    "    country = Column(String(100), nullable=True)\n",
    "    gender = Column(String(20), nullable=True)\n",
    "    date_of_birth = Column(Date, nullable=True)\n",
    "    job_title = Column(String(100), nullable=True)\n",
    "    # New Feature Column - will be added to DB if running create_all and it's missing\n",
    "    days_since_birth = Column(Integer, nullable=True)\n",
    "\n",
    "class Products(Base):\n",
    "    __tablename__ = 'products'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    product_id = Column(Integer, primary_key=True)\n",
    "    category = Column(String(100), nullable=True)\n",
    "    sub_category = Column(String(100))\n",
    "    description_pt = Column(Text)\n",
    "    description_de = Column(Text)\n",
    "    description_fr = Column(Text)\n",
    "    description_es = Column(Text)\n",
    "    description_en = Column(Text)\n",
    "    description_zh = Column(Text)\n",
    "    color = Column(String(50), nullable=True)\n",
    "    sizes = Column(String(100), nullable=True)\n",
    "    production_cost = Column(Float)\n",
    "\n",
    "class Discounts(Base):\n",
    "    __tablename__ = 'discounts'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True) # Assuming this is the PK in DB\n",
    "    start_date = Column(Date)\n",
    "    end_date = Column(Date)\n",
    "    discount_rate = Column(Float)\n",
    "    description = Column(String(255), nullable=True)\n",
    "    category = Column(String(100), nullable=True)\n",
    "    sub_category = Column(String(100))\n",
    "\n",
    "class Transactions(Base):\n",
    "    __tablename__ = 'transactions'\n",
    "    __table_args__ = {'schema': SCHEMA_NAME}\n",
    "    # Assuming 'id' is the primary key already in the DB\n",
    "    id = Column(Integer, primary_key=True) # Assuming this is pre-existing PK\n",
    "    invoice_id = Column(String(50), nullable=True)\n",
    "    line_number = Column(Integer)\n",
    "    customer_id = Column(Integer, ForeignKey(f'{SCHEMA_NAME}.customers.customer_id'))\n",
    "    product_id = Column(Integer, ForeignKey(f'{SCHEMA_NAME}.products.product_id'))\n",
    "    size = Column(String(10))\n",
    "    color = Column(String(50), nullable=True)\n",
    "    unit_price = Column(Float)\n",
    "    quantity = Column(Integer, nullable=True)\n",
    "    transaction_date = Column(DateTime) # Or Timestamp if that's the DB type\n",
    "    discount = Column(Float, nullable=True)\n",
    "    line_total = Column(Float)\n",
    "    store_id = Column(Integer, ForeignKey(f'{SCHEMA_NAME}.stores.store_id'))\n",
    "    employee_id = Column(Integer, ForeignKey(f'{SCHEMA_NAME}.employees.employee_id'))\n",
    "    currency = Column(String(10), nullable=True)\n",
    "    currency_symbol = Column(String(5))\n",
    "    sku = Column(String(100))\n",
    "    transaction_type = Column(String(20))\n",
    "    payment_method = Column(String(50))\n",
    "    invoice_total = Column(Float)\n",
    "    is_return = Column(Boolean)\n",
    "    # New Feature Columns - will be added to DB if running create_all and they are missing\n",
    "    net_line_total = Column(Float, nullable=True)\n",
    "    transaction_day_of_week = Column(Integer, nullable=True)\n",
    "\n",
    "class DailyStoreSales(Base):\n",
    "    __tablename__ = 'daily_store_sales'\n",
    "    __table_args__ = (\n",
    "        PrimaryKeyConstraint('store_id', 'sale_date'), # Using composite primary key\n",
    "        {'schema': SCHEMA_NAME}\n",
    "    )\n",
    "    store_id = Column(Integer, ForeignKey(f'{SCHEMA_NAME}.stores.store_id'), nullable=False)\n",
    "    sale_date = Column(Date, nullable=False)\n",
    "    transaction_count = Column(Integer, nullable=True)\n",
    "    total_sales = Column(Float, nullable=True)\n",
    "    average_unit_price = Column(Float, nullable=True)\n",
    "    total_quantity_sold = Column(Integer, nullable=True)\n",
    "    total_discount_given = Column(Float, nullable=True)\n",
    "    total_net_sales = Column(Float, nullable=True)\n",
    "\n",
    "\n",
    "# --- Data Processing Functions (Feature Engineering & Aggregation) ---\n",
    "# These functions now assume input DataFrames have column names matching SQLAlchemy models (snake_case)\n",
    "\n",
    "def add_customer_features(customers_df):\n",
    "    \"\"\"Adds feature engineered columns to the customers DataFrame.\"\"\"\n",
    "    logger.info(\"Adding customer features (days_since_birth)...\")\n",
    "    # Assumes 'date_of_birth' column exists and is DateType/TimestampType in DB\n",
    "    processed_df = customers_df.withColumn(\n",
    "        \"days_since_birth\",\n",
    "        when(col(\"date_of_birth\").isNotNull(), datediff(current_date(), col(\"date_of_birth\")))\n",
    "        .otherwise(lit(None)) # Handle cases where date_of_birth is null\n",
    "    )\n",
    "    logger.info(\"Customer feature engineering complete.\")\n",
    "    return processed_df\n",
    "\n",
    "def add_transaction_features(transactions_df):\n",
    "    \"\"\"Adds feature engineered columns to the transactions DataFrame.\"\"\"\n",
    "    logger.info(\"Adding transaction features (net_line_total, day_of_week)...\")\n",
    "    # Ensure required columns are cast to appropriate types before calculation/extraction\n",
    "    # Assuming columns are already in snake_case and appropriate types from DB read,\n",
    "    # but explicit casting here adds robustness if DB schema is slightly different.\n",
    "    transactions_df = transactions_df.withColumn(\"unit_price\", col(\"unit_price\").cast(FloatType()))\\\n",
    "                                     .withColumn(\"quantity\", col(\"quantity\").cast(IntegerType()))\\\n",
    "                                     .withColumn(\"discount\", col(\"discount\").cast(FloatType()))\\\n",
    "                                     .withColumn(\"transaction_date\", col(\"transaction_date\").cast(TimestampType())) # Ensure datetime type\n",
    "\n",
    "\n",
    "    processed_df = transactions_df.withColumn(\n",
    "        \"net_line_total\",\n",
    "        (col(\"unit_price\") * col(\"quantity\")) - col(\"discount\") # Calculation on cleaned columns\n",
    "    ).withColumn(\n",
    "        \"transaction_day_of_week\",\n",
    "         when(col(\"transaction_date\").isNotNull(), dayofweek(col(\"transaction_date\")))\n",
    "        .otherwise(lit(None))\n",
    "    )\n",
    "    logger.info(\"Transaction feature engineering complete.\")\n",
    "    return processed_df\n",
    "\n",
    "\n",
    "def aggregate_daily_store_sales(transactions_df):\n",
    "    \"\"\"Aggregates transaction data (assuming snake_case columns) to get daily sales summary per store.\"\"\"\n",
    "    logger.info(\"Starting daily store sales aggregation...\")\n",
    "\n",
    "    # Ensure transaction_date is a proper timestamp/datetime before casting to Date\n",
    "    # Assuming the input transactions_df already has 'transaction_date' as TimestampType\n",
    "    transactions_df = transactions_df.withColumn(\"sale_date\", to_date(col(\"transaction_date\")))\n",
    "\n",
    "    # Group by store_id and sale_date and perform aggregations\n",
    "    daily_sales_df = transactions_df.groupBy(\"store_id\", \"sale_date\").agg(\n",
    "        spark_count(\"*\").alias(\"transaction_count\"),\n",
    "        spark_sum(\"line_total\").alias(\"total_sales\"), # Using line_total as total sales per line\n",
    "        spark_avg(\"unit_price\").alias(\"average_unit_price\"),\n",
    "        spark_sum(\"quantity\").alias(\"total_quantity_sold\"),\n",
    "        spark_sum(\"discount\").alias(\"total_discount_given\"),\n",
    "        spark_sum(\"net_line_total\").alias(\"total_net_sales\") # Aggregate the new net_line_total\n",
    "    )\n",
    "\n",
    "    logger.info(\"Daily store sales aggregation complete.\")\n",
    "    return daily_sales_df.select(\n",
    "        \"store_id\",\n",
    "        \"sale_date\",\n",
    "        \"transaction_count\",\n",
    "        \"total_sales\",\n",
    "        \"average_unit_price\",\n",
    "        \"total_quantity_sold\",\n",
    "        \"total_discount_given\",\n",
    "        \"total_net_sales\"\n",
    "    )\n",
    "\n",
    "# --- Database Interaction Functions ---\n",
    "\n",
    "def setup_database(engine):\n",
    "    \"\"\"Create schema and set permissions if needed, then create tables (if they don't exist).\"\"\"\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            # Check if schema exists\n",
    "            schema_exists = conn.execute(\n",
    "                text(\"SELECT 1 FROM information_schema.schemata WHERE schema_name = :schema\"),\n",
    "                {'schema': SCHEMA_NAME}\n",
    "            ).scalar()\n",
    "\n",
    "            if not schema_exists:\n",
    "                logger.info(f\"Creating schema {SCHEMA_NAME}\")\n",
    "                conn.execute(text(f\"CREATE SCHEMA {SCHEMA_NAME}\"))\n",
    "                # Grant permissions - adjust user/role as needed\n",
    "                grant_user = os.getenv('DB_USER', 'postgres')\n",
    "                conn.execute(text(f\"\"\"\n",
    "                    GRANT ALL ON SCHEMA {SCHEMA_NAME} TO \"{grant_user}\";\n",
    "                    ALTER DEFAULT PRIVILEGES IN SCHEMA {SCHEMA_NAME}\n",
    "                    GRANT ALL ON TABLES TO \"{grant_user}\";\n",
    "                \"\"\"))\n",
    "                logger.info(f\"Schema {SCHEMA_NAME} created and permissions granted to {grant_user}.\")\n",
    "\n",
    "            # Create all tables if they don't exist (this is safe for existing tables)\n",
    "            # This will add new columns (days_since_birth, net_line_total, transaction_day_of_week)\n",
    "            # to existing tables if they are missing, and create the daily_store_sales table.\n",
    "            logger.info(\"Creating/Ensuring existence of database tables based on models...\")\n",
    "            Base.metadata.create_all(engine)\n",
    "            logger.info(\"Table creation/check complete.\")\n",
    "\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Database setup failed: {str(e)}\", exc_info=True)\n",
    "        return False\n",
    "\n",
    "def load_data_from_postgres_to_spark(spark, db_host, db_port, db_name, db_user, db_password, schema_name, table_name):\n",
    "    \"\"\"Reads a table from PostgreSQL into a Spark DataFrame using JDBC with optional partitioning.\"\"\"\n",
    "    logger.info(f\"Reading data from database table: {schema_name}.{table_name}\")\n",
    "    try:\n",
    "        jdbc_url = f'jdbc:postgresql://{db_host}:{db_port}/{db_name}'\n",
    "        db_properties = {\n",
    "            \"user\": db_user,\n",
    "            \"password\": db_password,\n",
    "            \"driver\": \"org.postgresql.Driver\",\n",
    "            \"fetchsize\": \"10000\" # Helps with large reads\n",
    "        }\n",
    "\n",
    "        # Construct the full table name including schema (quoted for case sensitivity/special chars)\n",
    "        full_table_name = f'\"{schema_name}\".\"{table_name}\"'\n",
    "\n",
    "        # === Add Partitioning Logic Here for Performance ===\n",
    "        # This is crucial for large tables like 'transactions'.\n",
    "        # You need to determine appropriate partitionColumn, lowerBound, upperBound, and numPartitions\n",
    "        partition_options = {}\n",
    "        if table_name == 'transactions':\n",
    "            logger.info(f\"Applying partitioning for {table_name}\")\n",
    "            logger.warning(f\"Partitioning options are NOT set for {table_name}. This will be slow for large tables. Consider adding partitionColumn, lowerBound, upperBound, and numPartitions.\")\n",
    "\n",
    "        spark_df = spark.read.jdbc(\n",
    "            url=jdbc_url,\n",
    "            table=full_table_name,\n",
    "            properties=db_properties,\n",
    "            **partition_options # Pass the partition options\n",
    "        )\n",
    "        logger.info(f\"Successfully read data from {schema_name}.{table_name}. Schema: {spark_df.printSchema()}\")\n",
    "        # logger.info(f\"Number of partitions after reading: {spark_df.rdd.getNumPartitions()}\") # Check partition count\n",
    "        return spark_df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to read data from {schema_name}.{table_name} into Spark: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "def upsert_data(session, table_class, records, primary_keys, batch_size=1000):\n",
    "    \"\"\"Upsert data into the database with batch processing using SQLAlchemy merge.\"\"\"\n",
    "    try:\n",
    "        total_records = len(records)\n",
    "        if total_records == 0:\n",
    "            logger.info(f\"No records to upsert for {table_class.__tablename__}.\")\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Starting to upsert {total_records} records for {table_class.__tablename__} in batches of {batch_size}\")\n",
    "\n",
    "        for i in range(0, total_records, batch_size):\n",
    "            batch = records[i:i + batch_size]\n",
    "            batch_num = (i // batch_size) + 1\n",
    "            total_batches = (total_records - 1) // batch_size + 1\n",
    "\n",
    "            try:\n",
    "                # Use merge for upsert logic, which handles existing primary keys (simple or composite)\n",
    "                for record in batch:\n",
    "                    # Convert Spark Row object to dictionary if needed, or ensure records are dicts\n",
    "                    record_dict = record if isinstance(record, dict) else record.asDict()\n",
    "                    instance = table_class(**record_dict)\n",
    "                    session.merge(instance) # merge handles both insert and update based on PK\n",
    "\n",
    "                session.commit()\n",
    "                logger.info(f\"Successfully processed batch {batch_num}/{total_batches} for {table_class.__tablename__}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                session.rollback()\n",
    "                logger.error(f\"Error processing batch {batch_num} for {table_class.__tablename__}: {str(e)}\", exc_info=True)\n",
    "                # Log affected primary key(s) if possible\n",
    "                try:\n",
    "                    pk_values = [\n",
    "                        {pk: record_dict.get(pk) for pk in primary_keys}\n",
    "                        for record_dict in (record if isinstance(record, dict) else record.asDict() for record in batch)\n",
    "                    ]\n",
    "                    logger.error(f\"Batch primary key values: {pk_values}\")\n",
    "                except Exception as log_e:\n",
    "                    logger.error(f\"Could not log batch primary keys: {log_e}\")\n",
    "\n",
    "                # Decide whether to re-raise or continue. Re-raising stops on first error.\n",
    "                raise # Stop on error\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        session.rollback()\n",
    "        logger.error(f\"Critical error in upsert_data for {table_class.__tablename__}: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "def write_spark_df_to_postgres(spark_df, table_name, engine):\n",
    "    \"\"\"Write Spark DataFrame to PostgreSQL using SQLAlchemy upsert (merge) logic.\"\"\"\n",
    "    try:\n",
    "        # Map table name to SQLAlchemy class name\n",
    "        # Handles 'daily_store_sales' -> 'DailyStoreSales'\n",
    "        table_class_name = ''.join(word.capitalize() for word in table_name.split('_'))\n",
    "        table_class = globals().get(table_class_name)\n",
    "\n",
    "        if not table_class:\n",
    "             logger.error(f\"SQLAlchemy class not found for table name: {table_name}\")\n",
    "             return\n",
    "\n",
    "        # Define primary keys for each table\n",
    "        primary_keys_map = {\n",
    "            'stores': ['store_id'],\n",
    "            'employees': ['employee_id'],\n",
    "            'customers': ['customer_id'],\n",
    "            'products': ['product_id'],\n",
    "            'discounts': ['id'],\n",
    "            'transactions': ['id'], # Assuming 'id' is the PK\n",
    "            'daily_store_sales': ['store_id', 'sale_date'] # Composite key\n",
    "        }\n",
    "        primary_keys = primary_keys_map.get(table_name)\n",
    "\n",
    "        if not primary_keys:\n",
    "            logger.error(f\"Primary key(s) not defined for table {table_name}\")\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Starting write process for table {table_name}...\")\n",
    "\n",
    "        # Convert Spark DataFrame to Pandas DataFrames iteratively\n",
    "        # Using toLocalIterator for larger dataframes to avoid driver memory issues\n",
    "        pandas_iter = spark_df.toLocalIterator()\n",
    "        records_buffer = []\n",
    "        batch_size = 5000 # Batch size for converting Spark rows to dicts and upserting\n",
    "\n",
    "        Session = sessionmaker(bind=engine)\n",
    "        session = Session()\n",
    "        try:\n",
    "            i = 0\n",
    "            for row in pandas_iter:\n",
    "                records_buffer.append(row.asDict()) # Convert Spark Row to dictionary\n",
    "                i += 1\n",
    "\n",
    "                if i % batch_size == 0:\n",
    "                    logger.info(f\"Buffer size reached {batch_size}, processing batch {i // batch_size}\")\n",
    "                    upsert_data(session, table_class, records_buffer, primary_keys)\n",
    "                    records_buffer = [] # Clear buffer\n",
    "                    # Commit is done inside upsert_data for each batch\n",
    "\n",
    "            # Process any remaining records in the buffer\n",
    "            if records_buffer:\n",
    "                logger.info(f\"Processing final batch of {len(records_buffer)} records\")\n",
    "                upsert_data(session, table_class, records_buffer, primary_keys)\n",
    "\n",
    "            # Verify count only for smaller tables or if specifically needed and feasible\n",
    "            # Note: Counting can be slow on large tables, especially after upserts\n",
    "            if table_name in ['stores', 'employees', 'discounts', 'daily_store_sales']: # Only count the tables we loaded/updated\n",
    "                 try:\n",
    "                    with engine.connect() as conn:\n",
    "                        # Use quoted identifiers for the schema and table name\n",
    "                        count = conn.execute(text(f'SELECT COUNT(*) FROM \"{SCHEMA_NAME}\".\"{table_name}\"')).scalar()\n",
    "                        logger.info(f\"Final count for {table_name}: {count} records\")\n",
    "                 except Exception as count_e:\n",
    "                    logger.warning(f\"Could not verify count for {table_name}: {count_e}\")\n",
    "            else:\n",
    "                 logger.info(f\"Skipped final count verification for table {table_name} (read from DB).\")\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "             logger.error(f\"Error during data processing and upsert for {table_name}: {str(e)}\", exc_info=True)\n",
    "             session.rollback() # Rollback any outstanding transaction if an error occurred before a commit\n",
    "             raise\n",
    "        finally:\n",
    "            session.close()\n",
    "            logger.info(f\"Session closed for table {table_name}.\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fatal error in write_spark_df_to_postgres for {table_name}: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "\n",
    "def process_and_aggregate_from_db():\n",
    "    \"\"\"Main function to read data from DB, process, aggregate, and load aggregated data.\"\"\"\n",
    "    # Load environment variables\n",
    "    DB_USER = os.getenv('DB_USER')\n",
    "    DB_PASSWORD = os.getenv('DB_PASSWORD')\n",
    "    DB_HOST = os.getenv('DB_HOST')\n",
    "    DB_PORT = os.getenv('DB_PORT')\n",
    "    DB_NAME = os.getenv('DB_NAME')\n",
    "\n",
    "    required_vars = {\n",
    "        'DB_USER': DB_USER,\n",
    "        'DB_PASSWORD': DB_PASSWORD,\n",
    "        'DB_HOST': DB_HOST,\n",
    "        'DB_NAME': DB_NAME,\n",
    "    }\n",
    "\n",
    "    for var_name, var_value in required_vars.items():\n",
    "        if not var_value:\n",
    "            raise ValueError(f\"Missing required environment variable: {var_name}\")\n",
    "\n",
    "    DB_PORT = DB_PORT if DB_PORT else '5432' # Set default for PORT\n",
    "\n",
    "    engine = None # Initialize engine outside try for finally block\n",
    "\n",
    "    try:\n",
    "        # Create SQLAlchemy engine for schema setup and writing (only for aggregated data)\n",
    "        engine = create_engine(\n",
    "            f'postgresql+psycopg2://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}',\n",
    "            pool_size=15, # Adjust pool size\n",
    "            max_overflow=25, # Adjust max overflow\n",
    "            pool_pre_ping=True,\n",
    "            pool_recycle=3600,\n",
    "            connect_args={'connect_timeout': 10}\n",
    "        )\n",
    "\n",
    "        # Setup Database: Create schema and tables if they don't exist\n",
    "        # This will ensure the daily_store_sales table exists and add FE columns if missing\n",
    "        if not setup_database(engine):\n",
    "            raise RuntimeError(\"Database setup failed\")\n",
    "\n",
    "        # --- Read Data from Database into Spark ---\n",
    "        logger.info(\"Starting to read data from database tables...\")\n",
    "        start_time_read_db = datetime.now()\n",
    "        try:\n",
    "            # Read all necessary tables from the database\n",
    "            customers_df = load_data_from_postgres_to_spark(spark, DB_HOST, DB_PORT, DB_NAME, DB_USER, DB_PASSWORD, SCHEMA_NAME, 'customers')\n",
    "            products_df = load_data_from_postgres_to_spark(spark, DB_HOST, DB_PORT, DB_NAME, DB_USER, DB_PASSWORD, SCHEMA_NAME, 'products')\n",
    "            transactions_df = load_data_from_postgres_to_spark(spark, DB_HOST, DB_PORT, DB_NAME, DB_USER, DB_PASSWORD, SCHEMA_NAME, 'transactions')\n",
    "            stores_df = load_data_from_postgres_to_spark(spark, DB_HOST, DB_PORT, DB_NAME, DB_USER, DB_PASSWORD, SCHEMA_NAME, 'stores')\n",
    "            employees_df = load_data_from_postgres_to_spark(spark, DB_HOST, DB_PORT, DB_NAME, DB_USER, DB_PASSWORD, SCHEMA_NAME, 'employees')\n",
    "            discounts_df = load_data_from_postgres_to_spark(spark, DB_HOST, DB_PORT, DB_NAME, DB_USER, DB_PASSWORD, SCHEMA_NAME, 'discounts')\n",
    "\n",
    "            logger.info(f\"Completed reading data from database in {datetime.now() - start_time_read_db}\")\n",
    "\n",
    "        except Exception as e:\n",
    "             logger.error(f\"Failed to read data from DB. Cannot proceed with processing. Error: {str(e)}\", exc_info=True)\n",
    "             # Reading core data from DB is fatal if it fails\n",
    "             raise\n",
    "\n",
    "        # --- Perform Feature Engineering on DB-read DataFrames ---\n",
    "        logger.info(\"Starting feature engineering on DB-read dataframes...\")\n",
    "        start_time_fe = datetime.now()\n",
    "        try:\n",
    "            # Apply feature engineering functions to the relevant DataFrames\n",
    "            # These functions now expect snake_case columns from the DB read\n",
    "            processed_customer_df = add_customer_features(customers_df)\n",
    "            processed_transaction_df = add_transaction_features(transactions_df)\n",
    "            # No specific FE planned for other tables in this iteration\n",
    "\n",
    "            logger.info(f\"Feature engineering complete in {datetime.now() - start_time_fe}\")\n",
    "\n",
    "        except Exception as e:\n",
    "             logger.error(f\"Error during feature engineering: {str(e)}\", exc_info=True)\n",
    "             # Feature engineering failure is likely fatal as aggregation depends on it\n",
    "             raise\n",
    "\n",
    "        # --- Perform Aggregation and Load New Tables ---\n",
    "        if processed_transaction_df is not None: # Check if transactions data was processed\n",
    "            logger.info(\"Starting aggregation for daily store sales...\")\n",
    "            start_time_agg = datetime.now()\n",
    "            try:\n",
    "                # Aggregate using the feature-engineered transactions DataFrame\n",
    "                daily_sales_df = aggregate_daily_store_sales(processed_transaction_df)\n",
    "                # Load the aggregated data into the new daily_store_sales table\n",
    "                write_spark_df_to_postgres(daily_sales_df, 'daily_store_sales', engine)\n",
    "                logger.info(f\"Completed daily store sales aggregation and loading in {datetime.now() - start_time_agg}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error during daily store sales aggregation or loading: {str(e)}\", exc_info=True)\n",
    "                # Decide whether to stop or continue on aggregation error\n",
    "                # For now, let's treat aggregation failure as fatal\n",
    "                raise\n",
    "\n",
    "        else:\n",
    "            logger.warning(\"No processed transaction data available for aggregation.\")\n",
    "\n",
    "\n",
    "        logger.info(\"Overall data processing and aggregation completed.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fatal error during overall process: {str(e)}\", exc_info=True)\n",
    "        # Re-raise the exception so the main block catches it for process exit code\n",
    "        raise\n",
    "\n",
    "    finally:\n",
    "        # Ensure database engine and Spark session are closed/stopped\n",
    "        if engine:\n",
    "            engine.dispose()\n",
    "            logger.info(\"Database engine disposed.\")\n",
    "        if spark:\n",
    "            spark.stop()\n",
    "            logger.info(\"Spark session stopped.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # This block runs when the script is executed directly\n",
    "    try:\n",
    "        logger.info(\"Starting data processing and aggregation script (reading from DB)...\")\n",
    "        overall_start_time = datetime.now()\n",
    "        process_and_aggregate_from_db() # Call the new main function\n",
    "        overall_end_time = datetime.now()\n",
    "        logger.info(f\"Script finished. Total elapsed time: {overall_end_time - overall_start_time}\")\n",
    "        sys.exit(0) # Exit successfully\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Script terminated due to fatal error: {str(e)}\", exc_info=True)\n",
    "        sys.exit(1) # Exit with a non-zero status code to indicate failure\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
